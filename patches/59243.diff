commit 82d70a84c8ee42ef969a9cfddc0f5b30b16165f5
Author: Richard Henderson <richard.henderson@linaro.org>
Date:   Tue Nov 14 12:13:29 2023 -0800

    linux-user: Fix loaddr computation for some elf files
    
    The file offset of the load segment is not relevant to the
    low address, only the beginning of the virtual address page.
    
    Cc: qemu-stable@nongnu.org
    Fixes: a93934fecd4 ("elf: take phdr offset into account when calculating the program load address")
    Resolves: https://gitlab.com/qemu-project/qemu/-/issues/1952
    Signed-off-by: Richard Henderson <richard.henderson@linaro.org>
    Reviewed-by: Michael Tokarev <mjt@tls.msk.ru>

diff --git a/linux-user/elfload.c b/linux-user/elfload.c
index 4cd6891d7b..cf9e74468b 100644
--- a/linux-user/elfload.c
+++ b/linux-user/elfload.c
@@ -3261,300 +3261,300 @@ static bool parse_elf_properties(const ImageSource *src,
 static void load_elf_image(const char *image_name, const ImageSource *src,
                            struct image_info *info, struct elfhdr *ehdr,
                            char **pinterp_name)
 {
     g_autofree struct elf_phdr *phdr = NULL;
     abi_ulong load_addr, load_bias, loaddr, hiaddr, error;
     int i, prot_exec;
     Error *err = NULL;
 
     /*
      * First of all, some simple consistency checks.
      * Note that we rely on the bswapped ehdr staying in bprm_buf,
      * for later use by load_elf_binary and create_elf_tables.
      */
     if (!imgsrc_read(ehdr, 0, sizeof(*ehdr), src, &err)) {
         goto exit_errmsg;
     }
     if (!elf_check_ident(ehdr)) {
         error_setg(&err, "Invalid ELF image for this architecture");
         goto exit_errmsg;
     }
     bswap_ehdr(ehdr);
     if (!elf_check_ehdr(ehdr)) {
         error_setg(&err, "Invalid ELF image for this architecture");
         goto exit_errmsg;
     }
 
     phdr = imgsrc_read_alloc(ehdr->e_phoff,
                              ehdr->e_phnum * sizeof(struct elf_phdr),
                              src, &err);
     if (phdr == NULL) {
         goto exit_errmsg;
     }
     bswap_phdr(phdr, ehdr->e_phnum);
 
     info->nsegs = 0;
     info->pt_dynamic_addr = 0;
 
     mmap_lock();
 
     /*
      * Find the maximum size of the image and allocate an appropriate
      * amount of memory to handle that.  Locate the interpreter, if any.
      */
     loaddr = -1, hiaddr = 0;
     info->alignment = 0;
     info->exec_stack = EXSTACK_DEFAULT;
     for (i = 0; i < ehdr->e_phnum; ++i) {
         struct elf_phdr *eppnt = phdr + i;
         if (eppnt->p_type == PT_LOAD) {
-            abi_ulong a = eppnt->p_vaddr - eppnt->p_offset;
+            abi_ulong a = eppnt->p_vaddr & TARGET_PAGE_MASK;
             if (a < loaddr) {
                 loaddr = a;
             }
             a = eppnt->p_vaddr + eppnt->p_memsz - 1;
             if (a > hiaddr) {
                 hiaddr = a;
             }
             ++info->nsegs;
             info->alignment |= eppnt->p_align;
         } else if (eppnt->p_type == PT_INTERP && pinterp_name) {
             g_autofree char *interp_name = NULL;
 
             if (*pinterp_name) {
                 error_setg(&err, "Multiple PT_INTERP entries");
                 goto exit_errmsg;
             }
 
             interp_name = imgsrc_read_alloc(eppnt->p_offset, eppnt->p_filesz,
                                             src, &err);
             if (interp_name == NULL) {
                 goto exit_errmsg;
             }
             if (interp_name[eppnt->p_filesz - 1] != 0) {
                 error_setg(&err, "Invalid PT_INTERP entry");
                 goto exit_errmsg;
             }
             *pinterp_name = g_steal_pointer(&interp_name);
         } else if (eppnt->p_type == PT_GNU_PROPERTY) {
             if (!parse_elf_properties(src, info, eppnt, &err)) {
                 goto exit_errmsg;
             }
         } else if (eppnt->p_type == PT_GNU_STACK) {
             info->exec_stack = eppnt->p_flags & PF_X;
         }
     }
 
     load_addr = loaddr;
 
     if (pinterp_name != NULL) {
         if (ehdr->e_type == ET_EXEC) {
             /*
              * Make sure that the low address does not conflict with
              * MMAP_MIN_ADDR or the QEMU application itself.
              */
             probe_guest_base(image_name, loaddr, hiaddr);
         } else {
             abi_ulong align;
 
             /*
              * The binary is dynamic, but we still need to
              * select guest_base.  In this case we pass a size.
              */
             probe_guest_base(image_name, 0, hiaddr - loaddr);
 
             /*
              * Avoid collision with the loader by providing a different
              * default load address.
              */
             load_addr += elf_et_dyn_base;
 
             /*
              * TODO: Better support for mmap alignment is desirable.
              * Since we do not have complete control over the guest
              * address space, we prefer the kernel to choose some address
              * rather than force the use of LOAD_ADDR via MAP_FIXED.
              * But without MAP_FIXED we cannot guarantee alignment,
              * only suggest it.
              */
             align = pow2ceil(info->alignment);
             if (align) {
                 load_addr &= -align;
             }
         }
     }
 
     /*
      * Reserve address space for all of this.
      *
      * In the case of ET_EXEC, we supply MAP_FIXED_NOREPLACE so that we get
      * exactly the address range that is required.  Without reserved_va,
      * the guest address space is not isolated.  We have attempted to avoid
      * conflict with the host program itself via probe_guest_base, but using
      * MAP_FIXED_NOREPLACE instead of MAP_FIXED provides an extra check.
      *
      * Otherwise this is ET_DYN, and we are searching for a location
      * that can hold the memory space required.  If the image is
      * pre-linked, LOAD_ADDR will be non-zero, and the kernel should
      * honor that address if it happens to be free.
      *
      * In both cases, we will overwrite pages in this range with mappings
      * from the executable.
      */
     load_addr = target_mmap(load_addr, (size_t)hiaddr - loaddr + 1, PROT_NONE,
                             MAP_PRIVATE | MAP_ANON | MAP_NORESERVE |
                             (ehdr->e_type == ET_EXEC ? MAP_FIXED_NOREPLACE : 0),
                             -1, 0);
     if (load_addr == -1) {
         goto exit_mmap;
     }
     load_bias = load_addr - loaddr;
 
     if (elf_is_fdpic(ehdr)) {
         struct elf32_fdpic_loadseg *loadsegs = info->loadsegs =
             g_malloc(sizeof(*loadsegs) * info->nsegs);
 
         for (i = 0; i < ehdr->e_phnum; ++i) {
             switch (phdr[i].p_type) {
             case PT_DYNAMIC:
                 info->pt_dynamic_addr = phdr[i].p_vaddr + load_bias;
                 break;
             case PT_LOAD:
                 loadsegs->addr = phdr[i].p_vaddr + load_bias;
                 loadsegs->p_vaddr = phdr[i].p_vaddr;
                 loadsegs->p_memsz = phdr[i].p_memsz;
                 ++loadsegs;
                 break;
             }
         }
     }
 
     info->load_bias = load_bias;
     info->code_offset = load_bias;
     info->data_offset = load_bias;
     info->load_addr = load_addr;
     info->entry = ehdr->e_entry + load_bias;
     info->start_code = -1;
     info->end_code = 0;
     info->start_data = -1;
     info->end_data = 0;
     /* Usual start for brk is after all sections of the main executable. */
     info->brk = TARGET_PAGE_ALIGN(hiaddr + load_bias);
     info->elf_flags = ehdr->e_flags;
 
     prot_exec = PROT_EXEC;
 #ifdef TARGET_AARCH64
     /*
      * If the BTI feature is present, this indicates that the executable
      * pages of the startup binary should be mapped with PROT_BTI, so that
      * branch targets are enforced.
      *
      * The startup binary is either the interpreter or the static executable.
      * The interpreter is responsible for all pages of a dynamic executable.
      *
      * Elf notes are backward compatible to older cpus.
      * Do not enable BTI unless it is supported.
      */
     if ((info->note_flags & GNU_PROPERTY_AARCH64_FEATURE_1_BTI)
         && (pinterp_name == NULL || *pinterp_name == 0)
         && cpu_isar_feature(aa64_bti, ARM_CPU(thread_cpu))) {
         prot_exec |= TARGET_PROT_BTI;
     }
 #endif
 
     for (i = 0; i < ehdr->e_phnum; i++) {
         struct elf_phdr *eppnt = phdr + i;
         if (eppnt->p_type == PT_LOAD) {
             abi_ulong vaddr, vaddr_po, vaddr_ps, vaddr_ef, vaddr_em;
             int elf_prot = 0;
 
             if (eppnt->p_flags & PF_R) {
                 elf_prot |= PROT_READ;
             }
             if (eppnt->p_flags & PF_W) {
                 elf_prot |= PROT_WRITE;
             }
             if (eppnt->p_flags & PF_X) {
                 elf_prot |= prot_exec;
             }
 
             vaddr = load_bias + eppnt->p_vaddr;
             vaddr_po = vaddr & ~TARGET_PAGE_MASK;
             vaddr_ps = vaddr & TARGET_PAGE_MASK;
 
             vaddr_ef = vaddr + eppnt->p_filesz;
             vaddr_em = vaddr + eppnt->p_memsz;
 
             /*
              * Some segments may be completely empty, with a non-zero p_memsz
              * but no backing file segment.
              */
             if (eppnt->p_filesz != 0) {
                 error = imgsrc_mmap(vaddr_ps, eppnt->p_filesz + vaddr_po,
                                     elf_prot, MAP_PRIVATE | MAP_FIXED,
                                     src, eppnt->p_offset - vaddr_po);
                 if (error == -1) {
                     goto exit_mmap;
                 }
             }
 
             /* If the load segment requests extra zeros (e.g. bss), map it. */
             if (vaddr_ef < vaddr_em &&
                 !zero_bss(vaddr_ef, vaddr_em, elf_prot, &err)) {
                 goto exit_errmsg;
             }
 
             /* Find the full program boundaries.  */
             if (elf_prot & PROT_EXEC) {
                 if (vaddr < info->start_code) {
                     info->start_code = vaddr;
                 }
                 if (vaddr_ef > info->end_code) {
                     info->end_code = vaddr_ef;
                 }
             }
             if (elf_prot & PROT_WRITE) {
                 if (vaddr < info->start_data) {
                     info->start_data = vaddr;
                 }
                 if (vaddr_ef > info->end_data) {
                     info->end_data = vaddr_ef;
                 }
             }
 #ifdef TARGET_MIPS
         } else if (eppnt->p_type == PT_MIPS_ABIFLAGS) {
             Mips_elf_abiflags_v0 abiflags;
 
             if (!imgsrc_read(&abiflags, eppnt->p_offset, sizeof(abiflags),
                              src, &err)) {
                 goto exit_errmsg;
             }
             bswap_mips_abiflags(&abiflags);
             info->fp_abi = abiflags.fp_abi;
 #endif
         }
     }
 
     if (info->end_data == 0) {
         info->start_data = info->end_code;
         info->end_data = info->end_code;
     }
 
     if (qemu_log_enabled()) {
         load_symbols(ehdr, src, load_bias);
     }
 
     debuginfo_report_elf(image_name, src->fd, load_bias);
 
     mmap_unlock();
 
     close(src->fd);
     return;
 
  exit_mmap:
     error_setg_errno(&err, errno, "Error mapping file");
     goto exit_errmsg;
  exit_errmsg:
     error_reportf_err(err, "%s: ", image_name);
     exit(-1);
 }
