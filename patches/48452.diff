commit 1e54e5657927996e86b155d89f51c7b5a73461d2
Author: alexlyulkov <alex.lyulkov@gmail.com>
Date:   Thu Sep 14 14:25:24 2023 +0700

    Merge pull request #24266 from alexlyulkov:al/tf-argmax-default-dim
    
    Added default dimension value to tensorflow ArgMax and ArgMin layers #24266
    
    Added default dimension value to tensorflow ArgMax and ArgMin layers.
    Added exception when accessing layer's input with out of range index.
    Fixes https://bugs.chromium.org/p/oss-fuzz/issues/detail?id=48452

diff --git a/modules/dnn/src/tensorflow/tf_importer.cpp b/modules/dnn/src/tensorflow/tf_importer.cpp
index a23fac187b..756bdc949c 100644
--- a/modules/dnn/src/tensorflow/tf_importer.cpp
+++ b/modules/dnn/src/tensorflow/tf_importer.cpp
@@ -2665,18 +2665,24 @@ void TFImporter::parseActivation(tensorflow::GraphDef& net, const tensorflow::No
     connectToAllBlobs(layer_id, dstNet, parsePin(layer.input(0)), id, num_inputs);
 }
 
+// ArgMin or ArgMax node
 void TFImporter::parseArg(tensorflow::GraphDef& net, const tensorflow::NodeDef& layer, LayerParams& layerParams)
 {
     const std::string& name = layer.name();
     const std::string& type = layer.op();
 
-    Mat dimension = getTensorContent(getConstBlob(layer, value_id, 1));
-    CV_Assert(dimension.total() == 1 && dimension.type() == CV_32SC1);
-    layerParams.set("axis", *dimension.ptr<int>());
+    if (layer.input_size() < 2)
+        layerParams.set("axis", 0); // default dimension is 0
+    else
+    {
+        Mat dimension = getTensorContent(getConstBlob(layer, value_id, 1));
+        CV_Assert(dimension.total() == 1 && dimension.type() == CV_32SC1);
+        layerParams.set("axis", dimension.at<int>(0));
+    }
     layerParams.set("op", type == "ArgMax" ? "max" : "min");
     layerParams.set("keepdims", false); //tensorflow doesn't have this atrr, the output's dims minus one(default);
 
     int id = dstNet.addLayer(name, "Arg", layerParams);
     layer_id[name] = id;
     connect(layer_id, dstNet, parsePin(layer.input(0)), id, 0);
 }
@@ -2853,40 +2859,41 @@ void TFImporter::connectToAllBlobs(const std::map<String, int>& layer_id, Net& n
 const tensorflow::TensorProto& TFImporter::getConstBlob(const tensorflow::NodeDef &layer, std::map<String, int> const_layers,
                                               int input_blob_index, int* actual_inp_blob_idx) {
     if (input_blob_index == -1) {
         for(int i = 0; i < layer.input_size(); i++) {
             Pin input = parsePin(layer.input(i));
             if (const_layers.find(input.name) != const_layers.end()) {
                 if (input_blob_index != -1)
                     CV_Error(Error::StsError, "More than one input is Const op");
 
                 input_blob_index = i;
             }
         }
     }
 
     if (input_blob_index == -1)
         CV_Error(Error::StsError, "Const input blob for weights not found");
+    CV_CheckLT(input_blob_index, layer.input_size(), "Input index is out of range");
 
     Pin kernel_inp = parsePin(layer.input(input_blob_index));
     if (const_layers.find(kernel_inp.name) == const_layers.end())
         CV_Error(Error::StsError, "Input [" + layer.input(input_blob_index) +
                                   "] for node [" + layer.name() + "] not found");
     if (kernel_inp.blobIndex != 0)
         CV_Error(Error::StsError, "Unsupported kernel input");
 
     if(actual_inp_blob_idx) {
         *actual_inp_blob_idx = input_blob_index;
     }
 
     int nodeIdx = const_layers.at(kernel_inp.name);
     if (nodeIdx < netBin.node_size() && netBin.node(nodeIdx).name() == kernel_inp.name)
     {
         return netBin.node(nodeIdx).attr().at("value").tensor();
     }
     else
     {
         CV_Assert_N(nodeIdx < netTxt.node_size(),
                     netTxt.node(nodeIdx).name() == kernel_inp.name);
         return netTxt.node(nodeIdx).attr().at("value").tensor();
     }
 }
