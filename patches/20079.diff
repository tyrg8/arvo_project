commit c5203079596f4614bcf7b4e653b4bb8810e64a3e
Author: Chen Huitao <h980501427@163.com>
Date:   Tue Feb 25 11:36:06 2020 +0800

    fix some oss-fuzz (#1206)
    
    * fix oss-fuzz 18138.
    
    * fix oss-fuzz 20079.
    
    * fix oss-fuzz 20209.
    
    * fix oss-fuzz 20210.
    
    * fix oss-fuzz 20262.
    
    * rollback.
    
    * rollback.
    
    * fix oss-fuzz 20079.
    
    * fix oss-fuzz 20179.
    
    * fix oss-fuzz 20195.
    
    * fix oss-fuzz 20206.
    
    * fix oss-fuzz 20207.
    
    * fix oss-fuzz 20265.

diff --git a/qemu/cpu-exec.c b/qemu/cpu-exec.c
index 62975085..99b3d938 100644
--- a/qemu/cpu-exec.c
+++ b/qemu/cpu-exec.c
@@ -384,17 +384,20 @@ static TranslationBlock *tb_find_slow(CPUArchState *env, target_ulong pc,
                 virt_page2 = (pc & TARGET_PAGE_MASK) +
                     TARGET_PAGE_SIZE;
                 phys_page2 = get_page_addr_code(env, virt_page2);
                 if (tb->page_addr[1] == phys_page2)
                     goto found;
             } else {
                 goto found;
             }
         }
         ptb1 = &tb->phys_hash_next;
     }
 not_found:
     /* if no translated code available, then translate it now */
     tb = tb_gen_code(cpu, pc, cs_base, (int)flags, 0);   // qq
+    if (tb == NULL) {
+        return NULL;
+    }
 
 found:
     /* Move the last found TB to the head of the list */
diff --git a/qemu/fpu/softfloat.c b/qemu/fpu/softfloat.c
index c9b1785b..d409e78f 100644
--- a/qemu/fpu/softfloat.c
+++ b/qemu/fpu/softfloat.c
@@ -4632,48 +4632,48 @@ int32 floatx80_to_int32( floatx80 a STATUS_PARAM )
 int32 floatx80_to_int32_round_to_zero( floatx80 a STATUS_PARAM )
 {
     flag aSign;
     int32 aExp, shiftCount;
     uint64_t aSig, savedASig;
     int32_t z;
 
     if (floatx80_invalid_encoding(a)) {
         float_raise(float_flag_invalid STATUS_VAR);
         return (int32)(1U << 31);
     }
     aSig = extractFloatx80Frac( a );
     aExp = extractFloatx80Exp( a );
     aSign = extractFloatx80Sign( a );
     if ( 0x401E < aExp ) {
         if ( ( aExp == 0x7FFF ) && (uint64_t) ( aSig<<1 ) ) aSign = 0;
         goto invalid;
     }
     else if ( aExp < 0x3FFF ) {
         if ( aExp || aSig ) STATUS(float_exception_flags) |= float_flag_inexact;
         return 0;
     }
     shiftCount = 0x403E - aExp;
     savedASig = aSig;
     aSig >>= shiftCount;
     z = (int32_t)aSig;
-    if ( aSign ) z = - z;
+    if ( aSign && (z != 0x80000000) ) z = - z;
     if ( ( z < 0 ) ^ aSign ) {
  invalid:
         float_raise( float_flag_invalid STATUS_VAR);
         return aSign ? (int32_t) 0x80000000 : 0x7FFFFFFF;
     }
     if ( ( aSig<<shiftCount ) != savedASig ) {
         STATUS(float_exception_flags) |= float_flag_inexact;
     }
     return z;
 
 }
 
 /*----------------------------------------------------------------------------
 | Returns the result of converting the extended double-precision floating-
 | point value `a' to the 64-bit two's complement integer format.  The
 | conversion is performed according to the IEC/IEEE Standard for Binary
 | Floating-Point Arithmetic---which means in particular that the conversion
 | is rounded according to the current rounding mode.  If `a' is a NaN,
 | the largest positive integer is returned.  Otherwise, if the conversion
 | overflows, the largest integer with the same sign as `a' is returned.
 *----------------------------------------------------------------------------*/
diff --git a/qemu/target-arm/internals.h b/qemu/target-arm/internals.h
index 8b639ae4..c1ad7574 100644
--- a/qemu/target-arm/internals.h
+++ b/qemu/target-arm/internals.h
@@ -250,12 +250,12 @@ static inline uint32_t syn_aa32_smc(void)
 
 static inline uint32_t syn_aa64_bkpt(uint32_t imm16)
 {
-    return (EC_AA64_BKPT << ARM_EL_EC_SHIFT) | ARM_EL_IL | (imm16 & 0xffff);
+    return (((unsigned int)EC_AA64_BKPT) << ARM_EL_EC_SHIFT) | ARM_EL_IL | (imm16 & 0xffff);
 }
 
 static inline uint32_t syn_aa32_bkpt(uint32_t imm16, bool is_thumb)
 {
-    return (EC_AA32_BKPT << ARM_EL_EC_SHIFT) | (imm16 & 0xffff)
+    return (((unsigned int)EC_AA32_BKPT) << ARM_EL_EC_SHIFT) | (imm16 & 0xffff)
         | (is_thumb ? 0 : ARM_EL_IL);
 }
 
@@ -324,28 +324,28 @@ static inline uint32_t syn_insn_abort(int same_el, int ea, int s1ptw, int fsc)
 static inline uint32_t syn_data_abort(int same_el, int ea, int cm, int s1ptw,
                                       int wnr, int fsc)
 {
-    return (EC_DATAABORT << ARM_EL_EC_SHIFT) | (same_el << ARM_EL_EC_SHIFT)
+    return (((unsigned int) EC_DATAABORT) << ARM_EL_EC_SHIFT) | (same_el << ARM_EL_EC_SHIFT)
         | (ea << 9) | (cm << 8) | (s1ptw << 7) | (wnr << 6) | fsc;
 }
 
 static inline uint32_t syn_swstep(int same_el, int isv, int ex)
 {
-    return (EC_SOFTWARESTEP << ARM_EL_EC_SHIFT) | (same_el << ARM_EL_EC_SHIFT)
+    return (((unsigned int)EC_SOFTWARESTEP) << ARM_EL_EC_SHIFT) | (same_el << ARM_EL_EC_SHIFT)
         | (isv << 24) | (ex << 6) | 0x22;
 }
 
 static inline uint32_t syn_watchpoint(int same_el, int cm, int wnr)
 {
-    return (EC_WATCHPOINT << ARM_EL_EC_SHIFT) | (same_el << ARM_EL_EC_SHIFT)
+    return (((unsigned int)EC_WATCHPOINT) << ARM_EL_EC_SHIFT) | (same_el << ARM_EL_EC_SHIFT)
         | (cm << 8) | (wnr << 6) | 0x22;
 }
 
 static inline uint32_t syn_breakpoint(int same_el)
 {
-    return (EC_BREAKPOINT << ARM_EL_EC_SHIFT) | (same_el << ARM_EL_EC_SHIFT)
+    return (((unsigned int) EC_BREAKPOINT) << ARM_EL_EC_SHIFT) | (same_el << ARM_EL_EC_SHIFT)
         | ARM_EL_IL | 0x22;
 }
 
 /* Update a QEMU watchpoint based on the information the guest has set in the
  * DBGWCR<n>_EL1 and DBGWVR<n>_EL1 registers.
  */
diff --git a/qemu/target-arm/neon_helper.c b/qemu/target-arm/neon_helper.c
index 1426decd..a1f4af7a 100644
--- a/qemu/target-arm/neon_helper.c
+++ b/qemu/target-arm/neon_helper.c
@@ -1013,63 +1013,63 @@ uint32_t HELPER(neon_qrshl_u32)(CPUARMState *env, uint32_t val, uint32_t shiftop
 /* Handling addition overflow with 64 bit input values is more
  * tricky than with 32 bit values.  */
 uint64_t HELPER(neon_qrshl_u64)(CPUARMState *env, uint64_t val, uint64_t shiftop)
 {
     int8_t shift = (int8_t)shiftop;
     if (shift >= 64) {
         if (val) {
             SET_QC();
             val = ~0;
         }
     } else if (shift < -64) {
         val = 0;
     } else if (shift == -64) {
         val >>= 63;
     } else if (shift < 0) {
         val >>= (-shift - 1);
         if (val == UINT64_MAX) {
             /* In this case, it means that the rounding constant is 1,
              * and the addition would overflow. Return the actual
              * result directly.  */
             val = 0x8000000000000000ULL;
         } else {
             val++;
             val >>= 1;
         }
     } else { \
         uint64_t tmp = val;
         val <<= shift;
         if ((val >> shift) != tmp) {
             SET_QC();
             val = ~0;
         }
     }
     return val;
 }
 
 #define NEON_FN(dest, src1, src2) do { \
     int8_t tmp; \
     tmp = (int8_t)src2; \
     if (tmp >= (ssize_t)sizeof(src1) * 8) { \
         if (src1) { \
             SET_QC(); \
             dest = (1 << (sizeof(src1) * 8 - 1)); \
             if (src1 > 0) { \
                 dest--; \
             } \
         } else { \
             dest = 0; \
         } \
     } else if (tmp <= -(ssize_t)sizeof(src1) * 8) { \
         dest = 0; \
     } else if (tmp < 0) { \
         dest = (src1 + (1 << (-1 - tmp))) >> -tmp; \
     } else { \
-        dest = src1 << tmp; \
+        dest = ((uint64_t)src1) << tmp; \
         if ((dest >> tmp) != src1) { \
             SET_QC(); \
             dest = (uint32_t)(1 << (sizeof(src1) * 8 - 1)); \
             if (src1 > 0) { \
                 dest--; \
             } \
         } \
     }} while (0)
diff --git a/qemu/target-arm/translate.c b/qemu/target-arm/translate.c
index 31f0327a..b4800710 100644
--- a/qemu/target-arm/translate.c
+++ b/qemu/target-arm/translate.c
@@ -7691,1448 +7691,1448 @@ static void gen_srs(DisasContext *s,
 static void disas_arm_insn(DisasContext *s, unsigned int insn)  // qq
 {
     TCGContext *tcg_ctx = s->uc->tcg_ctx;
     unsigned int cond, val, op1, i, shift, rm, rs, rn, rd, sh;
     TCGv_i32 tmp;
     TCGv_i32 tmp2;
     TCGv_i32 tmp3;
     TCGv_i32 addr;
     TCGv_i64 tmp64;
 
     /* M variants do not implement ARM mode.  */
     if (arm_dc_feature(s, ARM_FEATURE_M)) {
         goto illegal_op;
     }
 
     // Unicorn: trace this instruction on request
     if (HOOK_EXISTS_BOUNDED(s->uc, UC_HOOK_CODE, s->pc - 4)) {
         gen_uc_tracecode(tcg_ctx, 4, UC_HOOK_CODE_IDX, s->uc, s->pc - 4);
         // the callback might want to stop emulation immediately
         check_exit_request(tcg_ctx);
     }
 
     cond = insn >> 28;
     if (cond == 0xf){
         /* In ARMv3 and v4 the NV condition is UNPREDICTABLE; we
          * choose to UNDEF. In ARMv5 and above the space is used
          * for miscellaneous unconditional instructions.
          */
         ARCH(5);
 
         /* Unconditional instructions.  */
         if (((insn >> 25) & 7) == 1) {
             /* NEON Data processing.  */
             if (!arm_dc_feature(s, ARM_FEATURE_NEON)) {
                 goto illegal_op;
             }
 
             if (disas_neon_data_insn(s, insn)) {
                 goto illegal_op;
             }
             return;
         }
         if ((insn & 0x0f100000) == 0x04000000) {
             /* NEON load/store.  */
             if (!arm_dc_feature(s, ARM_FEATURE_NEON)) {
                 goto illegal_op;
             }
 
             if (disas_neon_ls_insn(s, insn)) {
                 goto illegal_op;
             }
             return;
         }
         if ((insn & 0x0f000e10) == 0x0e000a00) {
             /* VFP.  */
             if (disas_vfp_insn(s, insn)) {
                 goto illegal_op;
             }
             return;
         }
         if (((insn & 0x0f30f000) == 0x0510f000) ||
             ((insn & 0x0f30f010) == 0x0710f000)) {
             if ((insn & (1 << 22)) == 0) {
                 /* PLDW; v7MP */
                 if (!arm_dc_feature(s, ARM_FEATURE_V7MP)) {
                     goto illegal_op;
                 }
             }
             /* Otherwise PLD; v5TE+ */
             ARCH(5TE);
             return;
         }
         if (((insn & 0x0f70f000) == 0x0450f000) ||
             ((insn & 0x0f70f010) == 0x0650f000)) {
             ARCH(7);
             return; /* PLI; V7 */
         }
         if (((insn & 0x0f700000) == 0x04100000) ||
             ((insn & 0x0f700010) == 0x06100000)) {
             if (!arm_dc_feature(s, ARM_FEATURE_V7MP)) {
                 goto illegal_op;
             }
             return; /* v7MP: Unallocated memory hint: must NOP */
         }
 
         if ((insn & 0x0ffffdff) == 0x01010000) {
             ARCH(6);
             /* setend */
             if (((insn >> 9) & 1) != s->bswap_code) {
                 /* Dynamic endianness switching not implemented. */
                 qemu_log_mask(LOG_UNIMP, "arm: unimplemented setend\n");
                 goto illegal_op;
             }
             return;
         } else if ((insn & 0x0fffff00) == 0x057ff000) {
             switch ((insn >> 4) & 0xf) {
             case 1: /* clrex */
                 ARCH(6K);
                 gen_clrex(s);
                 return;
             case 4: /* dsb */
             case 5: /* dmb */
             case 6: /* isb */
                 ARCH(7);
                 /* We don't emulate caches so these are a no-op.  */
                 return;
             default:
                 goto illegal_op;
             }
         } else if ((insn & 0x0e5fffe0) == 0x084d0500) {
             /* srs */
             if (IS_USER(s)) {
                 goto illegal_op;
             }
             ARCH(6);
             gen_srs(s, (insn & 0x1f), (insn >> 23) & 3, insn & (1 << 21));
             return;
         } else if ((insn & 0x0e50ffe0) == 0x08100a00) {
             /* rfe */
             int32_t offset;
             if (IS_USER(s))
                 goto illegal_op;
             ARCH(6);
             rn = (insn >> 16) & 0xf;
             addr = load_reg(s, rn);
             i = (insn >> 23) & 3;
             switch (i) {
             case 0: offset = -4; break; /* DA */
             case 1: offset = 0; break; /* IA */
             case 2: offset = -8; break; /* DB */
             case 3: offset = 4; break; /* IB */
             default: abort();
             }
             if (offset)
                 tcg_gen_addi_i32(tcg_ctx, addr, addr, offset);
             /* Load PC into tmp and CPSR into tmp2.  */
             tmp = tcg_temp_new_i32(tcg_ctx);
             gen_aa32_ld32u(s, tmp, addr, get_mem_index(s));
             tcg_gen_addi_i32(tcg_ctx, addr, addr, 4);
             tmp2 = tcg_temp_new_i32(tcg_ctx);
             gen_aa32_ld32u(s, tmp2, addr, get_mem_index(s));
             if (insn & (1 << 21)) {
                 /* Base writeback.  */
                 switch (i) {
                 case 0: offset = -8; break;
                 case 1: offset = 4; break;
                 case 2: offset = -4; break;
                 case 3: offset = 0; break;
                 default: abort();
                 }
                 if (offset)
                     tcg_gen_addi_i32(tcg_ctx, addr, addr, offset);
                 store_reg(s, rn, addr);
             } else {
                 tcg_temp_free_i32(tcg_ctx, addr);
             }
             gen_rfe(s, tmp, tmp2);
             return;
         } else if ((insn & 0x0e000000) == 0x0a000000) {
             /* branch link and change to thumb (blx <offset>) */
             int32_t offset;
 
             val = (uint32_t)s->pc;
             tmp = tcg_temp_new_i32(tcg_ctx);
             tcg_gen_movi_i32(tcg_ctx, tmp, val);
             store_reg(s, 14, tmp);
             /* Sign-extend the 24-bit offset */
             offset = ((int32_t)(insn << 8)) >> 8;
             /* offset * 4 + bit24 * 2 + (thumb bit) */
-            val += (offset << 2) | ((insn >> 23) & 2) | 1;
+            val += (((uint32_t)offset) << 2) | ((insn >> 23) & 2) | 1;
             /* pipeline offset */
             val += 4;
             /* protected by ARCH(5); above, near the start of uncond block */
             gen_bx_im(s, val);
             return;
         } else if ((insn & 0x0e000f00) == 0x0c000100) {
             if (arm_dc_feature(s, ARM_FEATURE_IWMMXT)) {
                 /* iWMMXt register transfer.  */
                 if (extract32(s->c15_cpar, 1, 1)) {
                     if (!disas_iwmmxt_insn(s, insn)) {
                         return;
                     }
                 }
             }
         } else if ((insn & 0x0fe00000) == 0x0c400000) {
             /* Coprocessor double register transfer.  */
             ARCH(5TE);
         } else if ((insn & 0x0f000010) == 0x0e000010) {
             /* Additional coprocessor register transfer.  */
         } else if ((insn & 0x0ff10020) == 0x01000000) {
             uint32_t mask;
             uint32_t val;
             /* cps (privileged) */
             if (IS_USER(s))
                 return;
             mask = val = 0;
             if (insn & (1 << 19)) {
                 if (insn & (1 << 8))
                     mask |= CPSR_A;
                 if (insn & (1 << 7))
                     mask |= CPSR_I;
                 if (insn & (1 << 6))
                     mask |= CPSR_F;
                 if (insn & (1 << 18))
                     val |= mask;
             }
             if (insn & (1 << 17)) {
                 mask |= CPSR_M;
                 val |= (insn & 0x1f);
             }
             if (mask) {
                 gen_set_psr_im(s, mask, 0, val);
             }
             return;
         }
         goto illegal_op;
     }
     if (cond != 0xe) {
         /* if not always execute, we generate a conditional jump to
            next instruction */
         s->condlabel = gen_new_label(tcg_ctx);
         arm_gen_test_cc(tcg_ctx, cond ^ 1, s->condlabel);
         s->condjmp = 1;
     }
     if ((insn & 0x0f900000) == 0x03000000) {
         if ((insn & (1 << 21)) == 0) {
             ARCH(6T2);
             rd = (insn >> 12) & 0xf;
             val = ((insn >> 4) & 0xf000) | (insn & 0xfff);
             if ((insn & (1 << 22)) == 0) {
                 /* MOVW */
                 tmp = tcg_temp_new_i32(tcg_ctx);
                 tcg_gen_movi_i32(tcg_ctx, tmp, val);
             } else {
                 /* MOVT */
                 tmp = load_reg(s, rd);
                 tcg_gen_ext16u_i32(tcg_ctx, tmp, tmp);
                 tcg_gen_ori_i32(tcg_ctx, tmp, tmp, val << 16);
             }
             store_reg(s, rd, tmp);
         } else {
             if (((insn >> 12) & 0xf) != 0xf)
                 goto illegal_op;
             if (((insn >> 16) & 0xf) == 0) {
                 gen_nop_hint(s, insn & 0xff);
             } else {
                 /* CPSR = immediate */
                 val = insn & 0xff;
                 shift = ((insn >> 8) & 0xf) * 2;
                 if (shift)
                     val = (val >> shift) | (val << (32 - shift));
                 i = ((insn & (1 << 22)) != 0);
                 if (gen_set_psr_im(s, msr_mask(s, (insn >> 16) & 0xf, i),
                                    i, val)) {
                     goto illegal_op;
                 }
             }
         }
     } else if ((insn & 0x0f900000) == 0x01000000
                && (insn & 0x00000090) != 0x00000090) {
         /* miscellaneous instructions */
         op1 = (insn >> 21) & 3;
         sh = (insn >> 4) & 0xf;
         rm = insn & 0xf;
         switch (sh) {
         case 0x0: /* move program status register */
             if (op1 & 1) {
                 /* PSR = reg */
                 tmp = load_reg(s, rm);
                 i = ((op1 & 2) != 0);
                 if (gen_set_psr(s, msr_mask(s, (insn >> 16) & 0xf, i), i, tmp))
                     goto illegal_op;
             } else {
                 /* reg = PSR */
                 rd = (insn >> 12) & 0xf;
                 if (op1 & 2) {
                     if (IS_USER(s))
                         goto illegal_op;
                     tmp = load_cpu_field(s->uc, spsr);
                 } else {
                     tmp = tcg_temp_new_i32(tcg_ctx);
                     gen_helper_cpsr_read(tcg_ctx, tmp, tcg_ctx->cpu_env);
                 }
                 store_reg(s, rd, tmp);
             }
             break;
         case 0x1:
             if (op1 == 1) {
                 /* branch/exchange thumb (bx).  */
                 ARCH(4T);
                 tmp = load_reg(s, rm);
                 gen_bx(s, tmp);
             } else if (op1 == 3) {
                 /* clz */
                 ARCH(5);
                 rd = (insn >> 12) & 0xf;
                 tmp = load_reg(s, rm);
                 gen_helper_clz(tcg_ctx, tmp, tmp);
                 store_reg(s, rd, tmp);
             } else {
                 goto illegal_op;
             }
             break;
         case 0x2:
             if (op1 == 1) {
                 ARCH(5J); /* bxj */
                 /* Trivial implementation equivalent to bx.  */
                 tmp = load_reg(s, rm);
                 gen_bx(s, tmp);
             } else {
                 goto illegal_op;
             }
             break;
         case 0x3:
             if (op1 != 1)
               goto illegal_op;
 
             ARCH(5);
             /* branch link/exchange thumb (blx) */
             tmp = load_reg(s, rm);
             tmp2 = tcg_temp_new_i32(tcg_ctx);
             tcg_gen_movi_i32(tcg_ctx, tmp2, s->pc);
             store_reg(s, 14, tmp2);
             gen_bx(s, tmp);
             break;
         case 0x4:
         {
             /* crc32/crc32c */
             uint32_t c = extract32(insn, 8, 4);
 
             /* Check this CPU supports ARMv8 CRC instructions.
              * op1 == 3 is UNPREDICTABLE but handle as UNDEFINED.
              * Bits 8, 10 and 11 should be zero.
              */
             if (!arm_dc_feature(s, ARM_FEATURE_CRC) || op1 == 0x3 ||
                 (c & 0xd) != 0) {
                 goto illegal_op;
             }
 
             rn = extract32(insn, 16, 4);
             rd = extract32(insn, 12, 4);
 
             tmp = load_reg(s, rn);
             tmp2 = load_reg(s, rm);
             if (op1 == 0) {
                 tcg_gen_andi_i32(tcg_ctx, tmp2, tmp2, 0xff);
             } else if (op1 == 1) {
                 tcg_gen_andi_i32(tcg_ctx, tmp2, tmp2, 0xffff);
             }
             tmp3 = tcg_const_i32(tcg_ctx, 1 << op1);
             if (c & 0x2) {
                 gen_helper_crc32c(tcg_ctx, tmp, tmp, tmp2, tmp3);
             } else {
                 gen_helper_crc32(tcg_ctx, tmp, tmp, tmp2, tmp3);
             }
             tcg_temp_free_i32(tcg_ctx, tmp2);
             tcg_temp_free_i32(tcg_ctx, tmp3);
             store_reg(s, rd, tmp);
             break;
         }
         case 0x5: /* saturating add/subtract */
             ARCH(5TE);
             rd = (insn >> 12) & 0xf;
             rn = (insn >> 16) & 0xf;
             tmp = load_reg(s, rm);
             tmp2 = load_reg(s, rn);
             if (op1 & 2)
                 gen_helper_double_saturate(tcg_ctx, tmp2, tcg_ctx->cpu_env, tmp2);
             if (op1 & 1)
                 gen_helper_sub_saturate(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
             else
                 gen_helper_add_saturate(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
             tcg_temp_free_i32(tcg_ctx, tmp2);
             store_reg(s, rd, tmp);
             break;
         case 7:
         {
             int imm16 = extract32(insn, 0, 4) | (extract32(insn, 8, 12) << 4);
             switch (op1) {
             case 1:
                 /* bkpt */
                 ARCH(5);
                 gen_exception_insn(s, 4, EXCP_BKPT,
                                    syn_aa32_bkpt(imm16, false));
                 break;
             case 2:
                 /* Hypervisor call (v7) */
                 ARCH(7);
                 if (IS_USER(s)) {
                     goto illegal_op;
                 }
                 gen_hvc(s, imm16);
                 break;
             case 3:
                 /* Secure monitor call (v6+) */
                 ARCH(6K);
                 if (IS_USER(s)) {
                     goto illegal_op;
                 }
                 gen_smc(s);
                 break;
             default:
                 goto illegal_op;
             }
             break;
         }
         case 0x8: /* signed multiply */
         case 0xa:
         case 0xc:
         case 0xe:
             ARCH(5TE);
             rs = (insn >> 8) & 0xf;
             rn = (insn >> 12) & 0xf;
             rd = (insn >> 16) & 0xf;
             if (op1 == 1) {
                 /* (32 * 16) >> 16 */
                 tmp = load_reg(s, rm);
                 tmp2 = load_reg(s, rs);
                 if (sh & 4)
                     tcg_gen_sari_i32(tcg_ctx, tmp2, tmp2, 16);
                 else
                     gen_sxth(tmp2);
                 tmp64 = gen_muls_i64_i32(s, tmp, tmp2);
                 tcg_gen_shri_i64(tcg_ctx, tmp64, tmp64, 16);
                 tmp = tcg_temp_new_i32(tcg_ctx);
                 tcg_gen_trunc_i64_i32(tcg_ctx, tmp, tmp64);
                 tcg_temp_free_i64(tcg_ctx, tmp64);
                 if ((sh & 2) == 0) {
                     tmp2 = load_reg(s, rn);
                     gen_helper_add_setq(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
                     tcg_temp_free_i32(tcg_ctx, tmp2);
                 }
                 store_reg(s, rd, tmp);
             } else {
                 /* 16 * 16 */
                 tmp = load_reg(s, rm);
                 tmp2 = load_reg(s, rs);
                 gen_mulxy(s, tmp, tmp2, sh & 2, sh & 4);
                 tcg_temp_free_i32(tcg_ctx, tmp2);
                 if (op1 == 2) {
                     tmp64 = tcg_temp_new_i64(tcg_ctx);
                     tcg_gen_ext_i32_i64(tcg_ctx, tmp64, tmp);
                     tcg_temp_free_i32(tcg_ctx, tmp);
                     gen_addq(s, tmp64, rn, rd);
                     gen_storeq_reg(s, rn, rd, tmp64);
                     tcg_temp_free_i64(tcg_ctx, tmp64);
                 } else {
                     if (op1 == 0) {
                         tmp2 = load_reg(s, rn);
                         gen_helper_add_setq(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
                         tcg_temp_free_i32(tcg_ctx, tmp2);
                     }
                     store_reg(s, rd, tmp);
                 }
             }
             break;
         default:
             goto illegal_op;
         }
     } else if (((insn & 0x0e000000) == 0 &&
                 (insn & 0x00000090) != 0x90) ||
                ((insn & 0x0e000000) == (1 << 25))) {
         int set_cc, logic_cc, shiftop;
 
         op1 = (insn >> 21) & 0xf;
         set_cc = (insn >> 20) & 1;
         logic_cc = table_logic_cc[op1] & set_cc;
 
         /* data processing instruction */
         if (insn & (1 << 25)) {
             /* immediate operand */
             val = insn & 0xff;
             shift = ((insn >> 8) & 0xf) * 2;
             if (shift) {
                 val = (val >> shift) | (val << (32 - shift));
             }
             tmp2 = tcg_temp_new_i32(tcg_ctx);
             tcg_gen_movi_i32(tcg_ctx, tmp2, val);
             if (logic_cc && shift) {
                 gen_set_CF_bit31(s, tmp2);
             }
         } else {
             /* register */
             rm = (insn) & 0xf;
             tmp2 = load_reg(s, rm);
             shiftop = (insn >> 5) & 3;
             if (!(insn & (1 << 4))) {
                 shift = (insn >> 7) & 0x1f;
                 gen_arm_shift_im(s, tmp2, shiftop, shift, logic_cc);
             } else {
                 rs = (insn >> 8) & 0xf;
                 tmp = load_reg(s, rs);
                 gen_arm_shift_reg(s, tmp2, shiftop, tmp, logic_cc);
             }
         }
         if (op1 != 0x0f && op1 != 0x0d) {
             rn = (insn >> 16) & 0xf;
             tmp = load_reg(s, rn);
         } else {
             TCGV_UNUSED_I32(tmp);
         }
         rd = (insn >> 12) & 0xf;
         switch(op1) {
         case 0x00:
             tcg_gen_and_i32(tcg_ctx, tmp, tmp, tmp2);
             if (logic_cc) {
                 gen_logic_CC(s, tmp);
             }
             store_reg_bx(s, rd, tmp);
             break;
         case 0x01:
             tcg_gen_xor_i32(tcg_ctx, tmp, tmp, tmp2);
             if (logic_cc) {
                 gen_logic_CC(s, tmp);
             }
             store_reg_bx(s, rd, tmp);
             break;
         case 0x02:
             if (set_cc && rd == 15) {
                 /* SUBS r15, ... is used for exception return.  */
                 if (IS_USER(s)) {
                     goto illegal_op;
                 }
                 gen_sub_CC(s, tmp, tmp, tmp2);
                 gen_exception_return(s, tmp);
             } else {
                 if (set_cc) {
                     gen_sub_CC(s, tmp, tmp, tmp2);
                 } else {
                     tcg_gen_sub_i32(tcg_ctx, tmp, tmp, tmp2);
                 }
                 store_reg_bx(s, rd, tmp);
             }
             break;
         case 0x03:
             if (set_cc) {
                 gen_sub_CC(s, tmp, tmp2, tmp);
             } else {
                 tcg_gen_sub_i32(tcg_ctx, tmp, tmp2, tmp);
             }
             store_reg_bx(s, rd, tmp);
             break;
         case 0x04:
             if (set_cc) {
                 gen_add_CC(s, tmp, tmp, tmp2);
             } else {
                 tcg_gen_add_i32(tcg_ctx, tmp, tmp, tmp2);
             }
             store_reg_bx(s, rd, tmp);
             break;
         case 0x05:
             if (set_cc) {
                 gen_adc_CC(s, tmp, tmp, tmp2);
             } else {
                 gen_add_carry(s, tmp, tmp, tmp2);
             }
             store_reg_bx(s, rd, tmp);
             break;
         case 0x06:
             if (set_cc) {
                 gen_sbc_CC(s, tmp, tmp, tmp2);
             } else {
                 gen_sub_carry(s, tmp, tmp, tmp2);
             }
             store_reg_bx(s, rd, tmp);
             break;
         case 0x07:
             if (set_cc) {
                 gen_sbc_CC(s, tmp, tmp2, tmp);
             } else {
                 gen_sub_carry(s, tmp, tmp2, tmp);
             }
             store_reg_bx(s, rd, tmp);
             break;
         case 0x08:
             if (set_cc) {
                 tcg_gen_and_i32(tcg_ctx, tmp, tmp, tmp2);
                 gen_logic_CC(s, tmp);
             }
             tcg_temp_free_i32(tcg_ctx, tmp);
             break;
         case 0x09:
             if (set_cc) {
                 tcg_gen_xor_i32(tcg_ctx, tmp, tmp, tmp2);
                 gen_logic_CC(s, tmp);
             }
             tcg_temp_free_i32(tcg_ctx, tmp);
             break;
         case 0x0a:
             if (set_cc) {
                 gen_sub_CC(s, tmp, tmp, tmp2);
             }
             tcg_temp_free_i32(tcg_ctx, tmp);
             break;
         case 0x0b:
             if (set_cc) {
                 gen_add_CC(s, tmp, tmp, tmp2);
             }
             tcg_temp_free_i32(tcg_ctx, tmp);
             break;
         case 0x0c:
             tcg_gen_or_i32(tcg_ctx, tmp, tmp, tmp2);
             if (logic_cc) {
                 gen_logic_CC(s, tmp);
             }
             store_reg_bx(s, rd, tmp);
             break;
         case 0x0d:
             if (logic_cc && rd == 15) {
                 /* MOVS r15, ... is used for exception return.  */
                 if (IS_USER(s)) {
                     goto illegal_op;
                 }
                 gen_exception_return(s, tmp2);
             } else {
                 if (logic_cc) {
                     gen_logic_CC(s, tmp2);
                 }
                 store_reg_bx(s, rd, tmp2);
             }
             break;
         case 0x0e:
             tcg_gen_andc_i32(tcg_ctx, tmp, tmp, tmp2);
             if (logic_cc) {
                 gen_logic_CC(s, tmp);
             }
             store_reg_bx(s, rd, tmp);
             break;
         default:
         case 0x0f:
             tcg_gen_not_i32(tcg_ctx, tmp2, tmp2);
             if (logic_cc) {
                 gen_logic_CC(s, tmp2);
             }
             store_reg_bx(s, rd, tmp2);
             break;
         }
         if (op1 != 0x0f && op1 != 0x0d) {
             tcg_temp_free_i32(tcg_ctx, tmp2);
         }
     } else {
         /* other instructions */
         op1 = (insn >> 24) & 0xf;
         switch(op1) {
         case 0x0:
         case 0x1:
             /* multiplies, extra load/stores */
             sh = (insn >> 5) & 3;
             if (sh == 0) {
                 if (op1 == 0x0) {
                     rd = (insn >> 16) & 0xf;
                     rn = (insn >> 12) & 0xf;
                     rs = (insn >> 8) & 0xf;
                     rm = (insn) & 0xf;
                     op1 = (insn >> 20) & 0xf;
                     switch (op1) {
                     case 0: case 1: case 2: case 3: case 6:
                         /* 32 bit mul */
                         tmp = load_reg(s, rs);
                         tmp2 = load_reg(s, rm);
                         tcg_gen_mul_i32(tcg_ctx, tmp, tmp, tmp2);
                         tcg_temp_free_i32(tcg_ctx, tmp2);
                         if (insn & (1 << 22)) {
                             /* Subtract (mls) */
                             ARCH(6T2);
                             tmp2 = load_reg(s, rn);
                             tcg_gen_sub_i32(tcg_ctx, tmp, tmp2, tmp);
                             tcg_temp_free_i32(tcg_ctx, tmp2);
                         } else if (insn & (1 << 21)) {
                             /* Add */
                             tmp2 = load_reg(s, rn);
                             tcg_gen_add_i32(tcg_ctx, tmp, tmp, tmp2);
                             tcg_temp_free_i32(tcg_ctx, tmp2);
                         }
                         if (insn & (1 << 20))
                             gen_logic_CC(s, tmp);
                         store_reg(s, rd, tmp);
                         break;
                     case 4:
                         /* 64 bit mul double accumulate (UMAAL) */
                         ARCH(6);
                         tmp = load_reg(s, rs);
                         tmp2 = load_reg(s, rm);
                         tmp64 = gen_mulu_i64_i32(s, tmp, tmp2);
                         gen_addq_lo(s, tmp64, rn);
                         gen_addq_lo(s, tmp64, rd);
                         gen_storeq_reg(s, rn, rd, tmp64);
                         tcg_temp_free_i64(tcg_ctx, tmp64);
                         break;
                     case 8: case 9: case 10: case 11:
                     case 12: case 13: case 14: case 15:
                         /* 64 bit mul: UMULL, UMLAL, SMULL, SMLAL. */
                         tmp = load_reg(s, rs);
                         tmp2 = load_reg(s, rm);
                         if (insn & (1 << 22)) {
                             tcg_gen_muls2_i32(tcg_ctx, tmp, tmp2, tmp, tmp2);
                         } else {
                             tcg_gen_mulu2_i32(tcg_ctx, tmp, tmp2, tmp, tmp2);
                         }
                         if (insn & (1 << 21)) { /* mult accumulate */
                             TCGv_i32 al = load_reg(s, rn);
                             TCGv_i32 ah = load_reg(s, rd);
                             tcg_gen_add2_i32(tcg_ctx, tmp, tmp2, tmp, tmp2, al, ah);
                             tcg_temp_free_i32(tcg_ctx, al);
                             tcg_temp_free_i32(tcg_ctx, ah);
                         }
                         if (insn & (1 << 20)) {
                             gen_logicq_cc(s, tmp, tmp2);
                         }
                         store_reg(s, rn, tmp);
                         store_reg(s, rd, tmp2);
                         break;
                     default:
                         goto illegal_op;
                     }
                 } else {
                     rn = (insn >> 16) & 0xf;
                     rd = (insn >> 12) & 0xf;
                     if (insn & (1 << 23)) {
                         /* load/store exclusive */
                         int op2 = (insn >> 8) & 3;
                         op1 = (insn >> 21) & 0x3;
 
                         switch (op2) {
                         case 0: /* lda/stl */
                             if (op1 == 1) {
                                 goto illegal_op;
                             }
                             ARCH(8);
                             break;
                         case 1: /* reserved */
                             goto illegal_op;
                         case 2: /* ldaex/stlex */
                             ARCH(8);
                             break;
                         case 3: /* ldrex/strex */
                             if (op1) {
                                 ARCH(6K);
                             } else {
                                 ARCH(6);
                             }
                             break;
                         }
 
                         addr = tcg_temp_local_new_i32(tcg_ctx);
                         load_reg_var(s, addr, rn);
 
                         /* Since the emulation does not have barriers,
                            the acquire/release semantics need no special
                            handling */
                         if (op2 == 0) {
                             if (insn & (1 << 20)) {
                                 tmp = tcg_temp_new_i32(tcg_ctx);
                                 switch (op1) {
                                 case 0: /* lda */
                                     gen_aa32_ld32u(s, tmp, addr, get_mem_index(s));
                                     break;
                                 case 2: /* ldab */
                                     gen_aa32_ld8u(s, tmp, addr, get_mem_index(s));
                                     break;
                                 case 3: /* ldah */
                                     gen_aa32_ld16u(s, tmp, addr, get_mem_index(s));
                                     break;
                                 default:
                                     abort();
                                 }
                                 store_reg(s, rd, tmp);
                             } else {
                                 rm = insn & 0xf;
                                 tmp = load_reg(s, rm);
                                 switch (op1) {
                                 case 0: /* stl */
                                     gen_aa32_st32(s, tmp, addr, get_mem_index(s));
                                     break;
                                 case 2: /* stlb */
                                     gen_aa32_st8(s, tmp, addr, get_mem_index(s));
                                     break;
                                 case 3: /* stlh */
                                     gen_aa32_st16(s, tmp, addr, get_mem_index(s));
                                     break;
                                 default:
                                     abort();
                                 }
                                 tcg_temp_free_i32(tcg_ctx, tmp);
                             }
                         } else if (insn & (1 << 20)) {
                             switch (op1) {
                             case 0: /* ldrex */
                                 gen_load_exclusive(s, rd, 15, addr, 2);
                                 break;
                             case 1: /* ldrexd */
                                 gen_load_exclusive(s, rd, rd + 1, addr, 3);
                                 break;
                             case 2: /* ldrexb */
                                 gen_load_exclusive(s, rd, 15, addr, 0);
                                 break;
                             case 3: /* ldrexh */
                                 gen_load_exclusive(s, rd, 15, addr, 1);
                                 break;
                             default:
                                 abort();
                             }
                         } else {
                             rm = insn & 0xf;
                             switch (op1) {
                             case 0:  /*  strex */
                                 gen_store_exclusive(s, rd, rm, 15, addr, 2);
                                 break;
                             case 1: /*  strexd */
                                 gen_store_exclusive(s, rd, rm, rm + 1, addr, 3);
                                 break;
                             case 2: /*  strexb */
                                 gen_store_exclusive(s, rd, rm, 15, addr, 0);
                                 break;
                             case 3: /* strexh */
                                 gen_store_exclusive(s, rd, rm, 15, addr, 1);
                                 break;
                             default:
                                 abort();
                             }
                         }
                         tcg_temp_free_i32(tcg_ctx, addr);
                     } else {
                         /* SWP instruction */
                         rm = (insn) & 0xf;
 
                         /* ??? This is not really atomic.  However we know
                            we never have multiple CPUs running in parallel,
                            so it is good enough.  */
                         addr = load_reg(s, rn);
                         tmp = load_reg(s, rm);
                         tmp2 = tcg_temp_new_i32(tcg_ctx);
                         if (insn & (1 << 22)) {
                             gen_aa32_ld8u(s, tmp2, addr, get_mem_index(s));
                             gen_aa32_st8(s, tmp, addr, get_mem_index(s));
                         } else {
                             gen_aa32_ld32u(s, tmp2, addr, get_mem_index(s));
                             gen_aa32_st32(s, tmp, addr, get_mem_index(s));
                         }
                         tcg_temp_free_i32(tcg_ctx, tmp);
                         tcg_temp_free_i32(tcg_ctx, addr);
                         store_reg(s, rd, tmp2);
                     }
                 }
             } else {
                 int address_offset;
                 int load = insn & (1 << 20);
                 int wbit = insn & (1 << 21);
                 int pbit = insn & (1 << 24);
                 int doubleword = 0;
                 /* Misc load/store */
                 rn = (insn >> 16) & 0xf;
                 rd = (insn >> 12) & 0xf;
                 if (!load && (sh & 2)) {
                     /* doubleword */
                     ARCH(5TE);
                     if (rd & 1) {
                         /* UNPREDICTABLE; we choose to UNDEF */
                         goto illegal_op;
                     }
                     load = (sh & 1) == 0;
                     doubleword = 1;
                 }
                 addr = load_reg(s, rn);
                 if (pbit)
                     gen_add_datah_offset(s, insn, 0, addr);
                 address_offset = 0;
                 if (doubleword) {
                     if (!load) {
                         /* store */
                         tmp = load_reg(s, rd);
                         gen_aa32_st32(s, tmp, addr, get_mem_index(s));
                         tcg_temp_free_i32(tcg_ctx, tmp);
                         tcg_gen_addi_i32(tcg_ctx, addr, addr, 4);
                         tmp = load_reg(s, rd + 1);
                         gen_aa32_st32(s, tmp, addr, get_mem_index(s));
                         tcg_temp_free_i32(tcg_ctx, tmp);
                     } else {
                         /* load */
                         tmp = tcg_temp_new_i32(tcg_ctx);
                         gen_aa32_ld32u(s, tmp, addr, get_mem_index(s));
                         store_reg(s, rd, tmp);
                         tcg_gen_addi_i32(tcg_ctx, addr, addr, 4);
                         tmp = tcg_temp_new_i32(tcg_ctx);
                         gen_aa32_ld32u(s, tmp, addr, get_mem_index(s));
                         rd++;
                     }
                     address_offset = -4;
                 } else if (load) {
                     /* load */
                     tmp = tcg_temp_new_i32(tcg_ctx);
                     switch(sh) {
                     case 1:
                         gen_aa32_ld16u(s, tmp, addr, get_mem_index(s));
                         break;
                     case 2:
                         gen_aa32_ld8s(s, tmp, addr, get_mem_index(s));
                         break;
                         default:
                     case 3:
                         gen_aa32_ld16s(s, tmp, addr, get_mem_index(s));
                         break;
                     }
                 } else {
                     /* store */
                     tmp = load_reg(s, rd);
                     gen_aa32_st16(s, tmp, addr, get_mem_index(s));
                     tcg_temp_free_i32(tcg_ctx, tmp);
                 }
                 /* Perform base writeback before the loaded value to
                    ensure correct behavior with overlapping index registers.
                    ldrd with base writeback is is undefined if the
                    destination and index registers overlap.  */
                 if (!pbit) {
                     gen_add_datah_offset(s, insn, address_offset, addr);
                     store_reg(s, rn, addr);
                 } else if (wbit) {
                     if (address_offset)
                         tcg_gen_addi_i32(tcg_ctx, addr, addr, address_offset);
                     store_reg(s, rn, addr);
                 } else {
                     tcg_temp_free_i32(tcg_ctx, addr);
                 }
                 if (load) {
                     /* Complete the load.  */
                     store_reg(s, rd, tmp);
                 }
             }
             break;
         case 0x4:
         case 0x5:
             goto do_ldst;
         case 0x6:
         case 0x7:
             if (insn & (1 << 4)) {
                 ARCH(6);
                 /* Armv6 Media instructions.  */
                 rm = insn & 0xf;
                 rn = (insn >> 16) & 0xf;
                 rd = (insn >> 12) & 0xf;
                 rs = (insn >> 8) & 0xf;
                 switch ((insn >> 23) & 3) {
                 case 0: /* Parallel add/subtract.  */
                     op1 = (insn >> 20) & 7;
                     tmp = load_reg(s, rn);
                     tmp2 = load_reg(s, rm);
                     sh = (insn >> 5) & 7;
                     if ((op1 & 3) == 0 || sh == 5 || sh == 6)
                         goto illegal_op;
                     gen_arm_parallel_addsub(s, op1, sh, tmp, tmp2);
                     tcg_temp_free_i32(tcg_ctx, tmp2);
                     store_reg(s, rd, tmp);
                     break;
                 case 1:
                     if ((insn & 0x00700020) == 0) {
                         /* Halfword pack.  */
                         tmp = load_reg(s, rn);
                         tmp2 = load_reg(s, rm);
                         shift = (insn >> 7) & 0x1f;
                         if (insn & (1 << 6)) {
                             /* pkhtb */
                             if (shift == 0)
                                 shift = 31;
                             tcg_gen_sari_i32(tcg_ctx, tmp2, tmp2, shift);
                             tcg_gen_andi_i32(tcg_ctx, tmp, tmp, 0xffff0000);
                             tcg_gen_ext16u_i32(tcg_ctx, tmp2, tmp2);
                         } else {
                             /* pkhbt */
                             if (shift)
                                 tcg_gen_shli_i32(tcg_ctx, tmp2, tmp2, shift);
                             tcg_gen_ext16u_i32(tcg_ctx, tmp, tmp);
                             tcg_gen_andi_i32(tcg_ctx, tmp2, tmp2, 0xffff0000);
                         }
                         tcg_gen_or_i32(tcg_ctx, tmp, tmp, tmp2);
                         tcg_temp_free_i32(tcg_ctx, tmp2);
                         store_reg(s, rd, tmp);
                     } else if ((insn & 0x00200020) == 0x00200000) {
                         /* [us]sat */
                         tmp = load_reg(s, rm);
                         shift = (insn >> 7) & 0x1f;
                         if (insn & (1 << 6)) {
                             if (shift == 0)
                                 shift = 31;
                             tcg_gen_sari_i32(tcg_ctx, tmp, tmp, shift);
                         } else {
                             tcg_gen_shli_i32(tcg_ctx, tmp, tmp, shift);
                         }
                         sh = (insn >> 16) & 0x1f;
                         tmp2 = tcg_const_i32(tcg_ctx, sh);
                         if (insn & (1 << 22))
                           gen_helper_usat(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
                         else
                           gen_helper_ssat(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
                         tcg_temp_free_i32(tcg_ctx, tmp2);
                         store_reg(s, rd, tmp);
                     } else if ((insn & 0x00300fe0) == 0x00200f20) {
                         /* [us]sat16 */
                         tmp = load_reg(s, rm);
                         sh = (insn >> 16) & 0x1f;
                         tmp2 = tcg_const_i32(tcg_ctx, sh);
                         if (insn & (1 << 22))
                           gen_helper_usat16(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
                         else
                           gen_helper_ssat16(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
                         tcg_temp_free_i32(tcg_ctx, tmp2);
                         store_reg(s, rd, tmp);
                     } else if ((insn & 0x00700fe0) == 0x00000fa0) {
                         /* Select bytes.  */
                         tmp = load_reg(s, rn);
                         tmp2 = load_reg(s, rm);
                         tmp3 = tcg_temp_new_i32(tcg_ctx);
                         tcg_gen_ld_i32(tcg_ctx, tmp3, tcg_ctx->cpu_env, offsetof(CPUARMState, GE));
                         gen_helper_sel_flags(tcg_ctx, tmp, tmp3, tmp, tmp2);
                         tcg_temp_free_i32(tcg_ctx, tmp3);
                         tcg_temp_free_i32(tcg_ctx, tmp2);
                         store_reg(s, rd, tmp);
                     } else if ((insn & 0x000003e0) == 0x00000060) {
                         tmp = load_reg(s, rm);
                         shift = (insn >> 10) & 3;
                         /* ??? In many cases it's not necessary to do a
                            rotate, a shift is sufficient.  */
                         if (shift != 0)
                             tcg_gen_rotri_i32(tcg_ctx, tmp, tmp, shift * 8);
                         op1 = (insn >> 20) & 7;
                         switch (op1) {
                         case 0: gen_sxtb16(tmp);  break;
                         case 2: gen_sxtb(tmp);    break;
                         case 3: gen_sxth(tmp);    break;
                         case 4: gen_uxtb16(tmp);  break;
                         case 6: gen_uxtb(tmp);    break;
                         case 7: gen_uxth(tmp);    break;
                         default: goto illegal_op;
                         }
                         if (rn != 15) {
                             tmp2 = load_reg(s, rn);
                             if ((op1 & 3) == 0) {
                                 gen_add16(s, tmp, tmp2);
                             } else {
                                 tcg_gen_add_i32(tcg_ctx, tmp, tmp, tmp2);
                                 tcg_temp_free_i32(tcg_ctx, tmp2);
                             }
                         }
                         store_reg(s, rd, tmp);
                     } else if ((insn & 0x003f0f60) == 0x003f0f20) {
                         /* rev */
                         tmp = load_reg(s, rm);
                         if (insn & (1 << 22)) {
                             if (insn & (1 << 7)) {
                                 gen_revsh(s, tmp);
                             } else {
                                 ARCH(6T2);
                                 gen_helper_rbit(tcg_ctx, tmp, tmp);
                             }
                         } else {
                             if (insn & (1 << 7))
                                 gen_rev16(s, tmp);
                             else
                                 tcg_gen_bswap32_i32(tcg_ctx, tmp, tmp);
                         }
                         store_reg(s, rd, tmp);
                     } else {
                         goto illegal_op;
                     }
                     break;
                 case 2: /* Multiplies (Type 3).  */
                     switch ((insn >> 20) & 0x7) {
                     case 5:
                         if (((insn >> 6) ^ (insn >> 7)) & 1) {
                             /* op2 not 00x or 11x : UNDEF */
                             goto illegal_op;
                         }
                         /* Signed multiply most significant [accumulate].
                            (SMMUL, SMMLA, SMMLS) */
                         tmp = load_reg(s, rm);
                         tmp2 = load_reg(s, rs);
                         tmp64 = gen_muls_i64_i32(s, tmp, tmp2);
 
                         if (rd != 15) {
                             tmp = load_reg(s, rd);
                             if (insn & (1 << 6)) {
                                 tmp64 = gen_subq_msw(s, tmp64, tmp);
                             } else {
                                 tmp64 = gen_addq_msw(s, tmp64, tmp);
                             }
                         }
                         if (insn & (1 << 5)) {
                             tcg_gen_addi_i64(tcg_ctx, tmp64, tmp64, 0x80000000u);
                         }
                         tcg_gen_shri_i64(tcg_ctx, tmp64, tmp64, 32);
                         tmp = tcg_temp_new_i32(tcg_ctx);
                         tcg_gen_trunc_i64_i32(tcg_ctx, tmp, tmp64);
                         tcg_temp_free_i64(tcg_ctx, tmp64);
                         store_reg(s, rn, tmp);
                         break;
                     case 0:
                     case 4:
                         /* SMLAD, SMUAD, SMLSD, SMUSD, SMLALD, SMLSLD */
                         if (insn & (1 << 7)) {
                             goto illegal_op;
                         }
                         tmp = load_reg(s, rm);
                         tmp2 = load_reg(s, rs);
                         if (insn & (1 << 5))
                             gen_swap_half(s, tmp2);
                         gen_smul_dual(s, tmp, tmp2);
                         if (insn & (1 << 22)) {
                             /* smlald, smlsld */
                             TCGv_i64 tmp64_2;
 
                             tmp64 = tcg_temp_new_i64(tcg_ctx);
                             tmp64_2 = tcg_temp_new_i64(tcg_ctx);
                             tcg_gen_ext_i32_i64(tcg_ctx, tmp64, tmp);
                             tcg_gen_ext_i32_i64(tcg_ctx, tmp64_2, tmp2);
                             tcg_temp_free_i32(tcg_ctx, tmp);
                             tcg_temp_free_i32(tcg_ctx, tmp2);
                             if (insn & (1 << 6)) {
                                 tcg_gen_sub_i64(tcg_ctx, tmp64, tmp64, tmp64_2);
                             } else {
                                 tcg_gen_add_i64(tcg_ctx, tmp64, tmp64, tmp64_2);
                             }
                             tcg_temp_free_i64(tcg_ctx, tmp64_2);
                             gen_addq(s, tmp64, rd, rn);
                             gen_storeq_reg(s, rd, rn, tmp64);
                             tcg_temp_free_i64(tcg_ctx, tmp64);
                         } else {
                             /* smuad, smusd, smlad, smlsd */
                             if (insn & (1 << 6)) {
                                 /* This subtraction cannot overflow. */
                                 tcg_gen_sub_i32(tcg_ctx, tmp, tmp, tmp2);
                             } else {
                                 /* This addition cannot overflow 32 bits;
                                  * however it may overflow considered as a
                                  * signed operation, in which case we must set
                                  * the Q flag.
                                  */
                                 gen_helper_add_setq(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
                             }
                             tcg_temp_free_i32(tcg_ctx, tmp2);
                             if (rd != 15)
                               {
                                 tmp2 = load_reg(s, rd);
                                 gen_helper_add_setq(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp, tmp2);
                                 tcg_temp_free_i32(tcg_ctx, tmp2);
                               }
                             store_reg(s, rn, tmp);
                         }
                         break;
                     case 1:
                     case 3:
                         /* SDIV, UDIV */
                         if (!arm_dc_feature(s, ARM_FEATURE_ARM_DIV)) {
                             goto illegal_op;
                         }
                         if (((insn >> 5) & 7) || (rd != 15)) {
                             goto illegal_op;
                         }
                         tmp = load_reg(s, rm);
                         tmp2 = load_reg(s, rs);
                         if (insn & (1 << 21)) {
                             gen_helper_udiv(tcg_ctx, tmp, tmp, tmp2);
                         } else {
                             gen_helper_sdiv(tcg_ctx, tmp, tmp, tmp2);
                         }
                         tcg_temp_free_i32(tcg_ctx, tmp2);
                         store_reg(s, rn, tmp);
                         break;
                     default:
                         goto illegal_op;
                     }
                     break;
                 case 3:
                     op1 = ((insn >> 17) & 0x38) | ((insn >> 5) & 7);
                     switch (op1) {
                     case 0: /* Unsigned sum of absolute differences.  */
                         ARCH(6);
                         tmp = load_reg(s, rm);
                         tmp2 = load_reg(s, rs);
                         gen_helper_usad8(tcg_ctx, tmp, tmp, tmp2);
                         tcg_temp_free_i32(tcg_ctx, tmp2);
                         if (rd != 15) {
                             tmp2 = load_reg(s, rd);
                             tcg_gen_add_i32(tcg_ctx, tmp, tmp, tmp2);
                             tcg_temp_free_i32(tcg_ctx, tmp2);
                         }
                         store_reg(s, rn, tmp);
                         break;
                     case 0x20: case 0x24: case 0x28: case 0x2c:
                         /* Bitfield insert/clear.  */
                         ARCH(6T2);
                         shift = (insn >> 7) & 0x1f;
                         i = (insn >> 16) & 0x1f;
                         i = i + 1 - shift;
                         if (rm == 15) {
                             tmp = tcg_temp_new_i32(tcg_ctx);
                             tcg_gen_movi_i32(tcg_ctx, tmp, 0);
                         } else {
                             tmp = load_reg(s, rm);
                         }
                         if (i != 32) {
                             tmp2 = load_reg(s, rd);
                             tcg_gen_deposit_i32(tcg_ctx, tmp, tmp2, tmp, shift, i);
                             tcg_temp_free_i32(tcg_ctx, tmp2);
                         }
                         store_reg(s, rd, tmp);
                         break;
                     case 0x12: case 0x16: case 0x1a: case 0x1e: /* sbfx */
                     case 0x32: case 0x36: case 0x3a: case 0x3e: /* ubfx */
                         ARCH(6T2);
                         tmp = load_reg(s, rm);
                         shift = (insn >> 7) & 0x1f;
                         i = ((insn >> 16) & 0x1f) + 1;
                         if (shift + i > 32)
                             goto illegal_op;
                         if (i < 32) {
                             if (op1 & 0x20) {
                                 gen_ubfx(s, tmp, shift, (1u << i) - 1);
                             } else {
                                 gen_sbfx(s, tmp, shift, i);
                             }
                         }
                         store_reg(s, rd, tmp);
                         break;
                     default:
                         goto illegal_op;
                     }
                     break;
                 }
                 break;
             }
         do_ldst:
             /* Check for undefined extension instructions
              * per the ARM Bible IE:
              * xxxx 0111 1111 xxxx  xxxx xxxx 1111 xxxx
              */
             sh = (0xf << 20) | (0xf << 4);
             if (op1 == 0x7 && ((insn & sh) == sh))
             {
                 goto illegal_op;
             }
             /* load/store byte/word */
             rn = (insn >> 16) & 0xf;
             rd = (insn >> 12) & 0xf;
             tmp2 = load_reg(s, rn);
             if ((insn & 0x01200000) == 0x00200000) {
                 /* ldrt/strt */
                 i = MMU_USER_IDX;
             } else {
                 i = get_mem_index(s);
             }
             if (insn & (1 << 24))
                 gen_add_data_offset(s, insn, tmp2);
             if (insn & (1 << 20)) {
                 /* load */
                 tmp = tcg_temp_new_i32(tcg_ctx);
                 if (insn & (1 << 22)) {
                     gen_aa32_ld8u(s, tmp, tmp2, i);
                 } else {
                     gen_aa32_ld32u(s, tmp, tmp2, i);
                 }
             } else {
                 /* store */
                 tmp = load_reg(s, rd);
                 if (insn & (1 << 22)) {
                     gen_aa32_st8(s, tmp, tmp2, i);
                 } else {
                     gen_aa32_st32(s, tmp, tmp2, i);
                 }
                 tcg_temp_free_i32(tcg_ctx, tmp);
             }
             if (!(insn & (1 << 24))) {
                 gen_add_data_offset(s, insn, tmp2);
                 store_reg(s, rn, tmp2);
             } else if (insn & (1 << 21)) {
                 store_reg(s, rn, tmp2);
             } else {
                 tcg_temp_free_i32(tcg_ctx, tmp2);
             }
             if (insn & (1 << 20)) {
                 /* Complete the load.  */
                 store_reg_from_load(s, rd, tmp);
             }
             break;
         case 0x08:
         case 0x09:
             {
                 int j, n, user, loaded_base;
                 TCGv_i32 loaded_var;
                 /* load/store multiple words */
                 /* XXX: store correct base if write back */
                 user = 0;
                 if (insn & (1 << 22)) {
                     if (IS_USER(s))
                         goto illegal_op; /* only usable in supervisor mode */
 
                     if ((insn & (1 << 15)) == 0)
                         user = 1;
                 }
                 rn = (insn >> 16) & 0xf;
                 addr = load_reg(s, rn);
 
                 /* compute total size */
                 loaded_base = 0;
                 TCGV_UNUSED_I32(loaded_var);
                 n = 0;
                 for(i=0;i<16;i++) {
                     if (insn & (1 << i))
                         n++;
                 }
                 /* XXX: test invalid n == 0 case ? */
                 if (insn & (1 << 23)) {
                     if (insn & (1 << 24)) {
                         /* pre increment */
                         tcg_gen_addi_i32(tcg_ctx, addr, addr, 4);
                     } else {
                         /* post increment */
                     }
                 } else {
                     if (insn & (1 << 24)) {
                         /* pre decrement */
                         tcg_gen_addi_i32(tcg_ctx, addr, addr, -(n * 4));
                     } else {
                         /* post decrement */
                         if (n != 1)
                         tcg_gen_addi_i32(tcg_ctx, addr, addr, -((n - 1) * 4));
                     }
                 }
                 j = 0;
                 for(i=0;i<16;i++) {
                     if (insn & (1 << i)) {
                         if (insn & (1 << 20)) {
                             /* load */
                             tmp = tcg_temp_new_i32(tcg_ctx);
                             gen_aa32_ld32u(s, tmp, addr, get_mem_index(s));
                             if (user) {
                                 tmp2 = tcg_const_i32(tcg_ctx, i);
                                 gen_helper_set_user_reg(tcg_ctx, tcg_ctx->cpu_env, tmp2, tmp);
                                 tcg_temp_free_i32(tcg_ctx, tmp2);
                                 tcg_temp_free_i32(tcg_ctx, tmp);
                             } else if (i == rn) {
                                 loaded_var = tmp;
                                 loaded_base = 1;
                             } else {
                                 store_reg_from_load(s, i, tmp);
                             }
                         } else {
                             /* store */
                             if (i == 15) {
                                 /* special case: r15 = PC + 8 */
                                 val = (long)s->pc + 4;
                                 tmp = tcg_temp_new_i32(tcg_ctx);
                                 tcg_gen_movi_i32(tcg_ctx, tmp, val);
                             } else if (user) {
                                 tmp = tcg_temp_new_i32(tcg_ctx);
                                 tmp2 = tcg_const_i32(tcg_ctx, i);
                                 gen_helper_get_user_reg(tcg_ctx, tmp, tcg_ctx->cpu_env, tmp2);
                                 tcg_temp_free_i32(tcg_ctx, tmp2);
                             } else {
                                 tmp = load_reg(s, i);
                             }
                             gen_aa32_st32(s, tmp, addr, get_mem_index(s));
                             tcg_temp_free_i32(tcg_ctx, tmp);
                         }
                         j++;
                         /* no need to add after the last transfer */
                         if (j != n)
                             tcg_gen_addi_i32(tcg_ctx, addr, addr, 4);
                     }
                 }
                 if (insn & (1 << 21)) {
                     /* write back */
                     if (insn & (1 << 23)) {
                         if (insn & (1 << 24)) {
                             /* pre increment */
                         } else {
                             /* post increment */
                             tcg_gen_addi_i32(tcg_ctx, addr, addr, 4);
                         }
                     } else {
                         if (insn & (1 << 24)) {
                             /* pre decrement */
                             if (n != 1)
                                 tcg_gen_addi_i32(tcg_ctx, addr, addr, -((n - 1) * 4));
                         } else {
                             /* post decrement */
                             tcg_gen_addi_i32(tcg_ctx, addr, addr, -(n * 4));
                         }
                     }
                     store_reg(s, rn, addr);
                 } else {
                     tcg_temp_free_i32(tcg_ctx, addr);
                 }
                 if (loaded_base) {
                     store_reg(s, rn, loaded_var);
                 }
                 if ((insn & (1 << 22)) && !user) {
                     /* Restore CPSR from SPSR.  */
                     tmp = load_cpu_field(s->uc, spsr);
                     gen_set_cpsr(s, tmp, CPSR_ERET_MASK);
                     tcg_temp_free_i32(tcg_ctx, tmp);
                     s->is_jmp = DISAS_UPDATE;
                 }
             }
             break;
         case 0xa:
         case 0xb:
             {
                 int32_t offset;
 
                 /* branch (and link) */
                 val = (int32_t)s->pc;
                 if (insn & (1 << 24)) {
                     tmp = tcg_temp_new_i32(tcg_ctx);
                     tcg_gen_movi_i32(tcg_ctx, tmp, val);
                     store_reg(s, 14, tmp);
                 }
                 offset = sextract32(insn << 2, 0, 26);
                 val += offset + 4;
                 gen_jmp(s, val);
             }
             break;
         case 0xc:
         case 0xd:
         case 0xe:
             if (((insn >> 8) & 0xe) == 10) {
                 /* VFP.  */
                 if (disas_vfp_insn(s, insn)) {
                     goto illegal_op;
                 }
             } else if (disas_coproc_insn(s, insn)) {
                 /* Coprocessor.  */
                 goto illegal_op;
             }
             break;
         case 0xf:   // qq
             /* swi */
             gen_set_pc_im(s, s->pc);
             s->svc_imm = extract32(insn, 0, 24);
             s->is_jmp = DISAS_SWI;
             break;
         default:
         illegal_op:
             gen_exception_insn(s, 4, EXCP_UDEF, syn_uncategorized());
             break;
         }
     }
 }
 
 /* Return true if this is a Thumb-2 logical op.  */
diff --git a/qemu/target-i386/ops_sse.h b/qemu/target-i386/ops_sse.h
index f142f335..e8c01f85 100644
--- a/qemu/target-i386/ops_sse.h
+++ b/qemu/target-i386/ops_sse.h
@@ -1416,8 +1416,8 @@ void glue(helper_phaddw, SUFFIX)(CPUX86State *env, Reg *d, Reg *s)
 void glue(helper_phaddd, SUFFIX)(CPUX86State *env, Reg *d, Reg *s)
 {
     d->L(0) = (int32_t)d->L(0) + (int32_t)d->L(1);
     XMM_ONLY(d->L(1) = (int32_t)d->L(2) + (int32_t)d->L(3));
-    d->L((1 << SHIFT) + 0) = (uint32_t)((int32_t)s->L(0) + (int32_t)s->L(1));
+    d->L((1 << SHIFT) + 0) = (uint32_t)((int32_t)s->L(0) + (uint32_t)s->L(1));
     XMM_ONLY(d->L(3) = (int32_t)s->L(2) + (int32_t)s->L(3));
 }
 
diff --git a/qemu/target-mips/translate.c b/qemu/target-mips/translate.c
index e91b330c..ad8e3093 100644
--- a/qemu/target-mips/translate.c
+++ b/qemu/target-mips/translate.c
@@ -11132,192 +11132,192 @@ static void decode_i64_mips16 (DisasContext *ctx,
 static int decode_extended_mips16_opc (CPUMIPSState *env, DisasContext *ctx)
 {
     TCGContext *tcg_ctx = ctx->uc->tcg_ctx;
     TCGv **cpu_gpr = (TCGv **)tcg_ctx->cpu_gpr;
     int extend = cpu_lduw_code(env, ctx->pc + 2);
     int op, rx, ry, funct, sa;
     int16_t imm, offset;
 
     ctx->opcode = (ctx->opcode << 16) | extend;
     op = (ctx->opcode >> 11) & 0x1f;
     sa = (ctx->opcode >> 22) & 0x1f;
     funct = (ctx->opcode >> 8) & 0x7;
     rx = xlat((ctx->opcode >> 8) & 0x7);
     ry = xlat((ctx->opcode >> 5) & 0x7);
     offset = imm = (int16_t) (((ctx->opcode >> 16) & 0x1f) << 11
                               | ((ctx->opcode >> 21) & 0x3f) << 5
                               | (ctx->opcode & 0x1f));
 
     /* The extended opcodes cleverly reuse the opcodes from their 16-bit
        counterparts.  */
     switch (op) {
     case M16_OPC_ADDIUSP:
         gen_arith_imm(ctx, OPC_ADDIU, rx, 29, imm);
         break;
     case M16_OPC_ADDIUPC:
         gen_addiupc(ctx, rx, imm, 0, 1);
         break;
     case M16_OPC_B:
         gen_compute_branch(ctx, OPC_BEQ, 4, 0, 0, (uint32_t)offset << 1, 0);
         /* No delay slot, so just process as a normal instruction */
         break;
     case M16_OPC_BEQZ:
-        gen_compute_branch(ctx, OPC_BEQ, 4, rx, 0, offset << 1, 0);
+        gen_compute_branch(ctx, OPC_BEQ, 4, rx, 0, (uint16_t)offset << 1, 0);
         /* No delay slot, so just process as a normal instruction */
         break;
     case M16_OPC_BNEQZ:
-        gen_compute_branch(ctx, OPC_BNE, 4, rx, 0, offset << 1, 0);
+        gen_compute_branch(ctx, OPC_BNE, 4, rx, 0, (uint16_t)offset << 1, 0);
         /* No delay slot, so just process as a normal instruction */
         break;
     case M16_OPC_SHIFT:
         switch (ctx->opcode & 0x3) {
         case 0x0:
             gen_shift_imm(ctx, OPC_SLL, rx, ry, sa);
             break;
         case 0x1:
 #if defined(TARGET_MIPS64)
             check_mips_64(ctx);
             gen_shift_imm(ctx, OPC_DSLL, rx, ry, sa);
 #else
             generate_exception(ctx, EXCP_RI);
 #endif
             break;
         case 0x2:
             gen_shift_imm(ctx, OPC_SRL, rx, ry, sa);
             break;
         case 0x3:
             gen_shift_imm(ctx, OPC_SRA, rx, ry, sa);
             break;
         }
         break;
 #if defined(TARGET_MIPS64)
     case M16_OPC_LD:
             check_mips_64(ctx);
         gen_ld(ctx, OPC_LD, ry, rx, offset);
         break;
 #endif
     case M16_OPC_RRIA:
         imm = ctx->opcode & 0xf;
         imm = imm | ((ctx->opcode >> 20) & 0x7f) << 4;
         imm = imm | ((ctx->opcode >> 16) & 0xf) << 11;
         imm = (int16_t) (imm << 1) >> 1;
         if ((ctx->opcode >> 4) & 0x1) {
 #if defined(TARGET_MIPS64)
             check_mips_64(ctx);
             gen_arith_imm(ctx, OPC_DADDIU, ry, rx, imm);
 #else
             generate_exception(ctx, EXCP_RI);
 #endif
         } else {
             gen_arith_imm(ctx, OPC_ADDIU, ry, rx, imm);
         }
         break;
     case M16_OPC_ADDIU8:
         gen_arith_imm(ctx, OPC_ADDIU, rx, rx, imm);
         break;
     case M16_OPC_SLTI:
         gen_slt_imm(ctx, OPC_SLTI, 24, rx, imm);
         break;
     case M16_OPC_SLTIU:
         gen_slt_imm(ctx, OPC_SLTIU, 24, rx, imm);
         break;
     case M16_OPC_I8:
         switch (funct) {
         case I8_BTEQZ:
-            gen_compute_branch(ctx, OPC_BEQ, 4, 24, 0, offset << 1, 0);
+            gen_compute_branch(ctx, OPC_BEQ, 4, 24, 0, (uint16_t)offset << 1, 0);
             break;
         case I8_BTNEZ:
-            gen_compute_branch(ctx, OPC_BNE, 4, 24, 0, offset << 1, 0);
+            gen_compute_branch(ctx, OPC_BNE, 4, 24, 0, (uint16_t)offset << 1, 0);
             break;
         case I8_SWRASP:
             gen_st(ctx, OPC_SW, 31, 29, imm);
             break;
         case I8_ADJSP:
             gen_arith_imm(ctx, OPC_ADDIU, 29, 29, imm);
             break;
         case I8_SVRS:
             {
                 int xsregs = (ctx->opcode >> 24) & 0x7;
                 int aregs = (ctx->opcode >> 16) & 0xf;
                 int do_ra = (ctx->opcode >> 6) & 0x1;
                 int do_s0 = (ctx->opcode >> 5) & 0x1;
                 int do_s1 = (ctx->opcode >> 4) & 0x1;
                 int framesize = (((ctx->opcode >> 20) & 0xf) << 4
                                  | (ctx->opcode & 0xf)) << 3;
 
                 if (ctx->opcode & (1 << 7)) {
                     gen_mips16_save(ctx, xsregs, aregs,
                                     do_ra, do_s0, do_s1,
                                     framesize);
                 } else {
                     gen_mips16_restore(ctx, xsregs, aregs,
                                        do_ra, do_s0, do_s1,
                                        framesize);
                 }
             }
             break;
         default:
             generate_exception(ctx, EXCP_RI);
             break;
         }
         break;
     case M16_OPC_LI:
         tcg_gen_movi_tl(tcg_ctx, *cpu_gpr[rx], (uint16_t) imm);
         break;
     case M16_OPC_CMPI:
         tcg_gen_xori_tl(tcg_ctx, *cpu_gpr[24], *cpu_gpr[rx], (uint16_t) imm);
         break;
 #if defined(TARGET_MIPS64)
     case M16_OPC_SD:
         gen_st(ctx, OPC_SD, ry, rx, offset);
         break;
 #endif
     case M16_OPC_LB:
         gen_ld(ctx, OPC_LB, ry, rx, offset);
         break;
     case M16_OPC_LH:
         gen_ld(ctx, OPC_LH, ry, rx, offset);
         break;
     case M16_OPC_LWSP:
         gen_ld(ctx, OPC_LW, rx, 29, offset);
         break;
     case M16_OPC_LW:
         gen_ld(ctx, OPC_LW, ry, rx, offset);
         break;
     case M16_OPC_LBU:
         gen_ld(ctx, OPC_LBU, ry, rx, offset);
         break;
     case M16_OPC_LHU:
         gen_ld(ctx, OPC_LHU, ry, rx, offset);
         break;
     case M16_OPC_LWPC:
         gen_ld(ctx, OPC_LWPC, rx, 0, offset);
         break;
 #if defined(TARGET_MIPS64)
     case M16_OPC_LWU:
         gen_ld(ctx, OPC_LWU, ry, rx, offset);
         break;
 #endif
     case M16_OPC_SB:
         gen_st(ctx, OPC_SB, ry, rx, offset);
         break;
     case M16_OPC_SH:
         gen_st(ctx, OPC_SH, ry, rx, offset);
         break;
     case M16_OPC_SWSP:
         gen_st(ctx, OPC_SW, rx, 29, offset);
         break;
     case M16_OPC_SW:
         gen_st(ctx, OPC_SW, ry, rx, offset);
         break;
 #if defined(TARGET_MIPS64)
     case M16_OPC_I64:
         decode_i64_mips16(ctx, ry, funct, offset, 1);
         break;
 #endif
     default:
         generate_exception(ctx, EXCP_RI);
         break;
     }
 
     return 4;
 }
@@ -18513,642 +18513,642 @@ static void hook_insn(CPUMIPSState *env, DisasContext *ctx, bool *insn_need_patc
 static void decode_opc (CPUMIPSState *env, DisasContext *ctx, bool *insn_need_patch, int *insn_patch_offset)
 {
     TCGContext *tcg_ctx = ctx->uc->tcg_ctx;
 #if defined(TARGET_MIPS64)
     TCGv **cpu_gpr = (TCGv **)tcg_ctx->cpu_gpr;
 #endif
     int32_t offset;
     int rs, rt, rd, sa;
     uint32_t op, op1;
     int16_t imm;
 
     /* make sure instructions are on a word boundary */
     if (ctx->pc & 0x3) {
         env->CP0_BadVAddr = ctx->pc;
         generate_exception_err(ctx, EXCP_AdEL, EXCP_INST_NOTAVAIL);
         return;
     }
 
     /* Handle blikely not taken case */
     if ((ctx->hflags & MIPS_HFLAG_BMASK_BASE) == MIPS_HFLAG_BL) {
         int l1 = gen_new_label(tcg_ctx);
 
         MIPS_DEBUG("blikely condition (" TARGET_FMT_lx ")", ctx->pc + 4);
         tcg_gen_brcondi_tl(tcg_ctx, TCG_COND_NE, *(TCGv *)tcg_ctx->bcond, 0, l1);
         tcg_gen_movi_i32(tcg_ctx, tcg_ctx->hflags, ctx->hflags & ~MIPS_HFLAG_BMASK);
         gen_goto_tb(ctx, 1, ctx->pc + 4);
         gen_set_label(tcg_ctx, l1);
         hook_insn(env, ctx, insn_need_patch, insn_patch_offset, 14);
     } else {
         hook_insn(env, ctx, insn_need_patch, insn_patch_offset, 1);
     }
 
     if (unlikely(qemu_loglevel_mask(CPU_LOG_TB_OP | CPU_LOG_TB_OP_OPT))) {
         tcg_gen_debug_insn_start(tcg_ctx, ctx->pc);
     }
 
     op = MASK_OP_MAJOR(ctx->opcode);
     rs = (ctx->opcode >> 21) & 0x1f;
     rt = (ctx->opcode >> 16) & 0x1f;
     rd = (ctx->opcode >> 11) & 0x1f;
     sa = (ctx->opcode >> 6) & 0x1f;
     imm = (int16_t)ctx->opcode;
     switch (op) {
     case OPC_SPECIAL:
         decode_opc_special(env, ctx);
         break;
     case OPC_SPECIAL2:
         decode_opc_special2_legacy(env, ctx);
         break;
     case OPC_SPECIAL3:
         decode_opc_special3(env, ctx);
         break;
     case OPC_REGIMM:
         op1 = MASK_REGIMM(ctx->opcode);
         switch (op1) {
         case OPC_BLTZL: /* REGIMM branches */
         case OPC_BGEZL:
         case OPC_BLTZALL:
         case OPC_BGEZALL:
             check_insn_opc_removed(ctx, ISA_MIPS32R6);
         case OPC_BLTZ:
         case OPC_BGEZ:
             gen_compute_branch(ctx, op1, 4, rs, -1, (uint32_t)imm << 2, 4);
             break;
         case OPC_BLTZAL:
         case OPC_BGEZAL:
             if (ctx->insn_flags & ISA_MIPS32R6) {
                 if (rs == 0) {
                     /* OPC_NAL, OPC_BAL */
                     gen_compute_branch(ctx, op1, 4, 0, -1, (uint32_t)imm << 2, 4);
                 } else {
                     generate_exception(ctx, EXCP_RI);
                 }
             } else {
                 gen_compute_branch(ctx, op1, 4, rs, -1, (uint32_t)imm << 2, 4);
             }
             break;
         case OPC_TGEI: case OPC_TGEIU: case OPC_TLTI: case OPC_TLTIU: case OPC_TEQI: /* REGIMM traps */
         case OPC_TNEI:
             check_insn_opc_removed(ctx, ISA_MIPS32R6);
             gen_trap(ctx, op1, rs, -1, imm);
             break;
         case OPC_SYNCI:
             check_insn(ctx, ISA_MIPS32R2);
             /* Break the TB to be able to sync copied instructions
                immediately */
             ctx->bstate = BS_STOP;
             break;
         case OPC_BPOSGE32:    /* MIPS DSP branch */
 #if defined(TARGET_MIPS64)
         case OPC_BPOSGE64:
 #endif
             check_dsp(ctx);
             gen_compute_branch(ctx, op1, 4, -1, -2, (uint32_t)imm << 2, 4);
             break;
 #if defined(TARGET_MIPS64)
         case OPC_DAHI:
             check_insn(ctx, ISA_MIPS32R6);
             check_mips_64(ctx);
             if (rs != 0) {
                 tcg_gen_addi_tl(tcg_ctx, *cpu_gpr[rs], *cpu_gpr[rs], (int64_t)imm << 32);
             }
             MIPS_DEBUG("dahi %s, %04x", regnames[rs], imm);
             break;
         case OPC_DATI:
             check_insn(ctx, ISA_MIPS32R6);
             check_mips_64(ctx);
             if (rs != 0) {
                 tcg_gen_addi_tl(tcg_ctx, *cpu_gpr[rs], *cpu_gpr[rs], (int64_t)imm << 48);
             }
             MIPS_DEBUG("dati %s, %04x", regnames[rs], imm);
             break;
 #endif
         default:            /* Invalid */
             MIPS_INVAL("regimm");
             generate_exception(ctx, EXCP_RI);
             break;
         }
         break;
     case OPC_CP0:
         check_cp0_enabled(ctx);
         op1 = MASK_CP0(ctx->opcode);
         switch (op1) {
         case OPC_MFC0:
         case OPC_MTC0:
         case OPC_MFTR:
         case OPC_MTTR:
 #if defined(TARGET_MIPS64)
         case OPC_DMFC0:
         case OPC_DMTC0:
 #endif
 #ifndef CONFIG_USER_ONLY
             gen_cp0(env, ctx, op1, rt, rd);
 #endif /* !CONFIG_USER_ONLY */
             break;
         case OPC_C0_FIRST: case OPC_C0_LAST:
 #ifndef CONFIG_USER_ONLY
             gen_cp0(env, ctx, MASK_C0(ctx->opcode), rt, rd);
 #endif /* !CONFIG_USER_ONLY */
             break;
         case OPC_MFMC0:
 #ifndef CONFIG_USER_ONLY
             {
                 uint32_t op2;
                 TCGv t0 = tcg_temp_new(tcg_ctx);
 
                 op2 = MASK_MFMC0(ctx->opcode);
                 switch (op2) {
                 case OPC_DMT:
                     check_insn(ctx, ASE_MT);
                     gen_helper_dmt(tcg_ctx, t0);
                     gen_store_gpr(tcg_ctx, t0, rt);
                     break;
                 case OPC_EMT:
                     check_insn(ctx, ASE_MT);
                     gen_helper_emt(tcg_ctx, t0);
                     gen_store_gpr(tcg_ctx, t0, rt);
                     break;
                 case OPC_DVPE:
                     check_insn(ctx, ASE_MT);
                     gen_helper_dvpe(tcg_ctx, t0, tcg_ctx->cpu_env);
                     gen_store_gpr(tcg_ctx, t0, rt);
                     break;
                 case OPC_EVPE:
                     check_insn(ctx, ASE_MT);
                     gen_helper_evpe(tcg_ctx, t0, tcg_ctx->cpu_env);
                     gen_store_gpr(tcg_ctx, t0, rt);
                     break;
                 case OPC_DI:
                     check_insn(ctx, ISA_MIPS32R2);
                     save_cpu_state(ctx, 1);
                     gen_helper_di(tcg_ctx, t0, tcg_ctx->cpu_env);
                     gen_store_gpr(tcg_ctx, t0, rt);
                     /* Stop translation as we may have switched the execution mode */
                     ctx->bstate = BS_STOP;
                     break;
                 case OPC_EI:
                     check_insn(ctx, ISA_MIPS32R2);
                     save_cpu_state(ctx, 1);
                     gen_helper_ei(tcg_ctx, t0, tcg_ctx->cpu_env);
                     gen_store_gpr(tcg_ctx, t0, rt);
                     /* Stop translation as we may have switched the execution mode */
                     ctx->bstate = BS_STOP;
                     break;
                 default:            /* Invalid */
                     MIPS_INVAL("mfmc0");
                     generate_exception(ctx, EXCP_RI);
                     break;
                 }
                 tcg_temp_free(tcg_ctx, t0);
             }
 #endif /* !CONFIG_USER_ONLY */
             break;
         case OPC_RDPGPR:
             check_insn(ctx, ISA_MIPS32R2);
             gen_load_srsgpr(ctx, rt, rd);
             break;
         case OPC_WRPGPR:
             check_insn(ctx, ISA_MIPS32R2);
             gen_store_srsgpr(ctx, rt, rd);
             break;
         default:
             MIPS_INVAL("cp0");
             generate_exception(ctx, EXCP_RI);
             break;
         }
         break;
     case OPC_BOVC: /* OPC_BEQZALC, OPC_BEQC, OPC_ADDI */
         if (ctx->insn_flags & ISA_MIPS32R6) {
             /* OPC_BOVC, OPC_BEQZALC, OPC_BEQC */
             gen_compute_compact_branch(ctx, op, rs, rt, (uint32_t)imm << 2);
         } else {
             /* OPC_ADDI */
             /* Arithmetic with immediate opcode */
             gen_arith_imm(ctx, op, rt, rs, imm);
         }
         break;
     case OPC_ADDIU:
          gen_arith_imm(ctx, op, rt, rs, imm);
          break;
     case OPC_SLTI: /* Set on less than with immediate opcode */
     case OPC_SLTIU:
          gen_slt_imm(ctx, op, rt, rs, imm);
          break;
     case OPC_ANDI: /* Arithmetic with immediate opcode */
     case OPC_LUI: /* OPC_AUI */
     case OPC_ORI:
     case OPC_XORI:
          gen_logic_imm(ctx, op, rt, rs, imm);
          break;
     case OPC_J: case OPC_JAL: /* Jump */
          offset = (int32_t)(ctx->opcode & 0x3FFFFFF) << 2;
          gen_compute_branch(ctx, op, 4, rs, rt, offset, 4);
          break;
     /* Branch */
     case OPC_BLEZC: /* OPC_BGEZC, OPC_BGEC, OPC_BLEZL */
         if (ctx->insn_flags & ISA_MIPS32R6) {
             if (rt == 0) {
                 generate_exception(ctx, EXCP_RI);
                 break;
             }
             /* OPC_BLEZC, OPC_BGEZC, OPC_BGEC */
             gen_compute_compact_branch(ctx, op, rs, rt, (uint32_t)imm << 2);
         } else {
             /* OPC_BLEZL */
             gen_compute_branch(ctx, op, 4, rs, rt, (uint32_t)imm << 2, 4);
         }
         break;
     case OPC_BGTZC: /* OPC_BLTZC, OPC_BLTC, OPC_BGTZL */
         if (ctx->insn_flags & ISA_MIPS32R6) {
             if (rt == 0) {
                 generate_exception(ctx, EXCP_RI);
                 break;
             }
             /* OPC_BGTZC, OPC_BLTZC, OPC_BLTC */
             gen_compute_compact_branch(ctx, op, rs, rt, (uint32_t)imm << 2);
         } else {
             /* OPC_BGTZL */
             gen_compute_branch(ctx, op, 4, rs, rt, (uint32_t)imm << 2, 4);
         }
         break;
     case OPC_BLEZALC: /* OPC_BGEZALC, OPC_BGEUC, OPC_BLEZ */
         if (rt == 0) {
             /* OPC_BLEZ */
             gen_compute_branch(ctx, op, 4, rs, rt, (uint32_t)imm << 2, 4);
         } else {
             check_insn(ctx, ISA_MIPS32R6);
             /* OPC_BLEZALC, OPC_BGEZALC, OPC_BGEUC */
             gen_compute_compact_branch(ctx, op, rs, rt, (uint32_t)imm << 2);
         }
         break;
     case OPC_BGTZALC: /* OPC_BLTZALC, OPC_BLTUC, OPC_BGTZ */
         if (rt == 0) {
             /* OPC_BGTZ */
             gen_compute_branch(ctx, op, 4, rs, rt, (uint32_t)imm << 2, 4);
         } else {
             check_insn(ctx, ISA_MIPS32R6);
             /* OPC_BGTZALC, OPC_BLTZALC, OPC_BLTUC */
             gen_compute_compact_branch(ctx, op, rs, rt, (uint32_t)imm << 2);
         }
         break;
     case OPC_BEQL:
     case OPC_BNEL:
          check_insn_opc_removed(ctx, ISA_MIPS32R6);
     case OPC_BEQ:
     case OPC_BNE:
          gen_compute_branch(ctx, op, 4, rs, rt, (uint32_t)imm << 2, 4);
          break;
     case OPC_LWL: /* Load and stores */
     case OPC_LWR:
     case OPC_LL:
         check_insn_opc_removed(ctx, ISA_MIPS32R6);
     case OPC_LB: case OPC_LH:
     case OPC_LW: case OPC_LBU: case OPC_LHU:
          gen_ld(ctx, op, rt, rs, imm);
          break;
     case OPC_SWL:
     case OPC_SWR:
         check_insn_opc_removed(ctx, ISA_MIPS32R6);
     case OPC_SB: case OPC_SH:
     case OPC_SW:
          gen_st(ctx, op, rt, rs, imm);
          break;
     case OPC_SC:
          check_insn_opc_removed(ctx, ISA_MIPS32R6);
          gen_st_cond(ctx, op, rt, rs, imm);
          break;
     case OPC_CACHE:
         check_insn_opc_removed(ctx, ISA_MIPS32R6);
         check_cp0_enabled(ctx);
         check_insn(ctx, ISA_MIPS3 | ISA_MIPS32);
         /* Treat as NOP. */
         break;
     case OPC_PREF:
         check_insn_opc_removed(ctx, ISA_MIPS32R6);
         check_insn(ctx, ISA_MIPS4 | ISA_MIPS32);
         /* Treat as NOP. */
         break;
 
     /* Floating point (COP1). */
     case OPC_LWC1:
     case OPC_LDC1:
     case OPC_SWC1:
     case OPC_SDC1:
         gen_cop1_ldst(ctx, op, rt, rs, imm);
         break;
 
     case OPC_CP1:
         op1 = MASK_CP1(ctx->opcode);
 
         switch (op1) {
         case OPC_MFHC1:
         case OPC_MTHC1:
             check_cp1_enabled(ctx);
             check_insn(ctx, ISA_MIPS32R2);
         case OPC_MFC1:
         case OPC_CFC1:
         case OPC_MTC1:
         case OPC_CTC1:
             check_cp1_enabled(ctx);
             gen_cp1(ctx, op1, rt, rd);
             break;
 #if defined(TARGET_MIPS64)
         case OPC_DMFC1:
         case OPC_DMTC1:
             check_cp1_enabled(ctx);
             check_insn(ctx, ISA_MIPS3);
             gen_cp1(ctx, op1, rt, rd);
             break;
 #endif
         case OPC_BC1EQZ: /* OPC_BC1ANY2 */
             check_cp1_enabled(ctx);
             if (ctx->insn_flags & ISA_MIPS32R6) {
                 /* OPC_BC1EQZ */
                 gen_compute_branch1_r6(ctx, MASK_CP1(ctx->opcode),
-                                rt, imm << 2);
+                                rt, ((uint16_t)imm) << 2);
             } else {
                 /* OPC_BC1ANY2 */
                 check_cop1x(ctx);
                 check_insn(ctx, ASE_MIPS3D);
                 gen_compute_branch1(ctx, MASK_BC1(ctx->opcode),
                                     (rt >> 2) & 0x7, ((uint32_t)imm) << 2);
             }
             break;
         case OPC_BC1NEZ:
             check_cp1_enabled(ctx);
             check_insn(ctx, ISA_MIPS32R6);
             gen_compute_branch1_r6(ctx, MASK_CP1(ctx->opcode),
-                            rt, imm << 2);
+                            rt, ((uint16_t)imm) << 2);
             break;
         case OPC_BC1ANY4:
             check_cp1_enabled(ctx);
             check_insn_opc_removed(ctx, ISA_MIPS32R6);
             check_cop1x(ctx);
             check_insn(ctx, ASE_MIPS3D);
             /* fall through */
         case OPC_BC1:
             check_cp1_enabled(ctx);
             check_insn_opc_removed(ctx, ISA_MIPS32R6);
             gen_compute_branch1(ctx, MASK_BC1(ctx->opcode),
                                 (rt >> 2) & 0x7, (uint32_t)imm << 2);
             break;
         case OPC_PS_FMT:
             check_cp1_enabled(ctx);
             check_insn_opc_removed(ctx, ISA_MIPS32R6);
         case OPC_S_FMT:
         case OPC_D_FMT:
             check_cp1_enabled(ctx);
             gen_farith(ctx, ctx->opcode & FOP(0x3f, 0x1f), rt, rd, sa,
                        (imm >> 8) & 0x7);
             break;
         case OPC_W_FMT:
         case OPC_L_FMT:
         {
             int r6_op = ctx->opcode & FOP(0x3f, 0x1f);
             check_cp1_enabled(ctx);
             if (ctx->insn_flags & ISA_MIPS32R6) {
                 switch (r6_op) {
                 case R6_OPC_CMP_AF_S:
                 case R6_OPC_CMP_UN_S:
                 case R6_OPC_CMP_EQ_S:
                 case R6_OPC_CMP_UEQ_S:
                 case R6_OPC_CMP_LT_S:
                 case R6_OPC_CMP_ULT_S:
                 case R6_OPC_CMP_LE_S:
                 case R6_OPC_CMP_ULE_S:
                 case R6_OPC_CMP_SAF_S:
                 case R6_OPC_CMP_SUN_S:
                 case R6_OPC_CMP_SEQ_S:
                 case R6_OPC_CMP_SEUQ_S:
                 case R6_OPC_CMP_SLT_S:
                 case R6_OPC_CMP_SULT_S:
                 case R6_OPC_CMP_SLE_S:
                 case R6_OPC_CMP_SULE_S:
                 case R6_OPC_CMP_OR_S:
                 case R6_OPC_CMP_UNE_S:
                 case R6_OPC_CMP_NE_S:
                 case R6_OPC_CMP_SOR_S:
                 case R6_OPC_CMP_SUNE_S:
                 case R6_OPC_CMP_SNE_S:
                     gen_r6_cmp_s(ctx, ctx->opcode & 0x1f, rt, rd, sa);
                     break;
                 case R6_OPC_CMP_AF_D:
                 case R6_OPC_CMP_UN_D:
                 case R6_OPC_CMP_EQ_D:
                 case R6_OPC_CMP_UEQ_D:
                 case R6_OPC_CMP_LT_D:
                 case R6_OPC_CMP_ULT_D:
                 case R6_OPC_CMP_LE_D:
                 case R6_OPC_CMP_ULE_D:
                 case R6_OPC_CMP_SAF_D:
                 case R6_OPC_CMP_SUN_D:
                 case R6_OPC_CMP_SEQ_D:
                 case R6_OPC_CMP_SEUQ_D:
                 case R6_OPC_CMP_SLT_D:
                 case R6_OPC_CMP_SULT_D:
                 case R6_OPC_CMP_SLE_D:
                 case R6_OPC_CMP_SULE_D:
                 case R6_OPC_CMP_OR_D:
                 case R6_OPC_CMP_UNE_D:
                 case R6_OPC_CMP_NE_D:
                 case R6_OPC_CMP_SOR_D:
                 case R6_OPC_CMP_SUNE_D:
                 case R6_OPC_CMP_SNE_D:
                     gen_r6_cmp_d(ctx, ctx->opcode & 0x1f, rt, rd, sa);
                     break;
                 default:
                     gen_farith(ctx, ctx->opcode & FOP(0x3f, 0x1f), rt, rd, sa,
                                (imm >> 8) & 0x7);
                     break;
                 }
             } else {
                 gen_farith(ctx, ctx->opcode & FOP(0x3f, 0x1f), rt, rd, sa,
                            (imm >> 8) & 0x7);
             }
             break;
         }
         case OPC_BZ_V:
         case OPC_BNZ_V:
         case OPC_BZ_B:
         case OPC_BZ_H:
         case OPC_BZ_W:
         case OPC_BZ_D:
         case OPC_BNZ_B:
         case OPC_BNZ_H:
         case OPC_BNZ_W:
         case OPC_BNZ_D:
             check_insn(ctx, ASE_MSA);
             gen_msa_branch(env, ctx, op1);
             break;
         default:
             MIPS_INVAL("cp1");
             generate_exception(ctx, EXCP_RI);
             break;
         }
         break;
 
     /* Compact branches [R6] and COP2 [non-R6] */
     case OPC_BC: /* OPC_LWC2 */
     case OPC_BALC: /* OPC_SWC2 */
         if (ctx->insn_flags & ISA_MIPS32R6) {
             /* OPC_BC, OPC_BALC */
             gen_compute_compact_branch(ctx, op, 0, 0,
                                        sextract32(ctx->opcode << 2, 0, 28));
         } else {
             /* OPC_LWC2, OPC_SWC2 */
             /* COP2: Not implemented. */
             generate_exception_err(ctx, EXCP_CpU, 2);
         }
         break;
     case OPC_BEQZC: /* OPC_JIC, OPC_LDC2 */
     case OPC_BNEZC: /* OPC_JIALC, OPC_SDC2 */
         if (ctx->insn_flags & ISA_MIPS32R6) {
             if (rs != 0) {
                 /* OPC_BEQZC, OPC_BNEZC */
                 gen_compute_compact_branch(ctx, op, rs, 0,
                                            sextract32(ctx->opcode << 2, 0, 23));
             } else {
                 /* OPC_JIC, OPC_JIALC */
                 gen_compute_compact_branch(ctx, op, 0, rt, imm);
             }
         } else {
             /* OPC_LWC2, OPC_SWC2 */
             /* COP2: Not implemented. */
             generate_exception_err(ctx, EXCP_CpU, 2);
         }
         break;
     case OPC_CP2:
         check_insn(ctx, INSN_LOONGSON2F);
         /* Note that these instructions use different fields.  */
         gen_loongson_multimedia(ctx, sa, rd, rt);
         break;
 
     case OPC_CP3:
         check_insn_opc_removed(ctx, ISA_MIPS32R6);
         if (ctx->CP0_Config1 & (1 << CP0C1_FP)) {
             check_cp1_enabled(ctx);
             op1 = MASK_CP3(ctx->opcode);
             switch (op1) {
             case OPC_LWXC1:
             case OPC_LDXC1:
             case OPC_LUXC1:
             case OPC_SWXC1:
             case OPC_SDXC1:
             case OPC_SUXC1:
                 gen_flt3_ldst(ctx, op1, sa, rd, rs, rt);
                 break;
             case OPC_PREFX:
                 /* Treat as NOP. */
                 break;
             case OPC_ALNV_PS:
             case OPC_MADD_S:
             case OPC_MADD_D:
             case OPC_MADD_PS:
             case OPC_MSUB_S:
             case OPC_MSUB_D:
             case OPC_MSUB_PS:
             case OPC_NMADD_S:
             case OPC_NMADD_D:
             case OPC_NMADD_PS:
             case OPC_NMSUB_S:
             case OPC_NMSUB_D:
             case OPC_NMSUB_PS:
                 gen_flt3_arith(ctx, op1, sa, rs, rd, rt);
                 break;
             default:
                 MIPS_INVAL("cp3");
                 generate_exception (ctx, EXCP_RI);
                 break;
             }
         } else {
             generate_exception_err(ctx, EXCP_CpU, 1);
         }
         break;
 
 #if defined(TARGET_MIPS64)
     /* MIPS64 opcodes */
     case OPC_LDL: case OPC_LDR:
     case OPC_LLD:
         check_insn_opc_removed(ctx, ISA_MIPS32R6);
     case OPC_LWU:
     case OPC_LD:
         check_insn(ctx, ISA_MIPS3);
         check_mips_64(ctx);
         gen_ld(ctx, op, rt, rs, imm);
         break;
     case OPC_SDL: case OPC_SDR:
         check_insn_opc_removed(ctx, ISA_MIPS32R6);
     case OPC_SD:
         check_insn(ctx, ISA_MIPS3);
         check_mips_64(ctx);
         gen_st(ctx, op, rt, rs, imm);
         break;
     case OPC_SCD:
         check_insn_opc_removed(ctx, ISA_MIPS32R6);
         check_insn(ctx, ISA_MIPS3);
         check_mips_64(ctx);
         gen_st_cond(ctx, op, rt, rs, imm);
         break;
     case OPC_BNVC: /* OPC_BNEZALC, OPC_BNEC, OPC_DADDI */
         if (ctx->insn_flags & ISA_MIPS32R6) {
             /* OPC_BNVC, OPC_BNEZALC, OPC_BNEC */
             gen_compute_compact_branch(ctx, op, rs, rt, (uint32_t)imm << 2);
         } else {
             /* OPC_DADDI */
             check_insn(ctx, ISA_MIPS3);
             check_mips_64(ctx);
             gen_arith_imm(ctx, op, rt, rs, imm);
         }
         break;
     case OPC_DADDIU:
         check_insn(ctx, ISA_MIPS3);
         check_mips_64(ctx);
         gen_arith_imm(ctx, op, rt, rs, imm);
         break;
 #else
     case OPC_BNVC: /* OPC_BNEZALC, OPC_BNEC */
         if (ctx->insn_flags & ISA_MIPS32R6) {
             gen_compute_compact_branch(ctx, op, rs, rt, (uint32_t)imm << 2);
         } else {
             MIPS_INVAL("major opcode");
             generate_exception(ctx, EXCP_RI);
         }
         break;
 #endif
     case OPC_DAUI: /* OPC_JALX */
         if (ctx->insn_flags & ISA_MIPS32R6) {
 #if defined(TARGET_MIPS64)
             /* OPC_DAUI */
             check_mips_64(ctx);
             if (rt != 0) {
                 TCGv t0 = tcg_temp_new(tcg_ctx);
                 gen_load_gpr(ctx, t0, rs);
                 tcg_gen_addi_tl(tcg_ctx, *cpu_gpr[rt], t0, (uint32_t)imm << 16);
                 tcg_temp_free(tcg_ctx, t0);
             }
             MIPS_DEBUG("daui %s, %s, %04x", regnames[rt], regnames[rs], imm);
 #else
             generate_exception(ctx, EXCP_RI);
             MIPS_INVAL("major opcode");
 #endif
         } else {
             /* OPC_JALX */
             check_insn(ctx, ASE_MIPS16 | ASE_MICROMIPS);
             offset = (int32_t)(ctx->opcode & 0x3FFFFFF) << 2;
             gen_compute_branch(ctx, op, 4, rs, rt, offset, 4);
         }
         break;
     case OPC_MSA: /* OPC_MDMX */
         /* MDMX: Not implemented. */
         gen_msa(env, ctx);
         break;
     case OPC_PCREL:
         check_insn(ctx, ISA_MIPS32R6);
         gen_pcrel(ctx, rs, imm);
         break;
     default:            /* Invalid */
         MIPS_INVAL("major opcode");
         generate_exception(ctx, EXCP_RI);
         break;
     }
 }
diff --git a/qemu/target-sparc/helper.c b/qemu/target-sparc/helper.c
index e4ae5d40..602a8194 100644
--- a/qemu/target-sparc/helper.c
+++ b/qemu/target-sparc/helper.c
@@ -71,29 +71,29 @@ void helper_tick_set_limit(void *opaque, uint64_t limit)
 static target_ulong helper_udiv_common(CPUSPARCState *env, target_ulong a,
                                        target_ulong b, int cc)
 {
     SPARCCPU *cpu = sparc_env_get_cpu(env);
     int overflow = 0;
     uint64_t x0;
     uint32_t x1;
 
-    x0 = (a & 0xffffffff) | ((int64_t) (env->y) << 32);
+    x0 = (a & 0xffffffff) | ((uint64_t) (env->y) << 32);
     x1 = (b & 0xffffffff);
 
     if (x1 == 0) {
         cpu_restore_state(CPU(cpu), GETPC());
         helper_raise_exception(env, TT_DIV_ZERO);
     }
 
     x0 = x0 / x1;
     if (x0 > UINT32_MAX) {
         x0 = UINT32_MAX;
         overflow = 1;
     }
 
     if (cc) {
         env->cc_dst = x0;
         env->cc_src2 = overflow;
         env->cc_op = CC_OP_DIV;
     }
     return x0;
 }
diff --git a/qemu/tcg/optimize.c b/qemu/tcg/optimize.c
index 3316b76c..cb9626de 100644
--- a/qemu/tcg/optimize.c
+++ b/qemu/tcg/optimize.c
@@ -535,860 +535,863 @@ static bool swap_commutative2(TCGContext *s, TCGArg *p1, TCGArg *p2)
 /* Propagate constants and copies, fold constant expressions. */
 static TCGArg *tcg_constant_folding(TCGContext *s, uint16_t *tcg_opc_ptr,
                                     TCGArg *args, TCGOpDef *tcg_op_defs)
 {
     struct tcg_temp_info *temps = s->temps2;
     int nb_ops, op_index, nb_temps, nb_globals;
     TCGArg *gen_args;
 
     /* Array VALS has an element for each temp.
        If this temp holds a constant then its value is kept in VALS' element.
        If this temp is a copy of other ones then the other copies are
        available through the doubly linked circular list. */
 
     nb_temps = s->nb_temps;
     nb_globals = s->nb_globals;
     reset_all_temps(s, nb_temps);
 
     nb_ops = tcg_opc_ptr - s->gen_opc_buf;
     gen_args = args;
     for (op_index = 0; op_index < nb_ops; op_index++) {
         TCGOpcode op = s->gen_opc_buf[op_index];
         const TCGOpDef *def = &tcg_op_defs[op];
         tcg_target_ulong mask, partmask, affected;
         int nb_oargs, nb_iargs, nb_args, i;
         TCGArg tmp;
 
         if (op == INDEX_op_call) {
             *gen_args++ = tmp = *args++;
             nb_oargs = tmp >> 16;
             nb_iargs = tmp & 0xffff;
             nb_args = nb_oargs + nb_iargs + def->nb_cargs;
         } else {
             nb_oargs = def->nb_oargs;
             nb_iargs = def->nb_iargs;
             nb_args = def->nb_args;
         }
 
         /* Do copy propagation */
         for (i = nb_oargs; i < nb_oargs + nb_iargs; i++) {
+            if (args[i] >= TCG_MAX_TEMPS) {
+                return NULL;
+            }
             if (temps[args[i]].state == TCG_TEMP_COPY) {
                 args[i] = find_better_copy(s, args[i]);
             }
         }
 
         /* For commutative operations make constant second argument */
         switch (op) {
         CASE_OP_32_64(add):
         CASE_OP_32_64(mul):
         CASE_OP_32_64(and):
         CASE_OP_32_64(or):
         CASE_OP_32_64(xor):
         CASE_OP_32_64(eqv):
         CASE_OP_32_64(nand):
         CASE_OP_32_64(nor):
         CASE_OP_32_64(muluh):
         CASE_OP_32_64(mulsh):
             swap_commutative(s, args[0], &args[1], &args[2]);
             break;
         CASE_OP_32_64(brcond):
             if (swap_commutative(s, -1, &args[0], &args[1])) {
                 args[2] = tcg_swap_cond(args[2]);
             }
             break;
         CASE_OP_32_64(setcond):
             if (swap_commutative(s, args[0], &args[1], &args[2])) {
                 args[3] = tcg_swap_cond(args[3]);
             }
             break;
         CASE_OP_32_64(movcond):
             if (swap_commutative(s, -1, &args[1], &args[2])) {
                 args[5] = tcg_swap_cond(args[5]);
             }
             /* For movcond, we canonicalize the "false" input reg to match
                the destination reg so that the tcg backend can implement
                a "move if true" operation.  */
             if (swap_commutative(s, args[0], &args[4], &args[3])) {
                 args[5] = tcg_invert_cond(args[5]);
             }
             break;
         CASE_OP_32_64(add2):
             swap_commutative(s, args[0], &args[2], &args[4]);
             swap_commutative(s, args[1], &args[3], &args[5]);
             break;
         CASE_OP_32_64(mulu2):
         CASE_OP_32_64(muls2):
             swap_commutative(s, args[0], &args[2], &args[3]);
             break;
         case INDEX_op_brcond2_i32:
             if (swap_commutative2(s, &args[0], &args[2])) {
                 args[4] = tcg_swap_cond(args[4]);
             }
             break;
         case INDEX_op_setcond2_i32:
             if (swap_commutative2(s, &args[1], &args[3])) {
                 args[5] = tcg_swap_cond(args[5]);
             }
             break;
         default:
             break;
         }
 
         /* Simplify expressions for "shift/rot r, 0, a => movi r, 0",
            and "sub r, 0, a => neg r, a" case.  */
         switch (op) {
         CASE_OP_32_64(shl):
         CASE_OP_32_64(shr):
         CASE_OP_32_64(sar):
         CASE_OP_32_64(rotl):
         CASE_OP_32_64(rotr):
             if (temps[args[1]].state == TCG_TEMP_CONST
                 && temps[args[1]].val == 0) {
                 tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], 0);
                 args += 3;
                 gen_args += 2;
                 continue;
             }
             break;
         CASE_OP_32_64(sub):
             {
                 TCGOpcode neg_op;
                 bool have_neg;
 
                 if (temps[args[2]].state == TCG_TEMP_CONST) {
                     /* Proceed with possible constant folding. */
                     break;
                 }
                 if (op == INDEX_op_sub_i32) {
                     neg_op = INDEX_op_neg_i32;
                     have_neg = TCG_TARGET_HAS_neg_i32;
                 } else {
                     neg_op = INDEX_op_neg_i64;
                     have_neg = TCG_TARGET_HAS_neg_i64;
                 }
                 if (!have_neg) {
                     break;
                 }
                 if (temps[args[1]].state == TCG_TEMP_CONST
                     && temps[args[1]].val == 0) {
                     s->gen_opc_buf[op_index] = neg_op;
                     reset_temp(s, args[0]);
                     gen_args[0] = args[0];
                     gen_args[1] = args[2];
                     args += 3;
                     gen_args += 2;
                     continue;
                 }
             }
             break;
         CASE_OP_32_64(xor):
         CASE_OP_32_64(nand):
             if (temps[args[1]].state != TCG_TEMP_CONST
                 && temps[args[2]].state == TCG_TEMP_CONST
                 && temps[args[2]].val == -1) {
                 i = 1;
                 goto try_not;
             }
             break;
         CASE_OP_32_64(nor):
             if (temps[args[1]].state != TCG_TEMP_CONST
                 && temps[args[2]].state == TCG_TEMP_CONST
                 && temps[args[2]].val == 0) {
                 i = 1;
                 goto try_not;
             }
             break;
         CASE_OP_32_64(andc):
             if (temps[args[2]].state != TCG_TEMP_CONST
                 && temps[args[1]].state == TCG_TEMP_CONST
                 && temps[args[1]].val == -1) {
                 i = 2;
                 goto try_not;
             }
             break;
         CASE_OP_32_64(orc):
         CASE_OP_32_64(eqv):
             if (temps[args[2]].state != TCG_TEMP_CONST
                 && temps[args[1]].state == TCG_TEMP_CONST
                 && temps[args[1]].val == 0) {
                 i = 2;
                 goto try_not;
             }
             break;
         try_not:
             {
                 TCGOpcode not_op;
                 bool have_not;
 
                 if (def->flags & TCG_OPF_64BIT) {
                     not_op = INDEX_op_not_i64;
                     have_not = TCG_TARGET_HAS_not_i64;
                 } else {
                     not_op = INDEX_op_not_i32;
                     have_not = TCG_TARGET_HAS_not_i32;
                 }
                 if (!have_not) {
                     break;
                 }
                 s->gen_opc_buf[op_index] = not_op;
                 reset_temp(s, args[0]);
                 gen_args[0] = args[0];
                 gen_args[1] = args[i];
                 args += 3;
                 gen_args += 2;
                 continue;
             }
         default:
             break;
         }
 
         /* Simplify expression for "op r, a, const => mov r, a" cases */
         switch (op) {
         CASE_OP_32_64(add):
         CASE_OP_32_64(sub):
         CASE_OP_32_64(shl):
         CASE_OP_32_64(shr):
         CASE_OP_32_64(sar):
         CASE_OP_32_64(rotl):
         CASE_OP_32_64(rotr):
         CASE_OP_32_64(or):
         CASE_OP_32_64(xor):
         CASE_OP_32_64(andc):
             if (temps[args[1]].state != TCG_TEMP_CONST
                 && temps[args[2]].state == TCG_TEMP_CONST
                 && temps[args[2]].val == 0) {
                 goto do_mov3;
             }
             break;
         CASE_OP_32_64(and):
         CASE_OP_32_64(orc):
         CASE_OP_32_64(eqv):
             if (temps[args[1]].state != TCG_TEMP_CONST
                 && temps[args[2]].state == TCG_TEMP_CONST
                 && temps[args[2]].val == -1) {
                 goto do_mov3;
             }
             break;
         do_mov3:
             if (temps_are_copies(s, args[0], args[1])) {
                 s->gen_opc_buf[op_index] = INDEX_op_nop;
             } else {
                 tcg_opt_gen_mov(s, op_index, gen_args, op, args[0], args[1]);
                 gen_args += 2;
             }
             args += 3;
             continue;
         default:
             break;
         }
 
         /* Simplify using known-zero bits. Currently only ops with a single
            output argument is supported. */
         mask = -1;
         affected = -1;
         switch (op) {
         CASE_OP_32_64(ext8s):
             if ((temps[args[1]].mask & 0x80) != 0) {
                 break;
             }
         CASE_OP_32_64(ext8u):
             mask = 0xff;
             goto and_const;
         CASE_OP_32_64(ext16s):
             if ((temps[args[1]].mask & 0x8000) != 0) {
                 break;
             }
         CASE_OP_32_64(ext16u):
             mask = 0xffff;
             goto and_const;
         case INDEX_op_ext32s_i64:
             if ((temps[args[1]].mask & 0x80000000) != 0) {
                 break;
             }
         case INDEX_op_ext32u_i64:
             mask = 0xffffffffU;
             goto and_const;
 
         CASE_OP_32_64(and):
             mask = temps[args[2]].mask;
             if (temps[args[2]].state == TCG_TEMP_CONST) {
         and_const:
                 affected = temps[args[1]].mask & ~mask;
             }
             mask = temps[args[1]].mask & mask;
             break;
 
         CASE_OP_32_64(andc):
             /* Known-zeros does not imply known-ones.  Therefore unless
                args[2] is constant, we can't infer anything from it.  */
             if (temps[args[2]].state == TCG_TEMP_CONST) {
                 mask = ~temps[args[2]].mask;
                 goto and_const;
             }
             /* But we certainly know nothing outside args[1] may be set. */
             mask = temps[args[1]].mask;
             break;
 
         case INDEX_op_sar_i32:
             if (temps[args[2]].state == TCG_TEMP_CONST) {
                 tmp = temps[args[2]].val & 31;
                 mask = (int32_t)temps[args[1]].mask >> tmp;
             }
             break;
         case INDEX_op_sar_i64:
             if (temps[args[2]].state == TCG_TEMP_CONST) {
                 tmp = temps[args[2]].val & 63;
                 mask = (int64_t)temps[args[1]].mask >> tmp;
             }
             break;
 
         case INDEX_op_shr_i32:
             if (temps[args[2]].state == TCG_TEMP_CONST) {
                 tmp = temps[args[2]].val & 31;
                 mask = (uint32_t)temps[args[1]].mask >> tmp;
             }
             break;
         case INDEX_op_shr_i64:
             if (temps[args[2]].state == TCG_TEMP_CONST) {
                 tmp = temps[args[2]].val & 63;
                 mask = (uint64_t)temps[args[1]].mask >> tmp;
             }
             break;
 
         case INDEX_op_trunc_shr_i32:
             mask = (uint64_t)temps[args[1]].mask >> args[2];
             break;
 
         CASE_OP_32_64(shl):
             if (temps[args[2]].state == TCG_TEMP_CONST) {
                 tmp = temps[args[2]].val & (TCG_TARGET_REG_BITS - 1);
                 mask = temps[args[1]].mask << tmp;
             }
             break;
 
         CASE_OP_32_64(neg):
             /* Set to 1 all bits to the left of the rightmost.  */
             mask = 0-(temps[args[1]].mask & (0-temps[args[1]].mask));
             break;
 
         CASE_OP_32_64(deposit):
             mask = (tcg_target_ulong)deposit64(temps[args[1]].mask, args[3], args[4],
                              temps[args[2]].mask);
             break;
 
         CASE_OP_32_64(or):
         CASE_OP_32_64(xor):
             mask = temps[args[1]].mask | temps[args[2]].mask;
             break;
 
         CASE_OP_32_64(setcond):
         case INDEX_op_setcond2_i32:
             mask = 1;
             break;
 
         CASE_OP_32_64(movcond):
             mask = temps[args[3]].mask | temps[args[4]].mask;
             break;
 
         CASE_OP_32_64(ld8u):
             mask = 0xff;
             break;
         CASE_OP_32_64(ld16u):
             mask = 0xffff;
             break;
         case INDEX_op_ld32u_i64:
             mask = 0xffffffffu;
             break;
 
         CASE_OP_32_64(qemu_ld):
             {
                 TCGMemOp mop = args[nb_oargs + nb_iargs];
                 if (!(mop & MO_SIGN)) {
                     mask = (2ULL << ((8 << (mop & MO_SIZE)) - 1)) - 1;
                 }
             }
             break;
 
         default:
             break;
         }
 
         /* 32-bit ops generate 32-bit results.  For the result is zero test
            below, we can ignore high bits, but for further optimizations we
            need to record that the high bits contain garbage.  */
         partmask = mask;
         if (!(def->flags & TCG_OPF_64BIT)) {
             mask |= ~(tcg_target_ulong)0xffffffffu;
             partmask &= 0xffffffffu;
             affected &= 0xffffffffu;
         }
 
         if (partmask == 0) {
             assert(nb_oargs == 1);
             tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], 0);
             args += nb_args;
             gen_args += 2;
             continue;
         }
         if (affected == 0) {
             assert(nb_oargs == 1);
             if (temps_are_copies(s, args[0], args[1])) {
                 s->gen_opc_buf[op_index] = INDEX_op_nop;
             } else if (temps[args[1]].state != TCG_TEMP_CONST) {
                 tcg_opt_gen_mov(s, op_index, gen_args, op, args[0], args[1]);
                 gen_args += 2;
             } else {
                 tcg_opt_gen_movi(s, op_index, gen_args, op,
                                  args[0], temps[args[1]].val);
                 gen_args += 2;
             }
             args += nb_args;
             continue;
         }
 
         /* Simplify expression for "op r, a, 0 => movi r, 0" cases */
         switch (op) {
         CASE_OP_32_64(and):
         CASE_OP_32_64(mul):
         CASE_OP_32_64(muluh):
         CASE_OP_32_64(mulsh):
             if ((temps[args[2]].state == TCG_TEMP_CONST
                 && temps[args[2]].val == 0)) {
                 tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], 0);
                 args += 3;
                 gen_args += 2;
                 continue;
             }
             break;
         default:
             break;
         }
 
         /* Simplify expression for "op r, a, a => mov r, a" cases */
         switch (op) {
         CASE_OP_32_64(or):
         CASE_OP_32_64(and):
             if (temps_are_copies(s, args[1], args[2])) {
                 if (temps_are_copies(s, args[0], args[1])) {
                     s->gen_opc_buf[op_index] = INDEX_op_nop;
                 } else {
                     tcg_opt_gen_mov(s, op_index, gen_args, op,
                                     args[0], args[1]);
                     gen_args += 2;
                 }
                 args += 3;
                 continue;
             }
             break;
         default:
             break;
         }
 
         /* Simplify expression for "op r, a, a => movi r, 0" cases */
         switch (op) {
         CASE_OP_32_64(andc):
         CASE_OP_32_64(sub):
         CASE_OP_32_64(xor):
             if (temps_are_copies(s, args[1], args[2])) {
                 tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], 0);
                 gen_args += 2;
                 args += 3;
                 continue;
             }
             break;
         default:
             break;
         }
 
         /* Propagate constants through copy operations and do constant
            folding.  Constants will be substituted to arguments by register
            allocator where needed and possible.  Also detect copies. */
         switch (op) {
         CASE_OP_32_64(mov):
             if (temps_are_copies(s, args[0], args[1])) {
                 args += 2;
                 s->gen_opc_buf[op_index] = INDEX_op_nop;
                 break;
             }
             if (temps[args[1]].state != TCG_TEMP_CONST) {
                 tcg_opt_gen_mov(s, op_index, gen_args, op, args[0], args[1]);
                 gen_args += 2;
                 args += 2;
                 break;
             }
             /* Source argument is constant.  Rewrite the operation and
                let movi case handle it. */
             args[1] = temps[args[1]].val;
             /* fallthrough */
         CASE_OP_32_64(movi):
             tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], args[1]);
             gen_args += 2;
             args += 2;
             break;
 
         CASE_OP_32_64(not):
         CASE_OP_32_64(neg):
         CASE_OP_32_64(ext8s):
         CASE_OP_32_64(ext8u):
         CASE_OP_32_64(ext16s):
         CASE_OP_32_64(ext16u):
         case INDEX_op_ext32s_i64:
         case INDEX_op_ext32u_i64:
             if (temps[args[1]].state == TCG_TEMP_CONST) {
                 tmp = do_constant_folding(s, op, temps[args[1]].val, 0);
                 tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], tmp);
                 gen_args += 2;
                 args += 2;
                 break;
             }
             goto do_default;
 
         case INDEX_op_trunc_shr_i32:
             if (temps[args[1]].state == TCG_TEMP_CONST) {
                 tmp = do_constant_folding(s, op, temps[args[1]].val, args[2]);
                 tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], tmp);
                 gen_args += 2;
                 args += 3;
                 break;
             }
             goto do_default;
 
         CASE_OP_32_64(add):
         CASE_OP_32_64(sub):
         CASE_OP_32_64(mul):
         CASE_OP_32_64(or):
         CASE_OP_32_64(and):
         CASE_OP_32_64(xor):
         CASE_OP_32_64(shl):
         CASE_OP_32_64(shr):
         CASE_OP_32_64(sar):
         CASE_OP_32_64(rotl):
         CASE_OP_32_64(rotr):
         CASE_OP_32_64(andc):
         CASE_OP_32_64(orc):
         CASE_OP_32_64(eqv):
         CASE_OP_32_64(nand):
         CASE_OP_32_64(nor):
         CASE_OP_32_64(muluh):
         CASE_OP_32_64(mulsh):
         CASE_OP_32_64(div):
         CASE_OP_32_64(divu):
         CASE_OP_32_64(rem):
         CASE_OP_32_64(remu):
             if (temps[args[1]].state == TCG_TEMP_CONST
                 && temps[args[2]].state == TCG_TEMP_CONST) {
                 tmp = do_constant_folding(s, op, temps[args[1]].val,
                                           temps[args[2]].val);
                 tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], tmp);
                 gen_args += 2;
                 args += 3;
                 break;
             }
             goto do_default;
 
         CASE_OP_32_64(deposit):
             if (temps[args[1]].state == TCG_TEMP_CONST
                 && temps[args[2]].state == TCG_TEMP_CONST) {
                 tmp = (TCGArg)deposit64(temps[args[1]].val, args[3], args[4],
                                 temps[args[2]].val);
                 tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], tmp);
                 gen_args += 2;
                 args += 5;
                 break;
             }
             goto do_default;
 
         CASE_OP_32_64(setcond):
             tmp = do_constant_folding_cond(s, op, args[1], args[2], args[3]);
             if (tmp != 2) {
                 tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], tmp);
                 gen_args += 2;
                 args += 4;
                 break;
             }
             goto do_default;
 
         CASE_OP_32_64(brcond):
             tmp = do_constant_folding_cond(s, op, args[0], args[1], args[2]);
             if (tmp != 2) {
                 if (tmp) {
                     reset_all_temps(s, nb_temps);
                     s->gen_opc_buf[op_index] = INDEX_op_br;
                     gen_args[0] = args[3];
                     gen_args += 1;
                 } else {
                     s->gen_opc_buf[op_index] = INDEX_op_nop;
                 }
                 args += 4;
                 break;
             }
             goto do_default;
 
         CASE_OP_32_64(movcond):
             tmp = do_constant_folding_cond(s, op, args[1], args[2], args[5]);
             if (tmp != 2) {
                 if (temps_are_copies(s, args[0], args[4-tmp])) {
                     s->gen_opc_buf[op_index] = INDEX_op_nop;
                 } else if (temps[args[4-tmp]].state == TCG_TEMP_CONST) {
                     tcg_opt_gen_movi(s, op_index, gen_args, op,
                                      args[0], temps[args[4-tmp]].val);
                     gen_args += 2;
                 } else {
                     tcg_opt_gen_mov(s, op_index, gen_args, op,
                                     args[0], args[4-tmp]);
                     gen_args += 2;
                 }
                 args += 6;
                 break;
             }
             goto do_default;
 
         case INDEX_op_add2_i32:
         case INDEX_op_sub2_i32:
             if (temps[args[2]].state == TCG_TEMP_CONST
                 && temps[args[3]].state == TCG_TEMP_CONST
                 && temps[args[4]].state == TCG_TEMP_CONST
                 && temps[args[5]].state == TCG_TEMP_CONST) {
                 uint32_t al = temps[args[2]].val;
                 uint32_t ah = temps[args[3]].val;
                 uint32_t bl = temps[args[4]].val;
                 uint32_t bh = temps[args[5]].val;
                 uint64_t a = ((uint64_t)ah << 32) | al;
                 uint64_t b = ((uint64_t)bh << 32) | bl;
                 TCGArg rl, rh;
 
                 if (op == INDEX_op_add2_i32) {
                     a += b;
                 } else {
                     a -= b;
                 }
 
                 /* We emit the extra nop when we emit the add2/sub2.  */
                 assert(s->gen_opc_buf[op_index + 1] == INDEX_op_nop);
 
                 rl = args[0];
                 rh = args[1];
                 tcg_opt_gen_movi(s, op_index, &gen_args[0],
                                  op, rl, (uint32_t)a);
                 tcg_opt_gen_movi(s, ++op_index, &gen_args[2],
                                  op, rh, (uint32_t)(a >> 32));
                 gen_args += 4;
                 args += 6;
                 break;
             }
             goto do_default;
 
         case INDEX_op_mulu2_i32:
             if (temps[args[2]].state == TCG_TEMP_CONST
                 && temps[args[3]].state == TCG_TEMP_CONST) {
                 uint32_t a = temps[args[2]].val;
                 uint32_t b = temps[args[3]].val;
                 uint64_t r = (uint64_t)a * b;
                 TCGArg rl, rh;
 
                 /* We emit the extra nop when we emit the mulu2.  */
                 assert(s->gen_opc_buf[op_index + 1] == INDEX_op_nop);
 
                 rl = args[0];
                 rh = args[1];
                 tcg_opt_gen_movi(s, op_index, &gen_args[0],
                                  op, rl, (uint32_t)r);
                 tcg_opt_gen_movi(s, ++op_index, &gen_args[2],
                                  op, rh, (uint32_t)(r >> 32));
                 gen_args += 4;
                 args += 4;
                 break;
             }
             goto do_default;
 
         case INDEX_op_brcond2_i32:
             tmp = do_constant_folding_cond2(s, &args[0], &args[2], args[4]);
             if (tmp != 2) {
                 if (tmp) {
             do_brcond_true:
                     reset_all_temps(s, nb_temps);
                     s->gen_opc_buf[op_index] = INDEX_op_br;
                     gen_args[0] = args[5];
                     gen_args += 1;
                 } else {
             do_brcond_false:
                     s->gen_opc_buf[op_index] = INDEX_op_nop;
                 }
             } else if ((args[4] == TCG_COND_LT || args[4] == TCG_COND_GE)
                        && temps[args[2]].state == TCG_TEMP_CONST
                        && temps[args[3]].state == TCG_TEMP_CONST
                        && temps[args[2]].val == 0
                        && temps[args[3]].val == 0) {
                 /* Simplify LT/GE comparisons vs zero to a single compare
                    vs the high word of the input.  */
             do_brcond_high:
                 reset_all_temps(s, nb_temps);
                 s->gen_opc_buf[op_index] = INDEX_op_brcond_i32;
                 gen_args[0] = args[1];
                 gen_args[1] = args[3];
                 gen_args[2] = args[4];
                 gen_args[3] = args[5];
                 gen_args += 4;
             } else if (args[4] == TCG_COND_EQ) {
                 /* Simplify EQ comparisons where one of the pairs
                    can be simplified.  */
                 tmp = do_constant_folding_cond(s, INDEX_op_brcond_i32,
                                                args[0], args[2], TCG_COND_EQ);
                 if (tmp == 0) {
                     goto do_brcond_false;
                 } else if (tmp == 1) {
                     goto do_brcond_high;
                 }
                 tmp = do_constant_folding_cond(s, INDEX_op_brcond_i32,
                                                args[1], args[3], TCG_COND_EQ);
                 if (tmp == 0) {
                     goto do_brcond_false;
                 } else if (tmp != 1) {
                     goto do_default;
                 }
             do_brcond_low:
                 reset_all_temps(s, nb_temps);
                 s->gen_opc_buf[op_index] = INDEX_op_brcond_i32;
                 gen_args[0] = args[0];
                 gen_args[1] = args[2];
                 gen_args[2] = args[4];
                 gen_args[3] = args[5];
                 gen_args += 4;
             } else if (args[4] == TCG_COND_NE) {
                 /* Simplify NE comparisons where one of the pairs
                    can be simplified.  */
                 tmp = do_constant_folding_cond(s, INDEX_op_brcond_i32,
                                                args[0], args[2], TCG_COND_NE);
                 if (tmp == 0) {
                     goto do_brcond_high;
                 } else if (tmp == 1) {
                     goto do_brcond_true;
                 }
                 tmp = do_constant_folding_cond(s, INDEX_op_brcond_i32,
                                                args[1], args[3], TCG_COND_NE);
                 if (tmp == 0) {
                     goto do_brcond_low;
                 } else if (tmp == 1) {
                     goto do_brcond_true;
                 }
                 goto do_default;
             } else {
                 goto do_default;
             }
             args += 6;
             break;
 
         case INDEX_op_setcond2_i32:
             tmp = do_constant_folding_cond2(s, &args[1], &args[3], args[5]);
             if (tmp != 2) {
             do_setcond_const:
                 tcg_opt_gen_movi(s, op_index, gen_args, op, args[0], tmp);
                 gen_args += 2;
             } else if ((args[5] == TCG_COND_LT || args[5] == TCG_COND_GE)
                        && temps[args[3]].state == TCG_TEMP_CONST
                        && temps[args[4]].state == TCG_TEMP_CONST
                        && temps[args[3]].val == 0
                        && temps[args[4]].val == 0) {
                 /* Simplify LT/GE comparisons vs zero to a single compare
                    vs the high word of the input.  */
             do_setcond_high:
                 s->gen_opc_buf[op_index] = INDEX_op_setcond_i32;
                 reset_temp(s, args[0]);
                 temps[args[0]].mask = 1;
                 gen_args[0] = args[0];
                 gen_args[1] = args[2];
                 gen_args[2] = args[4];
                 gen_args[3] = args[5];
                 gen_args += 4;
             } else if (args[5] == TCG_COND_EQ) {
                 /* Simplify EQ comparisons where one of the pairs
                    can be simplified.  */
                 tmp = do_constant_folding_cond(s, INDEX_op_setcond_i32,
                                                args[1], args[3], TCG_COND_EQ);
                 if (tmp == 0) {
                     goto do_setcond_const;
                 } else if (tmp == 1) {
                     goto do_setcond_high;
                 }
                 tmp = do_constant_folding_cond(s, INDEX_op_setcond_i32,
                                                args[2], args[4], TCG_COND_EQ);
                 if (tmp == 0) {
                     goto do_setcond_high;
                 } else if (tmp != 1) {
                     goto do_default;
                 }
             do_setcond_low:
                 reset_temp(s, args[0]);
                 temps[args[0]].mask = 1;
                 s->gen_opc_buf[op_index] = INDEX_op_setcond_i32;
                 gen_args[0] = args[0];
                 gen_args[1] = args[1];
                 gen_args[2] = args[3];
                 gen_args[3] = args[5];
                 gen_args += 4;
             } else if (args[5] == TCG_COND_NE) {
                 /* Simplify NE comparisons where one of the pairs
                    can be simplified.  */
                 tmp = do_constant_folding_cond(s, INDEX_op_setcond_i32,
                                                args[1], args[3], TCG_COND_NE);
                 if (tmp == 0) {
                     goto do_setcond_high;
                 } else if (tmp == 1) {
                     goto do_setcond_const;
                 }
                 tmp = do_constant_folding_cond(s, INDEX_op_setcond_i32,
                                                args[2], args[4], TCG_COND_NE);
                 if (tmp == 0) {
                     goto do_setcond_low;
                 } else if (tmp == 1) {
                     goto do_setcond_const;
                 }
                 goto do_default;
             } else {
                 goto do_default;
             }
             args += 6;
             break;
 
         case INDEX_op_call:
             if (!(args[nb_oargs + nb_iargs + 1]
                   & (TCG_CALL_NO_READ_GLOBALS | TCG_CALL_NO_WRITE_GLOBALS))) {
                 for (i = 0; i < nb_globals; i++) {
                     reset_temp(s, i);
                 }
             }
             goto do_reset_output;
 
         default:
         do_default:
             /* Default case: we know nothing about operation (or were unable
                to compute the operation result) so no propagation is done.
                We trash everything if the operation is the end of a basic
                block, otherwise we only trash the output args.  "mask" is
                the non-zero bits mask for the first output arg.  */
             if (def->flags & TCG_OPF_BB_END) {
                 reset_all_temps(s, nb_temps);
             } else {
         do_reset_output:
                 for (i = 0; i < nb_oargs; i++) {
                     if (args[i] >= TCG_MAX_TEMPS) {
                         continue;
                     }
                     reset_temp(s, args[i]);
                     /* Save the corresponding known-zero bits mask for the
                        first output argument (only one supported so far). */
                     if (i == 0) {
                         temps[args[i]].mask = mask;
                     }
                 }
             }
             for (i = 0; i < nb_args; i++) {
                 gen_args[i] = args[i];
             }
             args += nb_args;
             gen_args += nb_args;
             break;
         }
     }
 
     return gen_args;
 }
diff --git a/qemu/tcg/tcg.c b/qemu/tcg/tcg.c
index fd0b2871..4d332129 100644
--- a/qemu/tcg/tcg.c
+++ b/qemu/tcg/tcg.c
@@ -2525,163 +2525,171 @@ static void dump_op_count(void)
 static inline int tcg_gen_code_common(TCGContext *s,
                                       tcg_insn_unit *gen_code_buf,
                                       long search_pc)
 {
     TCGOpcode opc;
     int op_index;
     const TCGOpDef *def;
     const TCGArg *args;
 
 #ifdef DEBUG_DISAS
     if (unlikely(qemu_loglevel_mask(CPU_LOG_TB_OP))) {
         qemu_log("OP:\n");
         tcg_dump_ops(s);
         qemu_log("\n");
     }
 #endif
 
 #ifdef CONFIG_PROFILER
     s->opt_time -= profile_getclock();
 #endif
 
 #ifdef USE_TCG_OPTIMIZATIONS
     s->gen_opparam_ptr =
         tcg_optimize(s, s->gen_opc_ptr, s->gen_opparam_buf, s->tcg_op_defs);
+    if (s->gen_opparam_ptr == NULL) {
+        tcg_out_tb_finalize(s);
+        return -2;
+    }
 #endif
 
 #ifdef CONFIG_PROFILER
     s->opt_time += profile_getclock();
     s->la_time -= profile_getclock();
 #endif
 
     tcg_liveness_analysis(s);
 
 #ifdef CONFIG_PROFILER
     s->la_time += profile_getclock();
 #endif
 
 #ifdef DEBUG_DISAS
     if (unlikely(qemu_loglevel_mask(CPU_LOG_TB_OP_OPT))) {
         qemu_log("OP after optimization and liveness analysis:\n");
         tcg_dump_ops(s);
         qemu_log("\n");
     }
 #endif
 
     tcg_reg_alloc_start(s);
 
     s->code_buf = gen_code_buf;
     s->code_ptr = gen_code_buf;
 
     tcg_out_tb_init(s);
 
     args = s->gen_opparam_buf;
     op_index = 0;
 
     for(;;) {
         opc = s->gen_opc_buf[op_index];
 #ifdef CONFIG_PROFILER
         tcg_table_op_count[opc]++;
 #endif
         def = &s->tcg_op_defs[opc];
 #if 0
         printf("%s: %d %d %d\n", def->name,
                def->nb_oargs, def->nb_iargs, def->nb_cargs);
         //        dump_regs(s);
 #endif
         switch(opc) {
         case INDEX_op_mov_i32:
         case INDEX_op_mov_i64:
             tcg_reg_alloc_mov(s, def, args, s->op_dead_args[op_index],
                               s->op_sync_args[op_index]);
             break;
         case INDEX_op_movi_i32:
         case INDEX_op_movi_i64:
             tcg_reg_alloc_movi(s, args, s->op_dead_args[op_index],
                                s->op_sync_args[op_index]);
             break;
         case INDEX_op_debug_insn_start:
             /* debug instruction */
             break;
         case INDEX_op_nop:
         case INDEX_op_nop1:
         case INDEX_op_nop2:
         case INDEX_op_nop3:
             break;
         case INDEX_op_nopn:
             args += args[0];
             goto next;
         case INDEX_op_discard:
             temp_dead(s, args[0]);
             break;
         case INDEX_op_set_label:
             tcg_reg_alloc_bb_end(s, s->reserved_regs);
             tcg_out_label(s, args[0], s->code_ptr);
             break;
         case INDEX_op_call:
             args += tcg_reg_alloc_call(s, def, opc, args,
                                        s->op_dead_args[op_index],
                                        s->op_sync_args[op_index]);
             goto next;
         case INDEX_op_end:
             goto the_end;
         default:
             /* Sanity check that we've not introduced any unhandled opcodes. */
             if (def->flags & TCG_OPF_NOT_PRESENT) {
                 tcg_abort();
             }
             /* Note: in order to speed up the code, it would be much
                faster to have specialized register allocator functions for
                some common argument patterns */
             tcg_reg_alloc_op(s, def, opc, args, s->op_dead_args[op_index],
                              s->op_sync_args[op_index]);
             break;
         }
         args += def->nb_args;
     next:
         if (search_pc >= 0 && (size_t)search_pc < tcg_current_code_size(s)) {
             return op_index;
         }
         op_index++;
 #ifndef NDEBUG
         check_regs(s);
 #endif
     }
  the_end:
     /* Generate TB finalization at the end of block */
     tcg_out_tb_finalize(s);
     return -1;
 }
 
 int tcg_gen_code(TCGContext *s, tcg_insn_unit *gen_code_buf)    // qq
 {
+    int ret;
 #ifdef CONFIG_PROFILER
     {
         int n;
         n = (s->gen_opc_ptr - s->gen_opc_buf);
         s->op_count += n;
         if (n > s->op_count_max)
             s->op_count_max = n;
 
         s->temp_count += s->nb_temps;
         if (s->nb_temps > s->temp_count_max)
             s->temp_count_max = s->nb_temps;
     }
 #endif
 
     //printf("====== before gen code\n");
     //tcg_dump_ops(s);
-    tcg_gen_code_common(s, gen_code_buf, -1);   // qq
+    ret = tcg_gen_code_common(s, gen_code_buf, -1);   // qq
+    if (ret == -2) {
+        return -1;
+    }
 
     //printf("====== after gen code\n");
     //tcg_dump_ops(s);
 
     /* flush instruction cache */
     flush_icache_range((uintptr_t)s->code_buf, (uintptr_t)s->code_ptr);
 
     return tcg_current_code_size(s);
 }
 
 /* Return the index of the micro operation such as the pc after is <
    offset bytes from the start of the TB.  The contents of gen_code_buf must
    not be changed, though writing the same values is ok.
    Return -1 if not found. */
diff --git a/qemu/translate-all.c b/qemu/translate-all.c
index ed4beb1e..2deddd1e 100644
--- a/qemu/translate-all.c
+++ b/qemu/translate-all.c
@@ -194,66 +194,69 @@ void tb_cleanup(struct uc_struct *uc)
    '*gen_code_size_ptr' contains the size of the generated code (host
    code).
 */
 static int cpu_gen_code(CPUArchState *env, TranslationBlock *tb, int *gen_code_size_ptr)    // qq
 {
     TCGContext *s = env->uc->tcg_ctx;
     tcg_insn_unit *gen_code_buf;
     int gen_code_size;
 #ifdef CONFIG_PROFILER
     int64_t ti;
 #endif
 
 #ifdef CONFIG_PROFILER
     s->tb_count1++; /* includes aborted translations because of
                        exceptions */
     ti = profile_getclock();
 #endif
     tcg_func_start(s);
 
     gen_intermediate_code(env, tb);
 
     // Unicorn: when tracing block, patch block size operand for callback
     if (env->uc->size_arg != -1 && HOOK_EXISTS_BOUNDED(env->uc, UC_HOOK_BLOCK, tb->pc)) {
         if (env->uc->block_full)    // block size is unknown
             *(s->gen_opparam_buf + env->uc->size_arg) = 0;
         else
             *(s->gen_opparam_buf + env->uc->size_arg) = tb->size;
     }
 
     /* generate machine code */
     gen_code_buf = tb->tc_ptr;
     tb->tb_next_offset[0] = 0xffff;
     tb->tb_next_offset[1] = 0xffff;
     s->tb_next_offset = tb->tb_next_offset;
 #ifdef USE_DIRECT_JUMP
     s->tb_jmp_offset = tb->tb_jmp_offset;
     s->tb_next = NULL;
 #else
     s->tb_jmp_offset = NULL;
     s->tb_next = tb->tb_next;
 #endif
 
 #ifdef CONFIG_PROFILER
     s->tb_count++;
     s->interm_time += profile_getclock() - ti;
     s->code_time -= profile_getclock();
 #endif
     gen_code_size = tcg_gen_code(s, gen_code_buf);
+    if (gen_code_size == -1) {
+        return -1;
+    }
     //printf(">>> code size = %u: ", gen_code_size);
     //int i;
     //for (i = 0; i < gen_code_size; i++) {
     //    printf(" %02x", gen_code_buf[i]);
     //}
     //printf("\n");
     *gen_code_size_ptr = gen_code_size;
 #ifdef CONFIG_PROFILER
     s->code_time += profile_getclock();
     s->code_in_len += tb->size;
     s->code_out_len += gen_code_size;
 #endif
 
     return 0;
 }
 
 /* The cpu state corresponding to 'searched_pc' is restored.
  */
@@ -1124,47 +1127,52 @@ static void build_page_bitmap(PageDesc *p)
 TranslationBlock *tb_gen_code(CPUState *cpu,
                               target_ulong pc, target_ulong cs_base,
                               int flags, int cflags)    // qq
 {
     CPUArchState *env = cpu->env_ptr;
     TCGContext *tcg_ctx = env->uc->tcg_ctx;
     TranslationBlock *tb;
     tb_page_addr_t phys_pc, phys_page2;
     int code_gen_size;
+    int ret;
 
     phys_pc = get_page_addr_code(env, pc);
     tb = tb_alloc(env->uc, pc);
     if (!tb) {
         /* flush must be done */
         tb_flush(env);
         /* cannot fail at this point */
         tb = tb_alloc(env->uc, pc);
         /* Don't forget to invalidate previous TB info.  */
         tcg_ctx->tb_ctx.tb_invalidated_flag = 1;
     }
     tb->tc_ptr = tcg_ctx->code_gen_ptr;
     tb->cs_base = cs_base;
     tb->flags = flags;
     tb->cflags = cflags;
-    cpu_gen_code(env, tb, &code_gen_size);  // qq
+    ret = cpu_gen_code(env, tb, &code_gen_size);  // qq
+    if (ret == -1) {
+        tb_free(env->uc, tb);
+        return NULL;
+    }
     tcg_ctx->code_gen_ptr = (void *)(((uintptr_t)tcg_ctx->code_gen_ptr +
             code_gen_size + CODE_GEN_ALIGN - 1) & ~(CODE_GEN_ALIGN - 1));
 
     phys_page2 = -1;
     /* check next page if needed */
     if (tb->size) {
         target_ulong virt_page2 = (pc + tb->size - 1) & TARGET_PAGE_MASK;
         if ((pc & TARGET_PAGE_MASK) != virt_page2) {
             phys_page2 = get_page_addr_code(env, virt_page2);
         }
     }
     tb_link_page(cpu->uc, tb, phys_pc, phys_page2);
     return tb;
 }
 
 /*
  * Invalidate all TBs which intersect with the target physical address range
  * [start;end[. NOTE: start and end may refer to *different* physical pages.
  * 'is_cpu_write_access' should be true if called from a real cpu write
  * access: the virtual CPU will exit the current TB if code is modified inside
  * this TB.
  */
