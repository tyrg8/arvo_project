commit fe996504f76f4204074c1f8f681925852c61b777
Author: Victoria Zhislina <niva213@gmail.com>
Date:   Mon Nov 26 12:16:53 2018 -0800

    AVX2 ver of highb dr prediction - Z1 bug fixed
    
    Extracted from https://aomedia-review.googlesource.com/c/aom/+/75642.
    
    BUG=aomedia:2259,aomedia:2260,oss-fuzz:11517
    
    Change-Id: Ifc4d8e4703080a70aac1f97e5dbefb2ab2b8e33b

diff --git a/aom_dsp/x86/intrapred_avx2.c b/aom_dsp/x86/intrapred_avx2.c
index 4525f95136..5f3e7bb899 100644
--- a/aom_dsp/x86/intrapred_avx2.c
+++ b/aom_dsp/x86/intrapred_avx2.c
@@ -1258,12 +1258,11 @@ static AOM_FORCE_INLINE void highbd_dr_prediction_z1_16xN_internal_avx2(
 static void highbd_dr_prediction_z1_16xN_avx2(int N, uint16_t *dst,
                                               ptrdiff_t stride,
                                               const uint16_t *above,
                                               int upsample_above, int dx) {
   __m256i dstvec[64];
-
   highbd_dr_prediction_z1_16xN_internal_avx2(N, dstvec, above, upsample_above,
                                              dx);
   for (int i = 0; i < N; i++) {
     _mm256_storeu_si256((__m256i *)(dst + stride * i), dstvec[i]);
   }
 }
@@ -1271,95 +1270,95 @@ static void highbd_dr_prediction_z1_16xN_avx2(int N, uint16_t *dst,
 static AOM_FORCE_INLINE void highbd_dr_prediction_z1_32xN_internal_avx2(
     int N, __m256i *dstvec, const uint16_t *above, int upsample_above, int dx) {
   int x;
   // here upsample_above is 0 by design of av1_use_intra_edge_upsample
   (void)upsample_above;
   const int frac_bits = 6;
   const int max_base_x = ((32 + N) - 1);
 
   // pre-filter above pixels
   // store in temp buffers:
   //   above[x] * 32 + 16
   //   above[x+1] - above[x]
   // final pixels will be caluculated as:
   //   (above[x] * 32 + 16 + (above[x+1] - above[x]) * shift) >> 5
   __m256i a0, a0_1, a1, a1_1, a32, a16;
   __m256i a_mbase_x, diff, max_base_x256, base_inc256, mask256;
 
   a16 = _mm256_set1_epi32(16);
   a_mbase_x = _mm256_set1_epi16(above[max_base_x]);
   max_base_x256 = _mm256_set1_epi16(max_base_x);
 
   x = dx;
   for (int r = 0; r < N; r++) {
     __m256i b, res[2], res1;
 
     int base = x >> frac_bits;
     if (base >= max_base_x) {
       for (int i = r; i < N; ++i) {
         dstvec[i] = a_mbase_x;  // save 32 values
         dstvec[i + N] = a_mbase_x;
       }
       return;
     }
 
     __m256i shift = _mm256_srli_epi32(
         _mm256_and_si256(_mm256_set1_epi32(x), _mm256_set1_epi32(0x3f)), 1);
 
     for (int j = 0; j < 32; j += 16) {
       int mdif = max_base_x - (base + j);
-      if (mdif == 0) {
+      if (mdif <= 0) {
         res1 = a_mbase_x;
       } else {
         a0 = _mm256_cvtepu16_epi32(
             _mm_loadu_si128((__m128i *)(above + base + j)));
         a1 = _mm256_cvtepu16_epi32(
             _mm_loadu_si128((__m128i *)(above + base + 1 + j)));
 
         diff = _mm256_sub_epi32(a1, a0);   // a[x+1] - a[x]
         a32 = _mm256_slli_epi32(a0, 5);    // a[x] * 32
         a32 = _mm256_add_epi32(a32, a16);  // a[x] * 32 + 16
         b = _mm256_mullo_epi32(diff, shift);
 
         res[0] = _mm256_add_epi32(a32, b);
         res[0] = _mm256_srli_epi32(res[0], 5);
         res[0] = _mm256_packus_epi32(
             res[0],
             _mm256_castsi128_si256(_mm256_extracti128_si256(res[0], 1)));
+        if (mdif > 8) {
+          a0_1 = _mm256_cvtepu16_epi32(
+              _mm_loadu_si128((__m128i *)(above + base + 8 + j)));
+          a1_1 = _mm256_cvtepu16_epi32(
+              _mm_loadu_si128((__m128i *)(above + base + 9 + j)));
 
-        a0_1 = _mm256_cvtepu16_epi32(
-            _mm_loadu_si128((__m128i *)(above + base + 8 + j)));
-        a1_1 = _mm256_cvtepu16_epi32(
-            _mm_loadu_si128((__m128i *)(above + base + 9 + j)));
+          diff = _mm256_sub_epi32(a1_1, a0_1);  // a[x+1] - a[x]
+          a32 = _mm256_slli_epi32(a0_1, 5);     // a[x] * 32
+          a32 = _mm256_add_epi32(a32, a16);     // a[x] * 32 + 16
+          b = _mm256_mullo_epi32(diff, shift);
 
-        diff = _mm256_sub_epi32(a1_1, a0_1);  // a[x+1] - a[x]
-        a32 = _mm256_slli_epi32(a0_1, 5);     // a[x] * 32
-        a32 = _mm256_add_epi32(a32, a16);     // a[x] * 32 + 16
-        b = _mm256_mullo_epi32(diff, shift);
-        if (mdif > 8) {
           res[1] = _mm256_add_epi32(a32, b);
           res[1] = _mm256_srli_epi32(res[1], 5);
           res[1] = _mm256_packus_epi32(
               res[1],
               _mm256_castsi128_si256(_mm256_extracti128_si256(res[1], 1)));
         } else {
           res[1] = a_mbase_x;
         }
         res1 = _mm256_inserti128_si256(res[0], _mm256_castsi256_si128(res[1]),
                                        1);  // 16 16bit values
         base_inc256 = _mm256_setr_epi16(
             base + j, base + j + 1, base + j + 2, base + j + 3, base + j + 4,
             base + j + 5, base + j + 6, base + j + 7, base + j + 8,
             base + j + 9, base + j + 10, base + j + 11, base + j + 12,
             base + j + 13, base + j + 14, base + j + 15);
 
         mask256 = _mm256_cmpgt_epi16(max_base_x256, base_inc256);
         res1 = _mm256_blendv_epi8(a_mbase_x, res1, mask256);
       }
       if (!j)
         dstvec[r] = res1;
       else
         dstvec[r + N] = res1;
     }
     x += dx;
   }
 }
@@ -1381,101 +1380,101 @@ static void highbd_dr_prediction_z1_32xN_avx2(int N, uint16_t *dst,
 static void highbd_dr_prediction_z1_64xN_avx2(int N, uint16_t *dst,
                                               ptrdiff_t stride,
                                               const uint16_t *above,
                                               int upsample_above, int dx) {
   int x;
 
   // here upsample_above is 0 by design of av1_use_intra_edge_upsample
   (void)upsample_above;
   const int frac_bits = 6;
   const int max_base_x = ((64 + N) - 1);
 
   // pre-filter above pixels
   // store in temp buffers:
   //   above[x] * 32 + 16
   //   above[x+1] - above[x]
   // final pixels will be caluculated as:
   //   (above[x] * 32 + 16 + (above[x+1] - above[x]) * shift) >> 5
   __m256i a0, a0_1, a1, a1_1, a32, a16;
   __m256i a_mbase_x, diff, max_base_x256, base_inc256, mask256;
 
   a16 = _mm256_set1_epi32(16);
   a_mbase_x = _mm256_set1_epi16(above[max_base_x]);
   max_base_x256 = _mm256_set1_epi16(max_base_x);
 
   x = dx;
   for (int r = 0; r < N; r++, dst += stride) {
     __m256i b, res[2], res1;
 
     int base = x >> frac_bits;
     if (base >= max_base_x) {
       for (int i = r; i < N; ++i) {
         _mm256_storeu_si256((__m256i *)dst, a_mbase_x);  // save 32 values
         _mm256_storeu_si256((__m256i *)(dst + 16), a_mbase_x);
         _mm256_storeu_si256((__m256i *)(dst + 32), a_mbase_x);
         _mm256_storeu_si256((__m256i *)(dst + 48), a_mbase_x);
         dst += stride;
       }
       return;
     }
 
     __m256i shift = _mm256_srli_epi32(
         _mm256_and_si256(_mm256_set1_epi32(x), _mm256_set1_epi32(0x3f)), 1);
 
     __m128i a0_128, a0_1_128, a1_128, a1_1_128;
     for (int j = 0; j < 64; j += 16) {
       int mdif = max_base_x - (base + j);
-      if (mdif == 0) {
+      if (mdif <= 0) {
         _mm256_storeu_si256((__m256i *)(dst + j), a_mbase_x);
       } else {
         a0_128 = _mm_loadu_si128((__m128i *)(above + base + j));
         a1_128 = _mm_loadu_si128((__m128i *)(above + base + 1 + j));
         a0 = _mm256_cvtepu16_epi32(a0_128);
         a1 = _mm256_cvtepu16_epi32(a1_128);
 
         diff = _mm256_sub_epi32(a1, a0);   // a[x+1] - a[x]
         a32 = _mm256_slli_epi32(a0, 5);    // a[x] * 32
         a32 = _mm256_add_epi32(a32, a16);  // a[x] * 32 + 16
         b = _mm256_mullo_epi32(diff, shift);
 
         res[0] = _mm256_add_epi32(a32, b);
         res[0] = _mm256_srli_epi32(res[0], 5);
         res[0] = _mm256_packus_epi32(
             res[0],
             _mm256_castsi128_si256(_mm256_extracti128_si256(res[0], 1)));
         if (mdif > 8) {
           a0_1_128 = _mm_loadu_si128((__m128i *)(above + base + 8 + j));
           a1_1_128 = _mm_loadu_si128((__m128i *)(above + base + 9 + j));
           a0_1 = _mm256_cvtepu16_epi32(a0_1_128);
           a1_1 = _mm256_cvtepu16_epi32(a1_1_128);
 
           diff = _mm256_sub_epi32(a1_1, a0_1);  // a[x+1] - a[x]
           a32 = _mm256_slli_epi32(a0_1, 5);     // a[x] * 32
           a32 = _mm256_add_epi32(a32, a16);     // a[x] * 32 + 16
           b = _mm256_mullo_epi32(diff, shift);
 
           res[1] = _mm256_add_epi32(a32, b);
           res[1] = _mm256_srli_epi32(res[1], 5);
           res[1] = _mm256_packus_epi32(
               res[1],
               _mm256_castsi128_si256(_mm256_extracti128_si256(res[1], 1)));
         } else {
           res[1] = a_mbase_x;
         }
         res1 = _mm256_inserti128_si256(res[0], _mm256_castsi256_si128(res[1]),
                                        1);  // 16 16bit values
         base_inc256 = _mm256_setr_epi16(
             base + j, base + j + 1, base + j + 2, base + j + 3, base + j + 4,
             base + j + 5, base + j + 6, base + j + 7, base + j + 8,
             base + j + 9, base + j + 10, base + j + 11, base + j + 12,
             base + j + 13, base + j + 14, base + j + 15);
 
         mask256 = _mm256_cmpgt_epi16(max_base_x256, base_inc256);
         res1 = _mm256_blendv_epi8(a_mbase_x, res1, mask256);
         _mm256_storeu_si256((__m256i *)(dst + j), res1);
       }
     }
     x += dx;
   }
 }
 
 // Directional prediction, zone 1: 0 < angle < 90
