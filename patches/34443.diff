commit e0e71bc963bf6fb8401d4e194bff6085c6dcf89b
Author: Lalit Maganti <lalitm@google.com>
Date:   Tue May 18 16:30:40 2021 +0100

    tp: workaround traces with massively out-of-order events
    
    This CL works around extremely slow loading of write_into_file
    traces which have events very much out of order (i.e. not
    respecting flush_period_ms).
    
    This allows these traces to be opened though much of the data inside
    these traces will be dropped as being out of order.
    
    Change-Id: I642bece275426b8b6aeec7ad89cde95717258fa0
    Bug: 188392852

diff --git a/src/trace_processor/trace_sorter.cc b/src/trace_processor/trace_sorter.cc
index 677d31ae3..95deb804c 100644
--- a/src/trace_processor/trace_sorter.cc
+++ b/src/trace_processor/trace_sorter.cc
@@ -59,138 +59,141 @@ void TraceSorter::Queue::Sort() {
 // Removes all the events in |queues_| that are earlier than the given window
 // size and moves them to the next parser stages, respecting global timestamp
 // order. This function is a "extract min from N sorted queues", with some
 // little cleverness: we know that events tend to be bursty, so events are
 // not going to be randomly distributed on the N |queues_|.
 // Upon each iteration this function finds the first two queues (if any) that
 // have the oldest events, and extracts events from the 1st until hitting the
 // min_ts of the 2nd. Imagine the queues are as follows:
 //
 //  q0           {min_ts: 10  max_ts: 30}
 //  q1    {min_ts:5              max_ts: 35}
 //  q2              {min_ts: 12    max_ts: 40}
 //
 // We know that we can extract all events from q1 until we hit ts=10 without
 // looking at any other queue. After hitting ts=10, we need to re-look to all of
 // them to figure out the next min-event.
 // There are more suitable data structures to do this (e.g. keeping a min-heap
 // to avoid re-scanning all the queues all the times) but doesn't seem worth it.
 // With Android traces (that have 8 CPUs) this function accounts for ~1-3% cpu
 // time in a profiler.
 void TraceSorter::SortAndExtractEventsBeyondWindow(int64_t window_size_ns) {
   DCHECK_ftrace_batch_cpu(kNoBatch);
 
   constexpr int64_t kTsMax = std::numeric_limits<int64_t>::max();
   const bool was_empty = global_min_ts_ == kTsMax && global_max_ts_ == 0;
   int64_t extract_end_ts = global_max_ts_ - window_size_ns;
   size_t iterations = 0;
   for (;; iterations++) {
     size_t min_queue_idx = 0;  // The index of the queue with the min(ts).
 
     // The top-2 min(ts) among all queues.
     // queues_[min_queue_idx].events.timestamp == min_queue_ts[0].
     int64_t min_queue_ts[2]{kTsMax, kTsMax};
 
     // This loop identifies the queue which starts with the earliest event and
     // also remembers the earliest event of the 2nd queue (in min_queue_ts[1]).
     bool has_queues_with_expired_events = false;
     for (size_t i = 0; i < queues_.size(); i++) {
       auto& queue = queues_[i];
       if (queue.events_.empty())
         continue;
       PERFETTO_DCHECK(queue.min_ts_ >= global_min_ts_);
       PERFETTO_DCHECK(queue.max_ts_ <= global_max_ts_);
       if (queue.min_ts_ < min_queue_ts[0]) {
         min_queue_ts[1] = min_queue_ts[0];
         min_queue_ts[0] = queue.min_ts_;
         min_queue_idx = i;
         has_queues_with_expired_events = true;
       } else if (queue.min_ts_ < min_queue_ts[1]) {
         min_queue_ts[1] = queue.min_ts_;
       }
     }
     if (!has_queues_with_expired_events) {
       // All the queues have events that start after the window (i.e. they are
       // too recent and not eligible to be extracted given the current window).
       break;
     }
 
     Queue& queue = queues_[min_queue_idx];
     auto& events = queue.events_;
     if (queue.needs_sorting())
       queue.Sort();
     PERFETTO_DCHECK(queue.min_ts_ == events.front().timestamp);
     PERFETTO_DCHECK(queue.min_ts_ == global_min_ts_);
 
     // Now that we identified the min-queue, extract all events from it until
     // we hit either: (1) the min-ts of the 2nd queue or (2) the window limit,
     // whichever comes first.
     int64_t extract_until_ts = std::min(extract_end_ts, min_queue_ts[1]);
     size_t num_extracted = 0;
     for (auto& event : events) {
-      int64_t timestamp = event.timestamp;
-      if (timestamp > extract_until_ts)
+      if (event.timestamp > extract_until_ts)
         break;
 
       ++num_extracted;
-      if (bypass_next_stage_for_testing_)
-        continue;
-
-      if (min_queue_idx == 0) {
-        // queues_[0] is for non-ftrace packets.
-        parser_->ParseTracePacket(timestamp, std::move(event));
-      } else {
-        // Ftrace queues start at offset 1. So queues_[1] = cpu[0] and so on.
-        uint32_t cpu = static_cast<uint32_t>(min_queue_idx - 1);
-        parser_->ParseFtracePacket(cpu, timestamp, std::move(event));
-      }
+      MaybePushEvent(min_queue_idx, std::move(event));
     }  // for (event: events)
 
     if (!num_extracted) {
       // No events can be extracted from any of the queues. This means that
       // either we hit the window or all queues are empty.
       break;
     }
 
     // Now remove the entries from the event buffer and update the queue-local
     // and global time bounds.
     events.erase_front(num_extracted);
 
     // Update the global_{min,max}_ts to reflect the bounds after extraction.
     if (events.empty()) {
       queue.min_ts_ = kTsMax;
       queue.max_ts_ = 0;
       global_min_ts_ = min_queue_ts[1];
 
       // If we extraced the max entry from a queue (i.e. we emptied the queue)
       // we need to recompute the global max, because it might have been the one
       // just extracted.
       global_max_ts_ = 0;
       for (auto& q : queues_)
         global_max_ts_ = std::max(global_max_ts_, q.max_ts_);
     } else {
       queue.min_ts_ = queue.events_.front().timestamp;
       global_min_ts_ = std::min(queue.min_ts_, min_queue_ts[1]);
     }
   }  // for(;;)
 
   // We decide to extract events only when we know (using the global_{min,max}
   // bounds) that there are eligible events. We should never end up in a
   // situation where we call this function but then realize that there was
   // nothing to extract.
   PERFETTO_DCHECK(iterations > 0 || was_empty);
 
 #if PERFETTO_DCHECK_IS_ON()
   // Check that the global min/max are consistent.
   int64_t dbg_min_ts = kTsMax;
   int64_t dbg_max_ts = 0;
   for (auto& q : queues_) {
     dbg_min_ts = std::min(dbg_min_ts, q.min_ts_);
     dbg_max_ts = std::max(dbg_max_ts, q.max_ts_);
   }
   PERFETTO_DCHECK(global_min_ts_ == dbg_min_ts);
   PERFETTO_DCHECK(global_max_ts_ == dbg_max_ts);
 #endif
 }
 
+void TraceSorter::MaybePushEvent(size_t queue_idx, TimestampedTracePiece ttp) {
+  if (bypass_next_stage_for_testing_)
+    return;
+
+  if (queue_idx == 0) {
+    // queues_[0] is for non-ftrace packets.
+    parser_->ParseTracePacket(ttp.timestamp, std::move(ttp));
+  } else {
+    // Ftrace queues start at offset 1. So queues_[1] = cpu[0] and so on.
+    uint32_t cpu = static_cast<uint32_t>(queue_idx - 1);
+    parser_->ParseFtracePacket(cpu, ttp.timestamp, std::move(ttp));
+  }
+}
+
 }  // namespace trace_processor
 }  // namespace perfetto
diff --git a/src/trace_processor/trace_sorter.h b/src/trace_processor/trace_sorter.h
index f65416336..4be615539 100644
--- a/src/trace_processor/trace_sorter.h
+++ b/src/trace_processor/trace_sorter.h
@@ -39,250 +39,267 @@ struct SystraceLine;
 // This class takes care of sorting events parsed from the trace stream in
 // arbitrary order and pushing them to the next pipeline stages (parsing) in
 // order. In order to support streaming use-cases, sorting happens within a
 // max window. Events are held in the TraceSorter staging area (events_) until
 // either (1) the (max - min) timestamp > window_size; (2) trace EOF.
 //
 // This class is designed around the assumption that:
 // - Most events come from ftrace.
 // - Ftrace events are sorted within each cpu most of the times.
 //
 // Due to this, this class is oprerates as a streaming merge-sort of N+1 queues
 // (N = num cpus + 1 for non-ftrace events). Each queue in turn gets sorted (if
 // necessary) before proceeding with the global merge-sort-extract.
 // When an event is pushed through, it is just appeneded to the end of one of
 // the N queues. While appending, we keep track of the fact that the queue
 // is still ordered or just lost ordering. When an out-of-order event is
 // detected on a queue we keep track of: (1) the offset within the queue where
 // the chaos begun, (2) the timestamp that broke the ordering.
 // When we decide to extract events from the queues into the next stages of
 // the trace processor, we re-sort the events in the queue. Rather than
 // re-sorting everything all the times, we use the above knowledge to restrict
 // sorting to the (hopefully smaller) tail of the |events_| staging area.
 // At any time, the first partition of |events_| [0 .. sort_start_idx_) is
 // ordered, and the second partition [sort_start_idx_.. end] is not.
 // We use a logarithmic bound search operation to figure out what is the index
 // within the first partition where sorting should start, and sort all events
 // from there to the end.
 class TraceSorter {
  public:
   TraceSorter(std::unique_ptr<TraceParser> parser, int64_t window_size_ns);
 
   inline void PushTracePacket(int64_t timestamp,
                               PacketSequenceState* state,
                               TraceBlobView packet) {
     DCHECK_ftrace_batch_cpu(kNoBatch);
-    auto* queue = GetQueue(0);
-    queue->Append(TimestampedTracePiece(timestamp, packet_idx_++,
-                                        std::move(packet),
-                                        state->current_generation()));
-    MaybeExtractEvents(queue);
+    AppendNonFtraceAndMaybeExtractEvents(
+        TimestampedTracePiece(timestamp, packet_idx_++, std::move(packet),
+                              state->current_generation()));
   }
 
   inline void PushJsonValue(int64_t timestamp, std::string json_value) {
-    auto* queue = GetQueue(0);
-    queue->Append(
+    DCHECK_ftrace_batch_cpu(kNoBatch);
+    AppendNonFtraceAndMaybeExtractEvents(
         TimestampedTracePiece(timestamp, packet_idx_++, std::move(json_value)));
-    MaybeExtractEvents(queue);
   }
 
   inline void PushFuchsiaRecord(int64_t timestamp,
                                 std::unique_ptr<FuchsiaRecord> record) {
     DCHECK_ftrace_batch_cpu(kNoBatch);
-    auto* queue = GetQueue(0);
-    queue->Append(
+    AppendNonFtraceAndMaybeExtractEvents(
         TimestampedTracePiece(timestamp, packet_idx_++, std::move(record)));
-    MaybeExtractEvents(queue);
   }
 
   inline void PushSystraceLine(std::unique_ptr<SystraceLine> systrace_line) {
     DCHECK_ftrace_batch_cpu(kNoBatch);
-    auto* queue = GetQueue(0);
-    int64_t timestamp = systrace_line->ts;
-    queue->Append(TimestampedTracePiece(timestamp, packet_idx_++,
-                                        std::move(systrace_line)));
-    MaybeExtractEvents(queue);
+    AppendNonFtraceAndMaybeExtractEvents(TimestampedTracePiece(
+        systrace_line->ts, packet_idx_++, std::move(systrace_line)));
+  }
+
+  inline void PushTrackEventPacket(int64_t timestamp,
+                                   std::unique_ptr<TrackEventData> data) {
+    AppendNonFtraceAndMaybeExtractEvents(
+        TimestampedTracePiece(timestamp, packet_idx_++, std::move(data)));
   }
 
   inline void PushFtraceEvent(uint32_t cpu,
                               int64_t timestamp,
                               TraceBlobView event,
                               PacketSequenceState* state) {
     set_ftrace_batch_cpu_for_DCHECK(cpu);
     GetQueue(cpu + 1)->Append(TimestampedTracePiece(
         timestamp, packet_idx_++,
         FtraceEventData{std::move(event), state->current_generation()}));
 
     // The caller must call FinalizeFtraceEventBatch() after having pushed a
     // batch of ftrace events. This is to amortize the overhead of handling
     // global ordering and doing that in batches only after all ftrace events
     // for a bundle are pushed.
   }
 
   // As with |PushFtraceEvent|, doesn't immediately sort the affected queues.
   // TODO(rsavitski): if a trace has a mix of normal & "compact" events (being
   // pushed through this function), the ftrace batches will no longer be fully
   // sorted by timestamp. In such situations, we will have to sort at the end of
   // the batch. We can do better as both sub-sequences are sorted however.
   // Consider adding extra queues, or pushing them in a merge-sort fashion
   // instead.
   inline void PushInlineFtraceEvent(uint32_t cpu,
                                     int64_t timestamp,
                                     InlineSchedSwitch inline_sched_switch) {
     set_ftrace_batch_cpu_for_DCHECK(cpu);
     GetQueue(cpu + 1)->Append(
         TimestampedTracePiece(timestamp, packet_idx_++, inline_sched_switch));
   }
   inline void PushInlineFtraceEvent(uint32_t cpu,
                                     int64_t timestamp,
                                     InlineSchedWaking inline_sched_waking) {
     set_ftrace_batch_cpu_for_DCHECK(cpu);
     GetQueue(cpu + 1)->Append(
         TimestampedTracePiece(timestamp, packet_idx_++, inline_sched_waking));
   }
-
-  inline void PushTrackEventPacket(int64_t timestamp,
-                                   std::unique_ptr<TrackEventData> data) {
-    auto* queue = GetQueue(0);
-    queue->Append(
-        TimestampedTracePiece(timestamp, packet_idx_++, std::move(data)));
-    MaybeExtractEvents(queue);
-  }
-
   inline void FinalizeFtraceEventBatch(uint32_t cpu) {
     DCHECK_ftrace_batch_cpu(cpu);
     set_ftrace_batch_cpu_for_DCHECK(kNoBatch);
     MaybeExtractEvents(GetQueue(cpu + 1));
   }
 
   // Extract all events ignoring the window.
   void ExtractEventsForced() {
     SortAndExtractEventsBeyondWindow(/*window_size_ns=*/0);
     queues_.resize(0);
   }
 
   // Sets the window size to be the size specified (which should be lower than
   // any previous window size specified) and flushes any data beyond
   // this window size.
   // It is undefined to call this function with a window size greater than than
   // the current size.
   void SetWindowSizeNs(int64_t window_size_ns) {
     PERFETTO_DCHECK(window_size_ns <= window_size_ns_);
 
     PERFETTO_DLOG("Setting window size to be %" PRId64 " ns", window_size_ns);
     window_size_ns_ = window_size_ns;
 
     // Fast path: if, globally, we are within the window size, then just exit.
     if (global_max_ts_ - global_min_ts_ < window_size_ns)
       return;
     SortAndExtractEventsBeyondWindow(window_size_ns_);
   }
 
   int64_t max_timestamp() const { return global_max_ts_; }
 
  private:
   static constexpr uint32_t kNoBatch = std::numeric_limits<uint32_t>::max();
 
   struct Queue {
     inline void Append(TimestampedTracePiece ttp) {
       const int64_t timestamp = ttp.timestamp;
       events_.emplace_back(std::move(ttp));
       min_ts_ = std::min(min_ts_, timestamp);
 
       // Events are often seen in order.
       if (PERFETTO_LIKELY(timestamp >= max_ts_)) {
         max_ts_ = timestamp;
       } else {
         // The event is breaking ordering. The first time it happens, keep
         // track of which index we are at. We know that everything before that
         // is sorted (because events were pushed monotonically). Everything
         // after that index, instead, will need a sorting pass before moving
         // events to the next pipeline stage.
         if (sort_start_idx_ == 0) {
           PERFETTO_DCHECK(events_.size() >= 2);
           sort_start_idx_ = events_.size() - 1;
           sort_min_ts_ = timestamp;
         } else {
           sort_min_ts_ = std::min(sort_min_ts_, timestamp);
         }
       }
 
       PERFETTO_DCHECK(min_ts_ <= max_ts_);
     }
 
     bool needs_sorting() const { return sort_start_idx_ != 0; }
     void Sort();
 
     base::CircularQueue<TimestampedTracePiece> events_;
     int64_t min_ts_ = std::numeric_limits<int64_t>::max();
     int64_t max_ts_ = 0;
     size_t sort_start_idx_ = 0;
     int64_t sort_min_ts_ = std::numeric_limits<int64_t>::max();
   };
 
   // This method passes any events older than window_size_ns to the
   // parser to be parsed and then stored.
   void SortAndExtractEventsBeyondWindow(int64_t windows_size_ns);
 
   inline Queue* GetQueue(size_t index) {
     if (PERFETTO_UNLIKELY(index >= queues_.size()))
       queues_.resize(index + 1);
     return &queues_[index];
   }
 
+  inline void AppendNonFtraceAndMaybeExtractEvents(TimestampedTracePiece ttp) {
+    // Fast path: if this event is before all other events in the sorter and
+    // happened more than the window size in the past, just push the event to
+    // the next stage. This saves all the sorting logic which would simply move
+    // this event to the head of the queue and then extract it out.
+    //
+    // In practice, these events will be rejected as being "out-of-order" later
+    // on in trace processor (i.e. in EventTracker or SliceTracker); we don't
+    // drop here to allow them to track packet drop stats.
+    //
+    // See b/188392852 for an example of where this condition would be hit in
+    // practice.
+    bool is_before_all_events = ttp.timestamp < global_max_ts_;
+    bool is_before_window = global_max_ts_ - ttp.timestamp >= window_size_ns_;
+    if (is_before_all_events && is_before_window) {
+      MaybePushEvent(0, std::move(ttp));
+      return;
+    }
+
+    // Slow path: append the event to the non-ftrace queue and extract any
+    // events if available.
+    Queue* queue = GetQueue(0);
+    queue->Append(std::move(ttp));
+    MaybeExtractEvents(queue);
+  }
+
   inline void MaybeExtractEvents(Queue* queue) {
     DCHECK_ftrace_batch_cpu(kNoBatch);
     global_max_ts_ = std::max(global_max_ts_, queue->max_ts_);
     global_min_ts_ = std::min(global_min_ts_, queue->min_ts_);
 
     // Fast path: if, globally, we are within the window size, then just exit.
     if (global_max_ts_ - global_min_ts_ < window_size_ns_)
       return;
     SortAndExtractEventsBeyondWindow(window_size_ns_);
   }
 
+  void MaybePushEvent(size_t queue_idx, TimestampedTracePiece ttp);
+
   std::unique_ptr<TraceParser> parser_;
 
   // queues_[0] is the general (non-ftrace) queue.
   // queues_[1] is the ftrace queue for CPU(0).
   // queues_[x] is the ftrace queue for CPU(x - 1).
   std::vector<Queue> queues_;
 
   // Events are propagated to the next stage only after (max - min) timestamp
   // is larger than this value.
   int64_t window_size_ns_;
 
   // max(e.timestamp for e in queues_).
   int64_t global_max_ts_ = 0;
 
   // min(e.timestamp for e in queues_).
   int64_t global_min_ts_ = std::numeric_limits<int64_t>::max();
 
   // Monotonic increasing value used to index timestamped trace pieces.
   uint64_t packet_idx_ = 0;
 
   // Used for performance tests. True when setting TRACE_PROCESSOR_SORT_ONLY=1.
   bool bypass_next_stage_for_testing_ = false;
 
 #if PERFETTO_DCHECK_IS_ON()
   // Used only for DCHECK-ing that FinalizeFtraceEventBatch() is called.
   uint32_t ftrace_batch_cpu_ = kNoBatch;
 
   inline void DCHECK_ftrace_batch_cpu(uint32_t cpu) {
     PERFETTO_DCHECK(ftrace_batch_cpu_ == kNoBatch || ftrace_batch_cpu_ == cpu);
   }
 
   inline void set_ftrace_batch_cpu_for_DCHECK(uint32_t cpu) {
     PERFETTO_DCHECK(ftrace_batch_cpu_ == cpu || ftrace_batch_cpu_ == kNoBatch ||
                     cpu == kNoBatch);
     ftrace_batch_cpu_ = cpu;
   }
 #else
   inline void DCHECK_ftrace_batch_cpu(uint32_t) {}
   inline void set_ftrace_batch_cpu_for_DCHECK(uint32_t) {}
 #endif
 };
 
 }  // namespace trace_processor
 }  // namespace perfetto
 
 #endif  // SRC_TRACE_PROCESSOR_TRACE_SORTER_H_
