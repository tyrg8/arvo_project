commit 4f4dc0a1a29d3689ba8e73a08c13d4f2e152aad1
Author: Zhao Zhili <quinkblack@foxmail.com>
Date:   Sat Sep 2 16:23:59 2023 +0800

    avfilter/dnn_backend_openvino: fix wild pointer on error path
    
    When ov_model_const_input_by_name/ov_model_const_output_by_name
    failed, input_port/output_port can be wild pointer.
    
    Signed-off-by: Zhao Zhili <zhilizhao@tencent.com>

diff --git a/libavfilter/dnn/dnn_backend_openvino.c b/libavfilter/dnn/dnn_backend_openvino.c
index 5de27719b2..ded156289b 100644
--- a/libavfilter/dnn/dnn_backend_openvino.c
+++ b/libavfilter/dnn/dnn_backend_openvino.c
@@ -186,142 +186,145 @@ static int get_datatype_size(DNNDataType dt)
 static int fill_model_input_ov(OVModel *ov_model, OVRequestItem *request)
 {
     DNNData input;
     LastLevelTaskItem *lltask;
     TaskItem *task;
     OVContext *ctx = &ov_model->ctx;
 #if HAVE_OPENVINO2
     int64_t* dims;
     ov_status_e status;
     ov_tensor_t* tensor = NULL;
     ov_shape_t input_shape = {0};
     ov_element_type_e precision;
     void *input_data_ptr = NULL;
 #else
     dimensions_t dims;
     precision_e precision;
     ie_blob_buffer_t blob_buffer;
     IEStatusCode status;
     ie_blob_t *input_blob = NULL;
 #endif
 
     lltask = ff_queue_peek_front(ov_model->lltask_queue);
     av_assert0(lltask);
     task = lltask->task;
 
 #if HAVE_OPENVINO2
     if (!ov_model_is_dynamic(ov_model->ov_model)) {
-        ov_output_const_port_free(ov_model->input_port);
+        if (ov_model->input_port) {
+            ov_output_const_port_free(ov_model->input_port);
+            ov_model->input_port = NULL;
+        }
         status = ov_model_const_input_by_name(ov_model->ov_model, task->input_name, &ov_model->input_port);
         if (status != OK) {
             av_log(ctx, AV_LOG_ERROR, "Failed to get input port shape.\n");
             return ov2_map_error(status, NULL);
         }
         status = ov_const_port_get_shape(ov_model->input_port, &input_shape);
         if (status != OK) {
             av_log(ctx, AV_LOG_ERROR, "Failed to get input port shape.\n");
             return ov2_map_error(status, NULL);
         }
         dims = input_shape.dims;
         status = ov_port_get_element_type(ov_model->input_port, &precision);
         if (status != OK) {
             av_log(ctx, AV_LOG_ERROR, "Failed to get input port data type.\n");
             ov_shape_free(&input_shape);
             return ov2_map_error(status, NULL);
         }
     } else {
         avpriv_report_missing_feature(ctx, "Do not support dynamic model.");
         return AVERROR(ENOSYS);
     }
     input.height = dims[2];
     input.width = dims[3];
     input.channels = dims[1];
     input.dt = precision_to_datatype(precision);
     input.data = av_malloc(input.height * input.width * input.channels * get_datatype_size(input.dt));
     if (!input.data) {
         ov_shape_free(&input_shape);
         return AVERROR(ENOMEM);
     }
     input_data_ptr = input.data;
 #else
     status = ie_infer_request_get_blob(request->infer_request, task->input_name, &input_blob);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to get input blob with name %s\n", task->input_name);
         return DNN_GENERIC_ERROR;
     }
 
     status |= ie_blob_get_dims(input_blob, &dims);
     status |= ie_blob_get_precision(input_blob, &precision);
     if (status != OK) {
         ie_blob_free(&input_blob);
         av_log(ctx, AV_LOG_ERROR, "Failed to get input blob dims/precision\n");
         return DNN_GENERIC_ERROR;
     }
 
     status = ie_blob_get_buffer(input_blob, &blob_buffer);
     if (status != OK) {
         ie_blob_free(&input_blob);
         av_log(ctx, AV_LOG_ERROR, "Failed to get input blob buffer\n");
         return DNN_GENERIC_ERROR;
     }
     input.height = dims.dims[2];
     input.width = dims.dims[3];
     input.channels = dims.dims[1];
     input.data = blob_buffer.buffer;
     input.dt = precision_to_datatype(precision);
 #endif
     // all models in openvino open model zoo use BGR as input,
     // change to be an option when necessary.
     input.order = DCO_BGR;
 
     for (int i = 0; i < ctx->options.batch_size; ++i) {
         lltask = ff_queue_pop_front(ov_model->lltask_queue);
         if (!lltask) {
             break;
         }
         request->lltasks[i] = lltask;
         request->lltask_count = i + 1;
         task = lltask->task;
         switch (ov_model->model->func_type) {
         case DFT_PROCESS_FRAME:
             if (task->do_ioproc) {
                 if (ov_model->model->frame_pre_proc != NULL) {
                     ov_model->model->frame_pre_proc(task->in_frame, &input, ov_model->model->filter_ctx);
                 } else {
                     ff_proc_from_frame_to_dnn(task->in_frame, &input, ctx);
                 }
             }
             break;
         case DFT_ANALYTICS_DETECT:
             ff_frame_to_dnn_detect(task->in_frame, &input, ctx);
             break;
         case DFT_ANALYTICS_CLASSIFY:
             ff_frame_to_dnn_classify(task->in_frame, &input, lltask->bbox_index, ctx);
             break;
         default:
             av_assert0(!"should not reach here");
             break;
         }
 #if HAVE_OPENVINO2
         status = ov_tensor_create_from_host_ptr(precision, input_shape, input.data, &tensor);
         ov_shape_free(&input_shape);
         if (status != OK) {
             av_log(ctx, AV_LOG_ERROR, "Failed to create tensor from host prt.\n");
             return ov2_map_error(status, NULL);
         }
         status = ov_infer_request_set_input_tensor(request->infer_request, tensor);
         if (status != OK) {
             av_log(ctx, AV_LOG_ERROR, "Failed to Set an input tensor for the model.\n");
             return ov2_map_error(status, NULL);
         }
 #endif
         input.data = (uint8_t *)input.data
                      + input.width * input.height * input.channels * get_datatype_size(input.dt);
     }
 #if HAVE_OPENVINO2
     av_freep(&input_data_ptr);
 #else
     ie_blob_free(&input_blob);
 #endif
 
     return 0;
 }
@@ -534,245 +537,247 @@ static void dnn_free_model_ov(DNNModel **model)
 static int init_model_ov(OVModel *ov_model, const char *input_name, const char *output_name)
 {
     int ret = 0;
     OVContext *ctx = &ov_model->ctx;
 #if HAVE_OPENVINO2
     ov_status_e status;
     ov_preprocess_input_tensor_info_t* input_tensor_info;
     ov_preprocess_output_tensor_info_t* output_tensor_info;
     ov_model_t *tmp_ov_model;
     ov_layout_t* NHWC_layout = NULL;
     const char* NHWC_desc = "NHWC";
     const char* device = ctx->options.device_type;
 #else
     IEStatusCode status;
     ie_available_devices_t a_dev;
     ie_config_t config = {NULL, NULL, NULL};
     char *all_dev_names = NULL;
 #endif
 
     // batch size
     if (ctx->options.batch_size <= 0) {
         ctx->options.batch_size = 1;
     }
 #if HAVE_OPENVINO2
     if (ctx->options.batch_size > 1) {
         avpriv_report_missing_feature(ctx, "Do not support batch_size > 1 for now,"
                                            "change batch_size to 1.\n");
         ctx->options.batch_size = 1;
     }
 
     status = ov_preprocess_prepostprocessor_create(ov_model->ov_model, &ov_model->preprocess);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to create preprocess for ov_model.\n");
         ret = ov2_map_error(status, NULL);
         goto err;
     }
 
     status = ov_preprocess_prepostprocessor_get_input_info_by_name(ov_model->preprocess, input_name, &ov_model->input_info);
     status |= ov_preprocess_prepostprocessor_get_output_info_by_name(ov_model->preprocess, output_name, &ov_model->output_info);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to get input/output info from preprocess.\n");
         ret = ov2_map_error(status, NULL);
         goto err;
     }
 
     status = ov_preprocess_input_info_get_tensor_info(ov_model->input_info, &input_tensor_info);
     status |= ov_preprocess_output_info_get_tensor_info(ov_model->output_info, &output_tensor_info);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to get tensor info from input/output.\n");
         ret = ov2_map_error(status, NULL);
         goto err;
     }
 
     //set input layout
     status = ov_layout_create(NHWC_desc, &NHWC_layout);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to create layout for input.\n");
         ret = ov2_map_error(status, NULL);
         goto err;
     }
 
     status = ov_preprocess_input_tensor_info_set_layout(input_tensor_info, NHWC_layout);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to set input tensor layout\n");
         ret = ov2_map_error(status, NULL);
         goto err;
     }
 
     if (ov_model->model->func_type != DFT_PROCESS_FRAME)
         //set precision only for detect and classify
         status = ov_preprocess_input_tensor_info_set_element_type(input_tensor_info, U8);
     status |= ov_preprocess_output_set_element_type(output_tensor_info, F32);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to set input/output element type\n");
         ret = ov2_map_error(status, NULL);
         goto err;
     }
 
     //update model
     if(ov_model->ov_model)
         tmp_ov_model = ov_model->ov_model;
     status = ov_preprocess_prepostprocessor_build(ov_model->preprocess, &ov_model->ov_model);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to update OV model\n");
         ret = ov2_map_error(status, NULL);
         goto err;
     }
     ov_model_free(tmp_ov_model);
 
     //update output_port
-    if (ov_model->output_port)
+    if (ov_model->output_port) {
         ov_output_const_port_free(ov_model->output_port);
+        ov_model->output_port = NULL;
+    }
     status = ov_model_const_output_by_name(ov_model->ov_model, output_name, &ov_model->output_port);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to get output port.\n");
         goto err;
     }
     //compile network
     status = ov_core_compile_model(ov_model->core, ov_model->ov_model, device, 0, &ov_model->compiled_model);
     if (status != OK) {
         ret = ov2_map_error(status, NULL);
         goto err;
     }
 #else
     if (ctx->options.batch_size > 1) {
         input_shapes_t input_shapes;
         status = ie_network_get_input_shapes(ov_model->network, &input_shapes);
         if (status != OK) {
             ret = DNN_GENERIC_ERROR;
             goto err;
         }
         for (int i = 0; i < input_shapes.shape_num; i++)
             input_shapes.shapes[i].shape.dims[0] = ctx->options.batch_size;
         status = ie_network_reshape(ov_model->network, input_shapes);
         ie_network_input_shapes_free(&input_shapes);
         if (status != OK) {
             ret = DNN_GENERIC_ERROR;
             goto err;
         }
     }
 
     // The order of dims in the openvino is fixed and it is always NCHW for 4-D data.
     // while we pass NHWC data from FFmpeg to openvino
     status = ie_network_set_input_layout(ov_model->network, input_name, NHWC);
     if (status != OK) {
         if (status == NOT_FOUND) {
             av_log(ctx, AV_LOG_ERROR, "Could not find \"%s\" in model, failed to set input layout as NHWC, "\
                                       "all input(s) are: \"%s\"\n", input_name, ov_model->all_input_names);
         } else{
             av_log(ctx, AV_LOG_ERROR, "Failed to set layout as NHWC for input %s\n", input_name);
         }
         ret = DNN_GENERIC_ERROR;
         goto err;
     }
     status = ie_network_set_output_layout(ov_model->network, output_name, NHWC);
     if (status != OK) {
         if (status == NOT_FOUND) {
             av_log(ctx, AV_LOG_ERROR, "Could not find \"%s\" in model, failed to set output layout as NHWC, "\
                                       "all output(s) are: \"%s\"\n", output_name, ov_model->all_output_names);
         } else{
             av_log(ctx, AV_LOG_ERROR, "Failed to set layout as NHWC for output %s\n", output_name);
         }
         ret = DNN_GENERIC_ERROR;
         goto err;
     }
 
     // all models in openvino open model zoo use BGR with range [0.0f, 255.0f] as input,
     // we don't have a AVPixelFormat to describe it, so we'll use AV_PIX_FMT_BGR24 and
     // ask openvino to do the conversion internally.
     // the current supported SR model (frame processing) is generated from tensorflow model,
     // and its input is Y channel as float with range [0.0f, 1.0f], so do not set for this case.
     // TODO: we need to get a final clear&general solution with all backends/formats considered.
     if (ov_model->model->func_type != DFT_PROCESS_FRAME) {
         status = ie_network_set_input_precision(ov_model->network, input_name, U8);
         if (status != OK) {
             av_log(ctx, AV_LOG_ERROR, "Failed to set input precision as U8 for %s\n", input_name);
             ret = DNN_GENERIC_ERROR;
             goto err;
         }
     }
 
     status = ie_core_load_network(ov_model->core, ov_model->network, ctx->options.device_type, &config, &ov_model->exe_network);
     if (status != OK) {
         av_log(ctx, AV_LOG_ERROR, "Failed to load OpenVINO model network\n");
         status = ie_core_get_available_devices(ov_model->core, &a_dev);
         if (status != OK) {
             av_log(ctx, AV_LOG_ERROR, "Failed to get available devices\n");
             ret = DNN_GENERIC_ERROR;
             goto err;
         }
         for (int i = 0; i < a_dev.num_devices; i++) {
             APPEND_STRING(all_dev_names, a_dev.devices[i])
         }
         av_log(ctx, AV_LOG_ERROR,"device %s may not be supported, all available devices are: \"%s\"\n",
                ctx->options.device_type, all_dev_names);
         ret = AVERROR(ENODEV);
         goto err;
     }
 #endif
     // create infer_requests for async execution
     if (ctx->options.nireq <= 0) {
         // the default value is a rough estimation
         ctx->options.nireq = av_cpu_count() / 2 + 1;
     }
 
     ov_model->request_queue = ff_safe_queue_create();
     if (!ov_model->request_queue) {
         ret = AVERROR(ENOMEM);
         goto err;
     }
 
     for (int i = 0; i < ctx->options.nireq; i++) {
         OVRequestItem *item = av_mallocz(sizeof(*item));
         if (!item) {
             ret = AVERROR(ENOMEM);
             goto err;
         }
 
 #if HAVE_OPENVINO2
         item->callback.callback_func = infer_completion_callback;
 #else
         item->callback.completeCallBackFunc = infer_completion_callback;
 #endif
         item->callback.args = item;
         if (ff_safe_queue_push_back(ov_model->request_queue, item) < 0) {
             av_freep(&item);
             ret = AVERROR(ENOMEM);
             goto err;
         }
 
 #if HAVE_OPENVINO2
         status = ov_compiled_model_create_infer_request(ov_model->compiled_model, &item->infer_request);
         if (status != OK) {
             av_log(ctx, AV_LOG_ERROR, "Failed to Creates an inference request object.\n");
             goto err;
         }
 #else
         status = ie_exec_network_create_infer_request(ov_model->exe_network, &item->infer_request);
         if (status != OK) {
             ret = DNN_GENERIC_ERROR;
             goto err;
         }
 #endif
 
         item->lltasks = av_malloc_array(ctx->options.batch_size, sizeof(*item->lltasks));
         if (!item->lltasks) {
             ret = AVERROR(ENOMEM);
             goto err;
         }
         item->lltask_count = 0;
     }
 
     ov_model->task_queue = ff_queue_create();
     if (!ov_model->task_queue) {
         ret = AVERROR(ENOMEM);
         goto err;
     }
 
     ov_model->lltask_queue = ff_queue_create();
     if (!ov_model->lltask_queue) {
         ret = AVERROR(ENOMEM);
         goto err;
     }
 
     return 0;
