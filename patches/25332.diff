commit 5d3d48a1552e3e589ccc97d23b87ae94c26eb929
Author: Antoine Pitrou <antoine@python.org>
Date:   Thu Sep 3 13:31:28 2020 +0200

    ARROW-9852: [C++] Validate dictionaries fully when combining deltas
    
    We need O(N) validation of dictionaries when combining deltas, because array concatenation may involve arbitrary reads (for example, nested dictionaries are compared for equality when concatenating).
    
    For the common case where no deltas are encountered, full validation is not required.
    
    Also, concatenation and validation are now performed at dictionary resolution time, rather than when reading the dictionary batches.
    
    Found by OSS-Fuzz.
    
    Closes #8050 from pitrou/ARROW-9852-oss-fuzz-ipc
    
    Authored-by: Antoine Pitrou <antoine@python.org>
    Signed-off-by: Antoine Pitrou <antoine@python.org>

diff --git a/cpp/src/arrow/ipc/dictionary.cc b/cpp/src/arrow/ipc/dictionary.cc
index 65f5835fa..bf760b0ec 100644
--- a/cpp/src/arrow/ipc/dictionary.cc
+++ b/cpp/src/arrow/ipc/dictionary.cc
@@ -143,10 +143,43 @@ int DictionaryFieldMapper::num_fields() const { return impl_->num_fields(); }
 // DictionaryMemo implementation
 
 struct DictionaryMemo::Impl {
-  // Map of dictionary id to dictionary array
-  std::unordered_map<int64_t, std::shared_ptr<ArrayData>> id_to_dictionary_;
+  // Map of dictionary id to dictionary array(s) (several in case of deltas)
+  std::unordered_map<int64_t, ArrayDataVector> id_to_dictionary_;
   std::unordered_map<int64_t, std::shared_ptr<DataType>> id_to_type_;
   DictionaryFieldMapper mapper_;
+
+  Result<decltype(id_to_dictionary_)::iterator> FindDictionary(int64_t id) {
+    auto it = id_to_dictionary_.find(id);
+    if (it == id_to_dictionary_.end()) {
+      return Status::KeyError("Dictionary with id ", id, " not found");
+    }
+    return it;
+  }
+
+  Result<std::shared_ptr<ArrayData>> ReifyDictionary(int64_t id, MemoryPool* pool) {
+    ARROW_ASSIGN_OR_RAISE(auto it, FindDictionary(id));
+    ArrayDataVector* data_vector = &it->second;
+
+    DCHECK(!data_vector->empty());
+    if (data_vector->size() > 1) {
+      // There are deltas, we need to concatenate them to the first dictionary.
+      ArrayVector to_combine;
+      to_combine.reserve(data_vector->size());
+      // IMPORTANT: At this point, the dictionary data may be untrusted.
+      // We need to validate it, as concatenation can crash on invalid or
+      // corrupted data.  Full validation is necessary for certain types
+      // (for example nested dictionaries).
+      // XXX: this won't work if there are unresolved nested dictionaries.
+      for (const auto& data : *data_vector) {
+        to_combine.push_back(MakeArray(data));
+        RETURN_NOT_OK(to_combine.back()->ValidateFull());
+      }
+      ARROW_ASSIGN_OR_RAISE(auto combined_dict, Concatenate(to_combine, pool));
+      *data_vector = {combined_dict->data()};
+    }
+
+    return data_vector->back();
+  }
 };
 
 DictionaryMemo::DictionaryMemo() : impl_(new Impl()) {}
@@ -166,12 +199,9 @@ Result<std::shared_ptr<DataType>> DictionaryMemo::GetDictionaryType(int64_t id)
 }
 
 // Returns KeyError if dictionary not found
-Result<std::shared_ptr<ArrayData>> DictionaryMemo::GetDictionary(int64_t id) const {
-  const auto it = impl_->id_to_dictionary_.find(id);
-  if (it == impl_->id_to_dictionary_.end()) {
-    return Status::KeyError("Dictionary with id ", id, " not found");
-  }
-  return it->second;
+Result<std::shared_ptr<ArrayData>> DictionaryMemo::GetDictionary(int64_t id,
+                                                                 MemoryPool* pool) const {
+  return impl_->ReifyDictionary(id, pool);
 }
 
 Status DictionaryMemo::AddDictionaryType(int64_t id,
@@ -192,38 +222,25 @@ bool DictionaryMemo::HasDictionary(int64_t id) const {
 
 Status DictionaryMemo::AddDictionary(int64_t id,
                                      const std::shared_ptr<ArrayData>& dictionary) {
-  if (HasDictionary(id)) {
+  const auto pair = impl_->id_to_dictionary_.emplace(id, ArrayDataVector{dictionary});
+  if (!pair.second) {
     return Status::KeyError("Dictionary with id ", id, " already exists");
   }
-  impl_->id_to_dictionary_[id] = dictionary;
   return Status::OK();
 }
 
 Status DictionaryMemo::AddDictionaryDelta(int64_t id,
-                                          const std::shared_ptr<ArrayData>& dictionary,
-                                          MemoryPool* pool) {
-  ARROW_ASSIGN_OR_RAISE(auto original_dict, GetDictionary(id));
-
-  // XXX This won't work if there is an unresolved nested dictionary.
-  // We could expose a ArrayData concatenation API, but then we wouldn't be able
-  // to validate below.
-  ArrayVector to_combine{MakeArray(original_dict), MakeArray(dictionary)};
-
-  // Validate the dictionaries for safe concatenation
-  for (const auto& array : to_combine) {
-    RETURN_NOT_OK(array->Validate());
-  }
-
-  ARROW_ASSIGN_OR_RAISE(auto combined_dict, Concatenate(to_combine, pool));
-  impl_->id_to_dictionary_[id] = combined_dict->data();
+                                          const std::shared_ptr<ArrayData>& dictionary) {
+  ARROW_ASSIGN_OR_RAISE(auto it, impl_->FindDictionary(id));
+  it->second.push_back(dictionary);
   return Status::OK();
 }
 
 Status DictionaryMemo::AddOrReplaceDictionary(
     int64_t id, const std::shared_ptr<ArrayData>& dictionary) {
-  impl_->id_to_dictionary_[id] = dictionary;
+  impl_->id_to_dictionary_[id] = {dictionary};
   return Status::OK();
 }
 
 // ----------------------------------------------------------------------
 // CollectDictionaries implementation
@@ -282,34 +299,36 @@ struct DictionaryCollector {
 };
 
 struct DictionaryResolver {
-  const DictionaryMemo& memo;
+  const DictionaryMemo& memo_;
+  MemoryPool* pool_;
 
   Status VisitChildren(const ArrayDataVector& data_vector, FieldPosition parent_pos) {
     int i = 0;
     for (const auto& data : data_vector) {
       // Some data entries may be missing if reading only a subset of the schema
       if (data != nullptr) {
         RETURN_NOT_OK(VisitField(parent_pos.child(i), data.get()));
       }
       ++i;
     }
     return Status::OK();
   }
 
   Status VisitField(FieldPosition field_pos, ArrayData* data) {
     const DataType* type = data->type.get();
     if (type->id() == Type::EXTENSION) {
       type = checked_cast<const ExtensionType&>(*type).storage_type().get();
     }
     if (type->id() == Type::DICTIONARY) {
-      ARROW_ASSIGN_OR_RAISE(const int64_t id, memo.fields().GetFieldId(field_pos.path()));
-      ARROW_ASSIGN_OR_RAISE(data->dictionary, memo.GetDictionary(id));
+      ARROW_ASSIGN_OR_RAISE(const int64_t id,
+                            memo_.fields().GetFieldId(field_pos.path()));
+      ARROW_ASSIGN_OR_RAISE(data->dictionary, memo_.GetDictionary(id, pool_));
       // Resolve nested dictionary data
       RETURN_NOT_OK(VisitField(field_pos, data->dictionary.get()));
     }
     // Resolve child data
     return VisitChildren(data->child_data, field_pos);
   }
 };
 
 }  // namespace
@@ -335,10 +354,11 @@ Status CollectDictionaries(const RecordBatch& batch, DictionaryMemo* memo) {
 
 }  // namespace internal
 
-Status ResolveDictionaries(const ArrayDataVector& columns, const DictionaryMemo& memo) {
-  DictionaryResolver resolver{memo};
+Status ResolveDictionaries(const ArrayDataVector& columns, const DictionaryMemo& memo,
+                           MemoryPool* pool) {
+  DictionaryResolver resolver{memo, pool};
   return resolver.VisitChildren(columns, FieldPosition());
 }
 
 }  // namespace ipc
 }  // namespace arrow
diff --git a/cpp/src/arrow/ipc/dictionary.h b/cpp/src/arrow/ipc/dictionary.h
index 35af709db..3f3bd838c 100644
--- a/cpp/src/arrow/ipc/dictionary.h
+++ b/cpp/src/arrow/ipc/dictionary.h
@@ -90,56 +90,55 @@ using DictionaryVector = std::vector<std::pair<int64_t, std::shared_ptr<Array>>>
 /// \brief Memoization data structure for reading dictionaries from IPC streams
 ///
 /// This structure tracks the following associations:
 /// - field position (structural) -> dictionary id
 /// - dictionary id -> value type
 /// - dictionary id -> dictionary (value) data
 ///
 /// Together, they allow resolving dictionary data when reading an IPC stream,
 /// using metadata recorded in the schema message and data recorded in the
 /// dictionary batch messages (see ResolveDictionaries).
 ///
 /// This structure isn't useful for writing an IPC stream, where only
 /// DictionaryFieldMapper is necessary.
 class ARROW_EXPORT DictionaryMemo {
  public:
   DictionaryMemo();
   ~DictionaryMemo();
 
   DictionaryFieldMapper& fields();
   const DictionaryFieldMapper& fields() const;
 
   /// \brief Return current dictionary corresponding to a particular
   /// id. Returns KeyError if id not found
-  Result<std::shared_ptr<ArrayData>> GetDictionary(int64_t id) const;
+  Result<std::shared_ptr<ArrayData>> GetDictionary(int64_t id, MemoryPool* pool) const;
 
   /// \brief Return dictionary value type corresponding to a
   /// particular dictionary id.
   Result<std::shared_ptr<DataType>> GetDictionaryType(int64_t id) const;
 
   /// \brief Return true if we have a dictionary for the input id
   bool HasDictionary(int64_t id) const;
 
   /// \brief Add a dictionary value type to the memo with a particular id.
   /// Returns KeyError if a different type is already registered with the same id.
   Status AddDictionaryType(int64_t id, const std::shared_ptr<DataType>& type);
 
   /// \brief Add a dictionary to the memo with a particular id. Returns
   /// KeyError if that dictionary already exists
   Status AddDictionary(int64_t id, const std::shared_ptr<ArrayData>& dictionary);
 
   /// \brief Append a dictionary delta to the memo with a particular id. Returns
   /// KeyError if that dictionary does not exists
-  Status AddDictionaryDelta(int64_t id, const std::shared_ptr<ArrayData>& dictionary,
-                            MemoryPool* pool);
+  Status AddDictionaryDelta(int64_t id, const std::shared_ptr<ArrayData>& dictionary);
 
   /// \brief Add a dictionary to the memo if it does not have one with the id,
   /// otherwise, replace the dictionary with the new one.
   Status AddOrReplaceDictionary(int64_t id, const std::shared_ptr<ArrayData>& dictionary);
 
  private:
   struct Impl;
   std::unique_ptr<Impl> impl_;
 };
 
 // For writing: collect dictionary entries to write to the IPC stream, in order
 // (i.e. inner dictionaries before dependent outer dictionaries).
@@ -152,7 +151,8 @@ Result<DictionaryVector> CollectDictionaries(const RecordBatch& batch,
 // Columns may be sparse, i.e. some entries may be left null
 // (e.g. if an inclusion mask was used).
 ARROW_EXPORT
-Status ResolveDictionaries(const ArrayDataVector& columns, const DictionaryMemo& memo);
+Status ResolveDictionaries(const ArrayDataVector& columns, const DictionaryMemo& memo,
+                           MemoryPool* pool);
 
 namespace internal {
 
diff --git a/cpp/src/arrow/ipc/reader.cc b/cpp/src/arrow/ipc/reader.cc
index 8a5672db8..a1b6ba4c0 100644
--- a/cpp/src/arrow/ipc/reader.cc
+++ b/cpp/src/arrow/ipc/reader.cc
@@ -437,51 +437,51 @@ Status DecompressBuffers(Compression::type compression, const IpcReadOptions& op
 Result<std::shared_ptr<RecordBatch>> LoadRecordBatchSubset(
     const flatbuf::RecordBatch* metadata, const std::shared_ptr<Schema>& schema,
     const std::vector<bool>* inclusion_mask, const DictionaryMemo* dictionary_memo,
     const IpcReadOptions& options, MetadataVersion metadata_version,
     Compression::type compression, io::RandomAccessFile* file) {
   ArrayLoader loader(metadata, metadata_version, options, file);
 
   ArrayDataVector columns(schema->num_fields());
   ArrayDataVector filtered_columns;
   FieldVector filtered_fields;
   std::shared_ptr<Schema> filtered_schema;
 
   for (int i = 0; i < schema->num_fields(); ++i) {
     const Field& field = *schema->field(i);
     if (!inclusion_mask || (*inclusion_mask)[i]) {
       // Read field
       auto column = std::make_shared<ArrayData>();
       RETURN_NOT_OK(loader.Load(&field, column.get()));
       if (metadata->length() != column->length) {
         return Status::IOError("Array length did not match record batch length");
       }
       columns[i] = std::move(column);
       if (inclusion_mask) {
         filtered_columns.push_back(columns[i]);
         filtered_fields.push_back(schema->field(i));
       }
     } else {
       // Skip field. This logic must be executed to advance the state of the
       // loader to the next field
       RETURN_NOT_OK(loader.SkipField(&field));
     }
   }
 
   // Dictionary resolution needs to happen on the unfiltered columns,
   // because fields are mapped structurally (by path in the original schema).
-  RETURN_NOT_OK(ResolveDictionaries(columns, *dictionary_memo));
+  RETURN_NOT_OK(ResolveDictionaries(columns, *dictionary_memo, options.memory_pool));
 
   if (inclusion_mask) {
     filtered_schema = ::arrow::schema(std::move(filtered_fields), schema->metadata());
     columns.clear();
   } else {
     filtered_schema = schema;
     filtered_columns = std::move(columns);
   }
   if (compression != Compression::UNCOMPRESSED) {
     RETURN_NOT_OK(DecompressBuffers(compression, options, &filtered_columns));
   }
 
   return RecordBatch::Make(filtered_schema, metadata->length(),
                            std::move(filtered_columns));
 }
@@ -671,47 +671,47 @@ Result<std::shared_ptr<RecordBatch>> ReadRecordBatch(
 Status ReadDictionary(const Buffer& metadata, DictionaryMemo* dictionary_memo,
                       const IpcReadOptions& options, io::RandomAccessFile* file) {
   const flatbuf::Message* message = nullptr;
   RETURN_NOT_OK(internal::VerifyMessage(metadata.data(), metadata.size(), &message));
   auto dictionary_batch = message->header_as_DictionaryBatch();
   if (dictionary_batch == nullptr) {
     return Status::IOError(
         "Header-type of flatbuffer-encoded Message is not DictionaryBatch.");
   }
 
   // The dictionary is embedded in a record batch with a single column
   auto batch_meta = dictionary_batch->data();
 
   CHECK_FLATBUFFERS_NOT_NULL(batch_meta, "DictionaryBatch.data");
 
   Compression::type compression;
   RETURN_NOT_OK(GetCompression(batch_meta, &compression));
   if (compression == Compression::UNCOMPRESSED &&
       message->version() == flatbuf::MetadataVersion::V4) {
     // Possibly obtain codec information from experimental serialization format
     // in 0.17.x
     RETURN_NOT_OK(GetCompressionExperimental(message, &compression));
   }
 
   int64_t id = dictionary_batch->id();
 
   // Look up the dictionary value type, which must have been added to the
   // DictionaryMemo already prior to invoking this function
   ARROW_ASSIGN_OR_RAISE(auto value_type, dictionary_memo->GetDictionaryType(id));
 
   // Load the dictionary data from the dictionary batch
   ArrayLoader loader(batch_meta, internal::GetMetadataVersion(message->version()),
                      options, file);
   auto dict_data = std::make_shared<ArrayData>();
   Field dummy_field("", value_type);
   RETURN_NOT_OK(loader.Load(&dummy_field, dict_data.get()));
 
   if (compression != Compression::UNCOMPRESSED) {
     ArrayDataVector dict_fields{dict_data};
     RETURN_NOT_OK(DecompressBuffers(compression, options, &dict_fields));
   }
 
   if (dictionary_batch->isDelta()) {
-    return dictionary_memo->AddDictionaryDelta(id, dict_data, options.memory_pool);
+    return dictionary_memo->AddDictionaryDelta(id, dict_data);
   }
   return dictionary_memo->AddOrReplaceDictionary(id, dict_data);
 }
diff --git a/cpp/src/arrow/testing/json_internal.cc b/cpp/src/arrow/testing/json_internal.cc
index e74cb8f49..9bd2f14ed 100644
--- a/cpp/src/arrow/testing/json_internal.cc
+++ b/cpp/src/arrow/testing/json_internal.cc
@@ -1611,22 +1611,22 @@ Status ReadArray(MemoryPool* pool, const rj::Value& json_array,
 Status ReadRecordBatch(const rj::Value& json_obj, const std::shared_ptr<Schema>& schema,
                        DictionaryMemo* dictionary_memo, MemoryPool* pool,
                        std::shared_ptr<RecordBatch>* batch) {
   DCHECK(json_obj.IsObject());
   const auto& batch_obj = json_obj.GetObject();
 
   ARROW_ASSIGN_OR_RAISE(const int64_t num_rows,
                         GetMemberInt<int64_t>(batch_obj, "count"));
 
   ARROW_ASSIGN_OR_RAISE(const auto json_columns, GetMemberArray(batch_obj, "columns"));
 
   ArrayDataVector columns(json_columns.Size());
   for (int i = 0; i < static_cast<int>(columns.size()); ++i) {
     ARROW_ASSIGN_OR_RAISE(columns[i],
                           ReadArrayData(pool, json_columns[i], schema->field(i)));
   }
 
-  RETURN_NOT_OK(ResolveDictionaries(columns, *dictionary_memo));
+  RETURN_NOT_OK(ResolveDictionaries(columns, *dictionary_memo, pool));
 
   *batch = RecordBatch::Make(schema, num_rows, columns);
   return Status::OK();
 }
