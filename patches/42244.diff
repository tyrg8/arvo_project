commit 4264daadd2487cd3e63cf83f5dca3f0ccc53af64
Author: Wouter van Oortmerssen <wvo@google.com>
Date:   Mon Dec 13 15:45:21 2021 -0800

    FlexBuffers fuzzer fixes
    
    - String dedup wasn't handling internal nulls correctly.
    - Verifier wasn't recursing for certain types.
    - Vector self-reference could create inf recursion.

diff --git a/include/flatbuffers/flexbuffers.h b/include/flatbuffers/flexbuffers.h
index d0859ff9..09e4d774 100644
--- a/include/flatbuffers/flexbuffers.h
+++ b/include/flatbuffers/flexbuffers.h
@@ -903,950 +903,946 @@ enum BuilderFlag {
 class Builder FLATBUFFERS_FINAL_CLASS {
  public:
   Builder(size_t initial_size = 256,
           BuilderFlag flags = BUILDER_FLAG_SHARE_KEYS)
       : buf_(initial_size),
         finished_(false),
         has_duplicate_keys_(false),
         flags_(flags),
         force_min_bit_width_(BIT_WIDTH_8),
         key_pool(KeyOffsetCompare(buf_)),
         string_pool(StringOffsetCompare(buf_)) {
     buf_.clear();
   }
 
 #ifdef FLATBUFFERS_DEFAULT_DECLARATION
   Builder(Builder &&) = default;
   Builder &operator=(Builder &&) = default;
 #endif
 
   /// @brief Get the serialized buffer (after you call `Finish()`).
   /// @return Returns a vector owned by this class.
   const std::vector<uint8_t> &GetBuffer() const {
     Finished();
     return buf_;
   }
 
   // Size of the buffer. Does not include unfinished values.
   size_t GetSize() const { return buf_.size(); }
 
   // Reset all state so we can re-use the buffer.
   void Clear() {
     buf_.clear();
     stack_.clear();
     finished_ = false;
     // flags_ remains as-is;
     force_min_bit_width_ = BIT_WIDTH_8;
     key_pool.clear();
     string_pool.clear();
   }
 
   // All value constructing functions below have two versions: one that
   // takes a key (for placement inside a map) and one that doesn't (for inside
   // vectors and elsewhere).
 
   void Null() { stack_.push_back(Value()); }
   void Null(const char *key) {
     Key(key);
     Null();
   }
 
   void Int(int64_t i) { stack_.push_back(Value(i, FBT_INT, WidthI(i))); }
   void Int(const char *key, int64_t i) {
     Key(key);
     Int(i);
   }
 
   void UInt(uint64_t u) { stack_.push_back(Value(u, FBT_UINT, WidthU(u))); }
   void UInt(const char *key, uint64_t u) {
     Key(key);
     UInt(u);
   }
 
   void Float(float f) { stack_.push_back(Value(f)); }
   void Float(const char *key, float f) {
     Key(key);
     Float(f);
   }
 
   void Double(double f) { stack_.push_back(Value(f)); }
   void Double(const char *key, double d) {
     Key(key);
     Double(d);
   }
 
   void Bool(bool b) { stack_.push_back(Value(b)); }
   void Bool(const char *key, bool b) {
     Key(key);
     Bool(b);
   }
 
   void IndirectInt(int64_t i) { PushIndirect(i, FBT_INDIRECT_INT, WidthI(i)); }
   void IndirectInt(const char *key, int64_t i) {
     Key(key);
     IndirectInt(i);
   }
 
   void IndirectUInt(uint64_t u) {
     PushIndirect(u, FBT_INDIRECT_UINT, WidthU(u));
   }
   void IndirectUInt(const char *key, uint64_t u) {
     Key(key);
     IndirectUInt(u);
   }
 
   void IndirectFloat(float f) {
     PushIndirect(f, FBT_INDIRECT_FLOAT, BIT_WIDTH_32);
   }
   void IndirectFloat(const char *key, float f) {
     Key(key);
     IndirectFloat(f);
   }
 
   void IndirectDouble(double f) {
     PushIndirect(f, FBT_INDIRECT_FLOAT, WidthF(f));
   }
   void IndirectDouble(const char *key, double d) {
     Key(key);
     IndirectDouble(d);
   }
 
   size_t Key(const char *str, size_t len) {
     auto sloc = buf_.size();
     WriteBytes(str, len + 1);
     if (flags_ & BUILDER_FLAG_SHARE_KEYS) {
       auto it = key_pool.find(sloc);
       if (it != key_pool.end()) {
         // Already in the buffer. Remove key we just serialized, and use
         // existing offset instead.
         buf_.resize(sloc);
         sloc = *it;
       } else {
         key_pool.insert(sloc);
       }
     }
     stack_.push_back(Value(static_cast<uint64_t>(sloc), FBT_KEY, BIT_WIDTH_8));
     return sloc;
   }
 
   size_t Key(const char *str) { return Key(str, strlen(str)); }
   size_t Key(const std::string &str) { return Key(str.c_str(), str.size()); }
 
   size_t String(const char *str, size_t len) {
     auto reset_to = buf_.size();
     auto sloc = CreateBlob(str, len, 1, FBT_STRING);
     if (flags_ & BUILDER_FLAG_SHARE_STRINGS) {
       StringOffset so(sloc, len);
       auto it = string_pool.find(so);
       if (it != string_pool.end()) {
         // Already in the buffer. Remove string we just serialized, and use
         // existing offset instead.
         buf_.resize(reset_to);
         sloc = it->first;
         stack_.back().u_ = sloc;
       } else {
         string_pool.insert(so);
       }
     }
     return sloc;
   }
   size_t String(const char *str) { return String(str, strlen(str)); }
   size_t String(const std::string &str) {
     return String(str.c_str(), str.size());
   }
   void String(const flexbuffers::String &str) {
     String(str.c_str(), str.length());
   }
 
   void String(const char *key, const char *str) {
     Key(key);
     String(str);
   }
   void String(const char *key, const std::string &str) {
     Key(key);
     String(str);
   }
   void String(const char *key, const flexbuffers::String &str) {
     Key(key);
     String(str);
   }
 
   size_t Blob(const void *data, size_t len) {
     return CreateBlob(data, len, 0, FBT_BLOB);
   }
   size_t Blob(const std::vector<uint8_t> &v) {
     return CreateBlob(v.data(), v.size(), 0, FBT_BLOB);
   }
 
   void Blob(const char *key, const void *data, size_t len) {
     Key(key);
     Blob(data, len);
   }
   void Blob(const char *key, const std::vector<uint8_t> &v) {
     Key(key);
     Blob(v);
   }
 
   // TODO(wvo): support all the FlexBuffer types (like flexbuffers::String),
   // e.g. Vector etc. Also in overloaded versions.
   // Also some FlatBuffers types?
 
   size_t StartVector() { return stack_.size(); }
   size_t StartVector(const char *key) {
     Key(key);
     return stack_.size();
   }
   size_t StartMap() { return stack_.size(); }
   size_t StartMap(const char *key) {
     Key(key);
     return stack_.size();
   }
 
   // TODO(wvo): allow this to specify an alignment greater than the natural
   // alignment.
   size_t EndVector(size_t start, bool typed, bool fixed) {
     auto vec = CreateVector(start, stack_.size() - start, 1, typed, fixed);
     // Remove temp elements and return vector.
     stack_.resize(start);
     stack_.push_back(vec);
     return static_cast<size_t>(vec.u_);
   }
 
   size_t EndMap(size_t start) {
     // We should have interleaved keys and values on the stack.
     // Make sure it is an even number:
     auto len = stack_.size() - start;
     FLATBUFFERS_ASSERT(!(len & 1));
     len /= 2;
     // Make sure keys are all strings:
     for (auto key = start; key < stack_.size(); key += 2) {
       FLATBUFFERS_ASSERT(stack_[key].type_ == FBT_KEY);
     }
     // Now sort values, so later we can do a binary search lookup.
     // We want to sort 2 array elements at a time.
     struct TwoValue {
       Value key;
       Value val;
     };
     // TODO(wvo): strict aliasing?
     // TODO(wvo): allow the caller to indicate the data is already sorted
     // for maximum efficiency? With an assert to check sortedness to make sure
     // we're not breaking binary search.
     // Or, we can track if the map is sorted as keys are added which would be
     // be quite cheap (cheaper than checking it here), so we can skip this
     // step automatically when appliccable, and encourage people to write in
     // sorted fashion.
     // std::sort is typically already a lot faster on sorted data though.
     auto dict = reinterpret_cast<TwoValue *>(stack_.data() + start);
     std::sort(
         dict, dict + len, [&](const TwoValue &a, const TwoValue &b) -> bool {
           auto as = reinterpret_cast<const char *>(buf_.data() + a.key.u_);
           auto bs = reinterpret_cast<const char *>(buf_.data() + b.key.u_);
           auto comp = strcmp(as, bs);
           // We want to disallow duplicate keys, since this results in a
           // map where values cannot be found.
           // But we can't assert here (since we don't want to fail on
           // random JSON input) or have an error mechanism.
           // Instead, we set has_duplicate_keys_ in the builder to
           // signal this.
           // TODO: Have to check for pointer equality, as some sort
           // implementation apparently call this function with the same
           // element?? Why?
           if (!comp && &a != &b) has_duplicate_keys_ = true;
           return comp < 0;
         });
     // First create a vector out of all keys.
     // TODO(wvo): if kBuilderFlagShareKeyVectors is true, see if we can share
     // the first vector.
     auto keys = CreateVector(start, len, 2, true, false);
     auto vec = CreateVector(start + 1, len, 2, false, false, &keys);
     // Remove temp elements and return map.
     stack_.resize(start);
     stack_.push_back(vec);
     return static_cast<size_t>(vec.u_);
   }
 
   // Call this after EndMap to see if the map had any duplicate keys.
   // Any map with such keys won't be able to retrieve all values.
   bool HasDuplicateKeys() const { return has_duplicate_keys_; }
 
   template<typename F> size_t Vector(F f) {
     auto start = StartVector();
     f();
     return EndVector(start, false, false);
   }
   template<typename F, typename T> size_t Vector(F f, T &state) {
     auto start = StartVector();
     f(state);
     return EndVector(start, false, false);
   }
   template<typename F> size_t Vector(const char *key, F f) {
     auto start = StartVector(key);
     f();
     return EndVector(start, false, false);
   }
   template<typename F, typename T>
   size_t Vector(const char *key, F f, T &state) {
     auto start = StartVector(key);
     f(state);
     return EndVector(start, false, false);
   }
 
   template<typename T> void Vector(const T *elems, size_t len) {
     if (flatbuffers::is_scalar<T>::value) {
       // This path should be a lot quicker and use less space.
       ScalarVector(elems, len, false);
     } else {
       auto start = StartVector();
       for (size_t i = 0; i < len; i++) Add(elems[i]);
       EndVector(start, false, false);
     }
   }
   template<typename T>
   void Vector(const char *key, const T *elems, size_t len) {
     Key(key);
     Vector(elems, len);
   }
   template<typename T> void Vector(const std::vector<T> &vec) {
     Vector(vec.data(), vec.size());
   }
 
   template<typename F> size_t TypedVector(F f) {
     auto start = StartVector();
     f();
     return EndVector(start, true, false);
   }
   template<typename F, typename T> size_t TypedVector(F f, T &state) {
     auto start = StartVector();
     f(state);
     return EndVector(start, true, false);
   }
   template<typename F> size_t TypedVector(const char *key, F f) {
     auto start = StartVector(key);
     f();
     return EndVector(start, true, false);
   }
   template<typename F, typename T>
   size_t TypedVector(const char *key, F f, T &state) {
     auto start = StartVector(key);
     f(state);
     return EndVector(start, true, false);
   }
 
   template<typename T> size_t FixedTypedVector(const T *elems, size_t len) {
     // We only support a few fixed vector lengths. Anything bigger use a
     // regular typed vector.
     FLATBUFFERS_ASSERT(len >= 2 && len <= 4);
     // And only scalar values.
     static_assert(flatbuffers::is_scalar<T>::value, "Unrelated types");
     return ScalarVector(elems, len, true);
   }
 
   template<typename T>
   size_t FixedTypedVector(const char *key, const T *elems, size_t len) {
     Key(key);
     return FixedTypedVector(elems, len);
   }
 
   template<typename F> size_t Map(F f) {
     auto start = StartMap();
     f();
     return EndMap(start);
   }
   template<typename F, typename T> size_t Map(F f, T &state) {
     auto start = StartMap();
     f(state);
     return EndMap(start);
   }
   template<typename F> size_t Map(const char *key, F f) {
     auto start = StartMap(key);
     f();
     return EndMap(start);
   }
   template<typename F, typename T> size_t Map(const char *key, F f, T &state) {
     auto start = StartMap(key);
     f(state);
     return EndMap(start);
   }
   template<typename T> void Map(const std::map<std::string, T> &map) {
     auto start = StartMap();
     for (auto it = map.begin(); it != map.end(); ++it)
       Add(it->first.c_str(), it->second);
     EndMap(start);
   }
 
   // If you wish to share a value explicitly (a value not shared automatically
   // through one of the BUILDER_FLAG_SHARE_* flags) you can do so with these
   // functions. Or if you wish to turn those flags off for performance reasons
   // and still do some explicit sharing. For example:
   // builder.IndirectDouble(M_PI);
   // auto id = builder.LastValue();  // Remember where we stored it.
   // .. more code goes here ..
   // builder.ReuseValue(id);  // Refers to same double by offset.
   // LastValue works regardless of whether the value has a key or not.
   // Works on any data type.
   struct Value;
   Value LastValue() { return stack_.back(); }
   void ReuseValue(Value v) { stack_.push_back(v); }
   void ReuseValue(const char *key, Value v) {
     Key(key);
     ReuseValue(v);
   }
 
   // Overloaded Add that tries to call the correct function above.
   void Add(int8_t i) { Int(i); }
   void Add(int16_t i) { Int(i); }
   void Add(int32_t i) { Int(i); }
   void Add(int64_t i) { Int(i); }
   void Add(uint8_t u) { UInt(u); }
   void Add(uint16_t u) { UInt(u); }
   void Add(uint32_t u) { UInt(u); }
   void Add(uint64_t u) { UInt(u); }
   void Add(float f) { Float(f); }
   void Add(double d) { Double(d); }
   void Add(bool b) { Bool(b); }
   void Add(const char *str) { String(str); }
   void Add(const std::string &str) { String(str); }
   void Add(const flexbuffers::String &str) { String(str); }
 
   template<typename T> void Add(const std::vector<T> &vec) { Vector(vec); }
 
   template<typename T> void Add(const char *key, const T &t) {
     Key(key);
     Add(t);
   }
 
   template<typename T> void Add(const std::map<std::string, T> &map) {
     Map(map);
   }
 
   template<typename T> void operator+=(const T &t) { Add(t); }
 
   // This function is useful in combination with the Mutate* functions above.
   // It forces elements of vectors and maps to have a minimum size, such that
   // they can later be updated without failing.
   // Call with no arguments to reset.
   void ForceMinimumBitWidth(BitWidth bw = BIT_WIDTH_8) {
     force_min_bit_width_ = bw;
   }
 
   void Finish() {
     // If you hit this assert, you likely have objects that were never included
     // in a parent. You need to have exactly one root to finish a buffer.
     // Check your Start/End calls are matched, and all objects are inside
     // some other object.
     FLATBUFFERS_ASSERT(stack_.size() == 1);
 
     // Write root value.
     auto byte_width = Align(stack_[0].ElemWidth(buf_.size(), 0));
     WriteAny(stack_[0], byte_width);
     // Write root type.
     Write(stack_[0].StoredPackedType(), 1);
     // Write root size. Normally determined by parent, but root has no parent :)
     Write(byte_width, 1);
 
     finished_ = true;
   }
 
  private:
   void Finished() const {
     // If you get this assert, you're attempting to get access a buffer
     // which hasn't been finished yet. Be sure to call
     // Builder::Finish with your root object.
     FLATBUFFERS_ASSERT(finished_);
   }
 
   // Align to prepare for writing a scalar with a certain size.
   uint8_t Align(BitWidth alignment) {
     auto byte_width = 1U << alignment;
     buf_.insert(buf_.end(), flatbuffers::PaddingBytes(buf_.size(), byte_width),
                 0);
     return static_cast<uint8_t>(byte_width);
   }
 
   void WriteBytes(const void *val, size_t size) {
     buf_.insert(buf_.end(), reinterpret_cast<const uint8_t *>(val),
                 reinterpret_cast<const uint8_t *>(val) + size);
   }
 
   template<typename T> void Write(T val, size_t byte_width) {
     FLATBUFFERS_ASSERT(sizeof(T) >= byte_width);
     val = flatbuffers::EndianScalar(val);
     WriteBytes(&val, byte_width);
   }
 
   void WriteDouble(double f, uint8_t byte_width) {
     switch (byte_width) {
       case 8: Write(f, byte_width); break;
       case 4: Write(static_cast<float>(f), byte_width); break;
       // case 2: Write(static_cast<half>(f), byte_width); break;
       // case 1: Write(static_cast<quarter>(f), byte_width); break;
       default: FLATBUFFERS_ASSERT(0);
     }
   }
 
   void WriteOffset(uint64_t o, uint8_t byte_width) {
     auto reloff = buf_.size() - o;
     FLATBUFFERS_ASSERT(byte_width == 8 || reloff < 1ULL << (byte_width * 8));
     Write(reloff, byte_width);
   }
 
   template<typename T> void PushIndirect(T val, Type type, BitWidth bit_width) {
     auto byte_width = Align(bit_width);
     auto iloc = buf_.size();
     Write(val, byte_width);
     stack_.push_back(Value(static_cast<uint64_t>(iloc), type, bit_width));
   }
 
   static BitWidth WidthB(size_t byte_width) {
     switch (byte_width) {
       case 1: return BIT_WIDTH_8;
       case 2: return BIT_WIDTH_16;
       case 4: return BIT_WIDTH_32;
       case 8: return BIT_WIDTH_64;
       default: FLATBUFFERS_ASSERT(false); return BIT_WIDTH_64;
     }
   }
 
   template<typename T> static Type GetScalarType() {
     static_assert(flatbuffers::is_scalar<T>::value, "Unrelated types");
     return flatbuffers::is_floating_point<T>::value ? FBT_FLOAT
            : flatbuffers::is_same<T, bool>::value
                ? FBT_BOOL
                : (flatbuffers::is_unsigned<T>::value ? FBT_UINT : FBT_INT);
   }
 
  public:
   // This was really intended to be private, except for LastValue/ReuseValue.
   struct Value {
     union {
       int64_t i_;
       uint64_t u_;
       double f_;
     };
 
     Type type_;
 
     // For scalars: of itself, for vector: of its elements, for string: length.
     BitWidth min_bit_width_;
 
     Value() : i_(0), type_(FBT_NULL), min_bit_width_(BIT_WIDTH_8) {}
 
     Value(bool b)
         : u_(static_cast<uint64_t>(b)),
           type_(FBT_BOOL),
           min_bit_width_(BIT_WIDTH_8) {}
 
     Value(int64_t i, Type t, BitWidth bw)
         : i_(i), type_(t), min_bit_width_(bw) {}
     Value(uint64_t u, Type t, BitWidth bw)
         : u_(u), type_(t), min_bit_width_(bw) {}
 
     Value(float f)
         : f_(static_cast<double>(f)),
           type_(FBT_FLOAT),
           min_bit_width_(BIT_WIDTH_32) {}
     Value(double f) : f_(f), type_(FBT_FLOAT), min_bit_width_(WidthF(f)) {}
 
     uint8_t StoredPackedType(BitWidth parent_bit_width_ = BIT_WIDTH_8) const {
       return PackedType(StoredWidth(parent_bit_width_), type_);
     }
 
     BitWidth ElemWidth(size_t buf_size, size_t elem_index) const {
       if (IsInline(type_)) {
         return min_bit_width_;
       } else {
         // We have an absolute offset, but want to store a relative offset
         // elem_index elements beyond the current buffer end. Since whether
         // the relative offset fits in a certain byte_width depends on
         // the size of the elements before it (and their alignment), we have
         // to test for each size in turn.
         for (size_t byte_width = 1;
              byte_width <= sizeof(flatbuffers::largest_scalar_t);
              byte_width *= 2) {
           // Where are we going to write this offset?
           auto offset_loc = buf_size +
                             flatbuffers::PaddingBytes(buf_size, byte_width) +
                             elem_index * byte_width;
           // Compute relative offset.
           auto offset = offset_loc - u_;
           // Does it fit?
           auto bit_width = WidthU(offset);
           if (static_cast<size_t>(static_cast<size_t>(1U) << bit_width) ==
               byte_width)
             return bit_width;
         }
         FLATBUFFERS_ASSERT(false);  // Must match one of the sizes above.
         return BIT_WIDTH_64;
       }
     }
 
     BitWidth StoredWidth(BitWidth parent_bit_width_ = BIT_WIDTH_8) const {
       if (IsInline(type_)) {
         return (std::max)(min_bit_width_, parent_bit_width_);
       } else {
         return min_bit_width_;
       }
     }
   };
 
  private:
   void WriteAny(const Value &val, uint8_t byte_width) {
     switch (val.type_) {
       case FBT_NULL:
       case FBT_INT: Write(val.i_, byte_width); break;
       case FBT_BOOL:
       case FBT_UINT: Write(val.u_, byte_width); break;
       case FBT_FLOAT: WriteDouble(val.f_, byte_width); break;
       default: WriteOffset(val.u_, byte_width); break;
     }
   }
 
   size_t CreateBlob(const void *data, size_t len, size_t trailing, Type type) {
     auto bit_width = WidthU(len);
     auto byte_width = Align(bit_width);
     Write<uint64_t>(len, byte_width);
     auto sloc = buf_.size();
     WriteBytes(data, len + trailing);
     stack_.push_back(Value(static_cast<uint64_t>(sloc), type, bit_width));
     return sloc;
   }
 
   template<typename T>
   size_t ScalarVector(const T *elems, size_t len, bool fixed) {
     auto vector_type = GetScalarType<T>();
     auto byte_width = sizeof(T);
     auto bit_width = WidthB(byte_width);
     // If you get this assert, you're trying to write a vector with a size
     // field that is bigger than the scalars you're trying to write (e.g. a
     // byte vector > 255 elements). For such types, write a "blob" instead.
     // TODO: instead of asserting, could write vector with larger elements
     // instead, though that would be wasteful.
     FLATBUFFERS_ASSERT(WidthU(len) <= bit_width);
     Align(bit_width);
     if (!fixed) Write<uint64_t>(len, byte_width);
     auto vloc = buf_.size();
     for (size_t i = 0; i < len; i++) Write(elems[i], byte_width);
     stack_.push_back(Value(static_cast<uint64_t>(vloc),
                            ToTypedVector(vector_type, fixed ? len : 0),
                            bit_width));
     return vloc;
   }
 
   Value CreateVector(size_t start, size_t vec_len, size_t step, bool typed,
                      bool fixed, const Value *keys = nullptr) {
     FLATBUFFERS_ASSERT(
         !fixed ||
         typed);  // typed=false, fixed=true combination is not supported.
     // Figure out smallest bit width we can store this vector with.
     auto bit_width = (std::max)(force_min_bit_width_, WidthU(vec_len));
     auto prefix_elems = 1;
     if (keys) {
       // If this vector is part of a map, we will pre-fix an offset to the keys
       // to this vector.
       bit_width = (std::max)(bit_width, keys->ElemWidth(buf_.size(), 0));
       prefix_elems += 2;
     }
     Type vector_type = FBT_KEY;
     // Check bit widths and types for all elements.
     for (size_t i = start; i < stack_.size(); i += step) {
       auto elem_width =
           stack_[i].ElemWidth(buf_.size(), i - start + prefix_elems);
       bit_width = (std::max)(bit_width, elem_width);
       if (typed) {
         if (i == start) {
           vector_type = stack_[i].type_;
         } else {
           // If you get this assert, you are writing a typed vector with
           // elements that are not all the same type.
           FLATBUFFERS_ASSERT(vector_type == stack_[i].type_);
         }
       }
     }
-    // If you get this assert, your fixed types are not one of:
+    // If you get this assert, your typed types are not one of:
     // Int / UInt / Float / Key.
-    FLATBUFFERS_ASSERT(!fixed || IsTypedVectorElementType(vector_type));
+    FLATBUFFERS_ASSERT(!typed || IsTypedVectorElementType(vector_type));
     auto byte_width = Align(bit_width);
     // Write vector. First the keys width/offset if available, and size.
     if (keys) {
       WriteOffset(keys->u_, byte_width);
       Write<uint64_t>(1ULL << keys->min_bit_width_, byte_width);
     }
     if (!fixed) Write<uint64_t>(vec_len, byte_width);
     // Then the actual data.
     auto vloc = buf_.size();
     for (size_t i = start; i < stack_.size(); i += step) {
       WriteAny(stack_[i], byte_width);
     }
     // Then the types.
     if (!typed) {
       for (size_t i = start; i < stack_.size(); i += step) {
         buf_.push_back(stack_[i].StoredPackedType(bit_width));
       }
     }
     return Value(static_cast<uint64_t>(vloc),
                  keys ? FBT_MAP
                       : (typed ? ToTypedVector(vector_type, fixed ? vec_len : 0)
                                : FBT_VECTOR),
                  bit_width);
   }
 
   // You shouldn't really be copying instances of this class.
   Builder(const Builder &);
   Builder &operator=(const Builder &);
 
   std::vector<uint8_t> buf_;
   std::vector<Value> stack_;
 
   bool finished_;
   bool has_duplicate_keys_;
 
   BuilderFlag flags_;
 
   BitWidth force_min_bit_width_;
 
   struct KeyOffsetCompare {
     explicit KeyOffsetCompare(const std::vector<uint8_t> &buf) : buf_(&buf) {}
     bool operator()(size_t a, size_t b) const {
       auto stra = reinterpret_cast<const char *>(buf_->data() + a);
       auto strb = reinterpret_cast<const char *>(buf_->data() + b);
       return strcmp(stra, strb) < 0;
     }
     const std::vector<uint8_t> *buf_;
   };
 
   typedef std::pair<size_t, size_t> StringOffset;
   struct StringOffsetCompare {
     explicit StringOffsetCompare(const std::vector<uint8_t> &buf)
         : buf_(&buf) {}
     bool operator()(const StringOffset &a, const StringOffset &b) const {
-      auto stra = reinterpret_cast<const char *>(buf_->data() + a.first);
-      auto strb = reinterpret_cast<const char *>(buf_->data() + b.first);
-      return strncmp(stra, strb, (std::min)(a.second, b.second) + 1) < 0;
+      auto stra = buf_->data() + a.first;
+      auto strb = buf_->data() + b.first;
+      auto cr = memcmp(stra, strb, (std::min)(a.second, b.second) + 1);
+      return cr < 0 || (cr == 0 && a.second < b.second);
     }
     const std::vector<uint8_t> *buf_;
   };
 
   typedef std::set<size_t, KeyOffsetCompare> KeyOffsetMap;
   typedef std::set<StringOffset, StringOffsetCompare> StringOffsetMap;
 
   KeyOffsetMap key_pool;
   StringOffsetMap string_pool;
 };
 
 // Helper class to verify the integrity of a FlexBuffer
 class Verifier FLATBUFFERS_FINAL_CLASS {
  public:
   Verifier(const uint8_t *buf, size_t buf_len,
            // Supplying this vector likely results in faster verification
            // of larger buffers with many shared keys/strings, but
            // comes at the cost of using additional memory 1/8th the size of
            // the buffer being verified, so it is allowed to be null
            // for special situations (memory constrained devices or
            // really small buffers etc). Do note that when not supplying
            // this buffer, you are not protected against buffers crafted
            // specifically to DoS you, i.e. recursive sharing that causes
            // exponential amounts of verification CPU time.
            std::vector<bool> *reuse_tracker)
       : buf_(buf), size_(buf_len), reuse_tracker_(reuse_tracker) {
     FLATBUFFERS_ASSERT(size_ < FLATBUFFERS_MAX_BUFFER_SIZE);
     if (reuse_tracker_) {
       reuse_tracker_->clear();
       reuse_tracker_->resize(size_);
     }
   }
 
  private:
   // Central location where any verification failures register.
   bool Check(bool ok) const {
     // clang-format off
     #ifdef FLATBUFFERS_DEBUG_VERIFICATION_FAILURE
       FLATBUFFERS_ASSERT(ok);
     #endif
     // clang-format on
     return ok;
   }
 
   // Verify any range within the buffer.
   bool VerifyFrom(size_t elem, size_t elem_len) const {
     return Check(elem_len < size_ && elem <= size_ - elem_len);
   }
   bool VerifyBefore(size_t elem, size_t elem_len) const {
     return Check(elem_len <= elem);
   }
 
   bool VerifyFromPointer(const uint8_t *p, size_t len) {
     auto o = static_cast<size_t>(p - buf_);
     return VerifyFrom(o, len);
   }
   bool VerifyBeforePointer(const uint8_t *p, size_t len) {
     auto o = static_cast<size_t>(p - buf_);
     return VerifyBefore(o, len);
   }
   
   bool VerifyByteWidth(size_t width) {
     return Check(width == 1 || width == 2 || width == 4 || width == 8);
   }
 
   bool VerifyType(int type) {
     return Check(type >= 0 && type < FBT_MAX_TYPE);
   }
 
   bool VerifyOffset(uint64_t off, const uint8_t *p) {
     return Check(off <= static_cast<uint64_t>(size_)) &&
                  off <= static_cast<uint64_t>(p - buf_);
   }
 
-  bool AlreadyVerified(const uint8_t *p) {
-    return reuse_tracker_ && (*reuse_tracker_)[p - buf_];
-  }
-
-  void MarkVerified(const uint8_t *p) {
-    if (reuse_tracker_)
-      (*reuse_tracker_)[p - buf_] = true;
+  bool CheckVerified(const uint8_t *p) {
+    if (!reuse_tracker_) return false;
+    if ((*reuse_tracker_)[p - buf_]) return true;
+    (*reuse_tracker_)[p - buf_] = true;
+    return false;
   }
 
   bool VerifyVector(const uint8_t *p, Type elem_type, uint8_t size_byte_width,
                     uint8_t elem_byte_width) {
     // Any kind of nesting goes thru this function, so guard against that
     // here.
-    if (AlreadyVerified(p))
+    if (CheckVerified(p))
       return true;
     if (!VerifyBeforePointer(p, size_byte_width))
       return false;
     auto sized = Sized(p, size_byte_width);
     auto num_elems = sized.size();
     auto max_elems = SIZE_MAX / elem_byte_width;
     if (!Check(num_elems < max_elems))
       return false;  // Protect against byte_size overflowing.
     auto byte_size = num_elems * elem_byte_width;
     if (!VerifyFromPointer(p, byte_size))
       return false;
-    if (!IsInline(elem_type)) {
-      if (elem_type == FBT_NULL) {
-        // Verify type bytes after the vector.
-        if (!VerifyFromPointer(p + byte_size, num_elems)) return false;
-        auto v = Vector(p, size_byte_width);
-        for (size_t i = 0; i < num_elems; i++)
-          if (!VerifyRef(v[i])) return false;
-      } else if (elem_type == FBT_KEY) {
-        auto v = TypedVector(p, elem_byte_width, FBT_KEY);
-        for (size_t i = 0; i < num_elems; i++)
-          if (!VerifyRef(v[i])) return false;
-      } else {
-        FLATBUFFERS_ASSERT(false);
-      }
+    if (elem_type == FBT_NULL) {
+      // Verify type bytes after the vector.
+      if (!VerifyFromPointer(p + byte_size, num_elems)) return false;
+      auto v = Vector(p, size_byte_width);
+      for (size_t i = 0; i < num_elems; i++)
+        if (!VerifyRef(v[i])) return false;
+    } else if (elem_type == FBT_KEY) {
+      auto v = TypedVector(p, elem_byte_width, FBT_KEY);
+      for (size_t i = 0; i < num_elems; i++)
+        if (!VerifyRef(v[i])) return false;
+    } else {
+      FLATBUFFERS_ASSERT(IsInline(elem_type));
     }
-    MarkVerified(p);
     return true;
   }
 
   bool VerifyKeys(const uint8_t *p, uint8_t byte_width) {
     // The vector part of the map has already been verified.
     const size_t num_prefixed_fields = 3;
     if (!VerifyBeforePointer(p, byte_width * num_prefixed_fields))
       return false;
     p -= byte_width * num_prefixed_fields;
     auto off = ReadUInt64(p, byte_width);
     if (!VerifyOffset(off, p))
       return false;
     auto key_byte_with =
       static_cast<uint8_t>(ReadUInt64(p + byte_width, byte_width));
     if (!VerifyByteWidth(key_byte_with))
       return false;
     return VerifyVector(p - off, FBT_KEY, key_byte_with, key_byte_with);
   }
 
   bool VerifyKey(const uint8_t* p) {
-    if (AlreadyVerified(p)) return true;
-    MarkVerified(p);
+    if (CheckVerified(p))
+      return true;
     while (p < buf_ + size_)
       if (*p++) return true;
     return false;
   }
 
   bool VerifyTerminator(const String &s) {
     return VerifyFromPointer(reinterpret_cast<const uint8_t *>(s.c_str()),
                              s.size() + 1);
   }
 
   bool VerifyRef(Reference r) {
     // r.parent_width_ and r.data_ already verified.
     if (!VerifyByteWidth(r.byte_width_) || !VerifyType(r.type_)) {
       return false;
     }
     if (IsInline(r.type_)) {
       // Inline scalars, don't require further verification.
       return true;
     }
     // All remaining types are an offset.
     auto off = ReadUInt64(r.data_, r.parent_width_);
     if (!VerifyOffset(off, r.data_))
       return false;
     auto p = r.Indirect();
     switch (r.type_) {
       case FBT_INDIRECT_INT:
       case FBT_INDIRECT_UINT:
       case FBT_INDIRECT_FLOAT:
         return VerifyFromPointer(p, r.byte_width_);
       case FBT_KEY:
         return VerifyKey(p);
       case FBT_MAP:
         return VerifyVector(p, FBT_NULL, r.byte_width_, r.byte_width_) &&
                VerifyKeys(p, r.byte_width_);
       case FBT_VECTOR:
         return VerifyVector(p, FBT_NULL, r.byte_width_, r.byte_width_);
       case FBT_VECTOR_INT:
         return VerifyVector(p, FBT_INT, r.byte_width_, r.byte_width_);
       case FBT_VECTOR_BOOL:
       case FBT_VECTOR_UINT:
         return VerifyVector(p, FBT_UINT, r.byte_width_, r.byte_width_);
       case FBT_VECTOR_FLOAT:
         return VerifyVector(p, FBT_FLOAT, r.byte_width_, r.byte_width_);
       case FBT_VECTOR_KEY:
         return VerifyVector(p, FBT_KEY, r.byte_width_, r.byte_width_);
       case FBT_VECTOR_STRING_DEPRECATED:
         // Use of FBT_KEY here intentional, see elsewhere.
         return VerifyVector(p, FBT_KEY, r.byte_width_, r.byte_width_);
       case FBT_BLOB:
         return VerifyVector(p, FBT_UINT, r.byte_width_, 1);
       case FBT_STRING:
         return VerifyVector(p, FBT_UINT, r.byte_width_, 1) &&
                VerifyTerminator(String(p, r.byte_width_));
       case FBT_VECTOR_INT2:
       case FBT_VECTOR_UINT2:
       case FBT_VECTOR_FLOAT2:
       case FBT_VECTOR_INT3:
       case FBT_VECTOR_UINT3:
       case FBT_VECTOR_FLOAT3:
       case FBT_VECTOR_INT4:
       case FBT_VECTOR_UINT4:
       case FBT_VECTOR_FLOAT4: {
         uint8_t len = 0;
         auto vtype = ToFixedTypedVectorElementType(r.type_, &len);
         if (!VerifyType(vtype))
           return false;
         return VerifyFromPointer(p, r.byte_width_ * len);
       }
       default:
         return false;
     }
   }
 
  public:
   bool VerifyBuffer() {
     if (!Check(size_ >= 3)) return false;
     auto end = buf_ + size_;
     auto byte_width = *--end;
     auto packed_type = *--end;
     return VerifyByteWidth(byte_width) &&
            Check(end - buf_ >= byte_width) &&
            VerifyRef(Reference(end - byte_width, byte_width, packed_type));
   }
 
  private:
   const uint8_t *buf_;
   size_t size_;
   std::vector<bool> *reuse_tracker_;
 };
 
 // Utility function that contructs the Verifier for you, see above for parameters.
